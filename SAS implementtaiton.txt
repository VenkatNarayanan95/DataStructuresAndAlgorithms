venry: #######################################################################################
# Metric Provider : SAS Metrics Plug-in                                               #
# Plugin Uses SAS dashboard data for computing SAS Metrics                            #
# Output : Updates the RedShift Table and return the success status                    #
#######################################################################################

import re
import os
import sys
import time
import argparse
from datetime import date
from coral import coralrpc
import coral.complex_types
from retrying import retry
from pyodinhttp import odin_retrieve_pair
from dateutil.relativedelta import relativedelta
from aee_oe_data_collector_aggregator_config.config.sas_config import *
from aee_oe_data_collector_aggregator_config.config.logger import Logger
from com.amazonaws.sasriskaggregationservice.sasriskaggregationservice import *
from com.amazon.coral.availability.throttlingexception import ThrottlingException
from aee_oe_data_collector_aggregator.utilities.downloadUploadToS3 import S3_Operations
from aee_oe_data_collector_aggregator_config.config.basic_config import Common_Constants, ODIN, COMMANDS
from aee_oe_data_collector_aggregator.utilities.redshift_operation_wrapper import redshift_operation_wrapper


class SAS_Plugin:

	def __init__(self):
		Logger.printLogs("INFO","Triggering SAS Plugin")
		self.rs_upsert = redshift_operation_wrapper()

	def SAS_Init(Self):
		# Fetching access key and secret key from admin ODIN
		aws_access_key_id,aws_secret_access_key = odin_retrieve_pair(ODIN.AEE_ODIN)
		aws_access_key_id = aws_access_key_id.data.decode('utf-8').encode()
		aws_secret_access_key = aws_secret_access_key.data.decode('utf-8').encode()

		# Creating orchestrator for client
		_orchestrator = coralrpc.new_orchestrator(
						 endpoint=SAS_Config.endpoint,
						 aws_service=SAS_Config.service,
						 aws_region=SAS_Config.region,
						 signature_algorithm='v4',
						 timeout=30, # 30 seconds why? New service so we don't know about the latency
						 aws_access_key=aws_access_key_id,
						 aws_secret_key=aws_secret_access_key,
						 aws_security_token = SAS_Config.security_token)

		# Creating SAS risk client
		return SASRiskAggregationServiceClient(_orchestrator)

	def _string_to_camelcase(self, s):
		return re.sub(r'(?:^|_)(.)', lambda m: m.group(1).upper(), s)


	def coral_to_python(self, obj):
		if isinstance(obj, coral.complex_types.List):
			return [self.coral_to_python(el) for el in obj]
		if isinstance(obj, coral.complex_types.Structure):
			required_string = {}
			for propname in dir(obj):
				if propname.startswith('_'):
					continue
				required_string[self._string_to_camelcase(propname)] = self.coral_to_python(getattr(obj, propname))
			return required_string
		if isinstance(obj, coral.simple_types.String):
			return str(obj)
		if isinstance(obj, coral.simple_types.Boolean):
			return bool(obj)
		return obj

	def get_entities(self):
		'''
		This function fetches the client names from Config file and returns client names.
		'''
		select_result = self.rs_upsert.run_redshift_select_query(SAS_Config.get_distinct_leader)
		Logger.printLogs("DEBUG","Query result {}".format(select_result))
		if select_result == 1:
			Logger.printLogs("EXCEPTION","Query resulted in exception and returned {}".format(select_result))
			sys.exit(1)

		entities = []
		for clients in select_result:
			entities.append(clients[0])
		if len(entities) < 2:
			entities = str("('") + entities[0] + str("')")
		else:
			entities = tuple(entities)
		return entities

	def get_distinct_managers(self, entities):
		'''
		Based on the entities from get_entities function, this function fetches, all the employees under that login.
		'''
		try:
			Logger.printLogs("INFO", " Calling Manager To Manager Aggregate Mapping Query")
			manager_to_manager_aggregate_mapping = self.rs_upsert.run_redshift_select_query(SAS_Config.manager_to_manager_aggregate_mapping_query.format(entities))

			if manager_to_manager_aggregate_mapping == 1:
				Logger.printLogs("ERROR","Manager Aggregate relation Mapping query failed with Error code 1")
				sys.exit(1)

			if len(manager_to_manager_aggregate_mapping) < 1:
				Logger.printLogs("WARNING","manager aggregate relation mapping is empty")

			distinct_managers = set()
			manager_to_entity_mapping = {}
			for mapping in manager_to_manager_aggregate_mapping:
				roll_up_to_manager = mapping[0]
				manager = mapping[1]
				if roll_up_to_manager in entities:
					distinct_managers.add(manager)
					manager_to_entity_mapping.update({manager:roll_up_to_manager})
			return tuple(distinct_managers), manager_to_entity_mapping, manager_to_manager_aggregate_mapping

		except Exception as error:
			Logger.printLogs("[ ERROR ]","Getting distinct managers failed with error {}".format(error))
			sys.exit(1)

	def load_SAS_data_to_redshift(self, program_name):
		'''
		Truncate the rawdata tables and update the new rawdata in tables. 
		Input: program_name i.e. type of SAS riks
		'''
		if (program_name == SAS_Config.program_name_apollo):
			try:
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table {}".format(SAS_Config.temp_redshift_apollo_table)))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.temp_redshift_apollo_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table " + SAS_Config.redshift_apollo_table))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.redshift_apollo_table))
				os.system(COMMANDS.Redshift_Update_Comamnd.format(ODIN.Redshift_ODIN,"csv",SAS_Config.temp_redshift_apollo_table,Common_Constants.S3_Bucket+'/'+SAS_Config.sas_apollo_temp_s3_location))
				Logger.printLogs("INFO","Data updation to table '{}' completed successfully.".format(SAS_Config.temp_redshift_apollo_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,SAS_Config.query_for_getting_required_data.format(SAS_Config.redshift_apollo_table,SAS_Config.temp_redshift_apollo_table)))
				Logger.printLogs("INFO","Unique Data updation to table '{}' completed successfully.".format(SAS_Config.redshift_apollo_table))

			except Exception as error:
				Logger.printLogs("[ ERROR ]","Transferring Apollo data to redshift table failed with message : {}".format(error))
				sys.exit(1)

		elif (program_name == SAS_Config.program_name_brazil):
			try:
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table {}".format(SAS_Config.temp_redshift_brazil_table)))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.temp_redshift_brazil_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table " + SAS_Config.redshift_brazil_table))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.redshift_brazil_table))
				os.system(COMMANDS.Redshift_Update_Comamnd.format(ODIN.Redshift_ODIN,"csv",SAS_Config.temp_redshift_brazil_table,Common_Constants.S3_Bucket+'/'+SAS_Config.sas_brazil_temp_s3_location))
				Logger.printLogs("INFO","Data updation to table '{}' completed successfully.".format(SAS_Config.redshift_brazil_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,SAS_Config.query_for_getting_required_data.format(SAS_Config.redshift_brazil_table,SAS_Config.temp_redshift_brazil_table)))
				Logger.printLogs("INFO","Unique Data updation to table '{}' completed successfully.".format(SAS_Config.redshift_brazil_table))

			except Exception as error:
				Logger.printLogs("[ ERROR ]","Transferring Brazil data to redshift table failed with message : {}".format(error))
				sys.exit(1)

		elif (program_name == SAS_Config.program_name_pipeline_targets):
			try:
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table {}".format(SAS_Config.temp_redshift_pipeline_target_table)))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.temp_redshift_pipeline_target_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table " + SAS_Config.redshift_pipeline_target_table))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.redshift_pipeline_target_table))
				os.system(COMMANDS.Redshift_Update_Comamnd.format(ODIN.Redshift_ODIN,"csv",SAS_Config.temp_redshift_pipeline_target_table,Common_Constants.S3_Bucket+'/'+SAS_Config.sas_pipeline_target_temp_s3_location))
				Logger.printLogs("INFO","Data updation to table '{}' completed successfully.".format(SAS_Config.temp_redshift_pipeline_target_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,SAS_Config.query_for_getting_required_data.format(SAS_Config.redshift_pipeline_target_table,SAS_Config.temp_redshift_pipeline_target_table)))
				Logger.printLogs("INFO","Unique Data updation to table '{}' completed successfully.".format(SAS_Config.redshift_pipeline_target_table))

			except Exception as error:
				Logger.printLogs("ERROR","Transferring Pipeline Target data to redshift table failed with message : {}".format(error))
				sys.exit(1)
		else:
			Logger.printLogs("ERROR","Wrong Program name passed")
			return

	def create_metrics(self, _client, metric_snapshot):
		'''
		Main function, responsible to start generating metrics.
		Input: 
		_client: Client of SAS API
		metric_snapshot: Type of metrics refresh i.e. WoW or MoM
		'''
		entities = self.get_entities()
		distinct_managers, manager_to_entity_mapping, manager_to_manager_aggregate_mapping =self.get_distinct_managers(entities)
		self.apollo_file = open(SAS_Config.sas_apollo_temp_file,"w")
		self.brazil_file = open(SAS_Config.sas_brazil_temp_file,"w")
		self.pipeline_file = open(SAS_Config.sas_pipeline_target_temp_file,"w")
		self.metric_snapshot = metric_snapshot
		Logger.printLogs("[INFO]","Executing it for all managers")
		iterator = 0
		for manager in distinct_managers:
			Logger.printLogs("[UPDATE]","{} Managers left to process".format(len(distinct_managers)-iterator))
			iterator = iterator+1

			data = self.call_get_high_level_resource_details(manager, _client)

			final_list_of_apollo_environments = []
			final_list_of_brazil_environments = []
			final_list_of_pipeline_targets = []

			try:
				apollo_result = self.coral_to_python(data)['HighLevelResourceDetailsByResourceTypeMap']['ApolloEnvironmentNameAndStage']
				final_list_of_apollo_environments = self.process_high_level_data(apollo_result, SAS_Config.program_name_apollo)
			except Exception as err:
				Logger.printLogs("[INFO]","No Apollo SAS risk found ! {}".format(err))
			try:
				brazil_data = self.coral_to_python(data)['HighLevelResourceDetailsByResourceTypeMap']['BrazilVersionSet']
				final_list_of_brazil_environments = self.process_high_level_data(brazil_data, SAS_Config.program_name_brazil)
			except Exception as err:
				Logger.printLogs("[INFO]","No Brazil version set SAS risk found ! {}".format(err))

			try:
				pipeline_target_data = self.coral_to_python(data)['HighLevelResourceDetailsByResourceTypeMap']['AmazonPipelineTarget']
				final_list_of_pipeline_targets = self.process_high_level_data(pipeline_target_data, SAS_Config.program_name_pipeline_targets)
			except Exception as err:
				Logger.printLogs("[INFO]","No Pipeline target SAS risk found ! {}".format(err))

			try:

				self.call_batch_details_wrapper(manager, _client, final_list_of_apollo_environments, SAS_Config.program_name_apollo)

				self.call_batch_details_wrapper(manager, _client, final_list_of_brazil_environments, SAS_Config.program_name_brazil)

				self.call_batch_details_wrapper(manager,_client,final_list_of_pipeline_targets, SAS_Config.program_name_pipeline_targets)


			except Exception as error:
				Logger.printLogs("ERROR","Getting final details of SAS failed with error {}".format(error))
				sys.exit(1)

		self.apollo_file.close()
		self.brazil_file.close()
		self.pipeline_file.close()

		try:
			S3_Operations.uploadDataToS3(ODIN.S3_ODIN, Common_Constants.S3_Bucket, SAS_Config.sas_apollo_temp_s3_location, SAS_Config.sas_apollo_temp_file)
			self.load_SAS_data_to_redshift(SAS_Config.program_name_apollo)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred while uploading Apollo csv file to S3 bucket. {}".format(err))
			sys.exit(1)

		try:
			S3_Operations.uploadDataToS3(ODIN.S3_ODIN, Common_Constants.S3_Bucket, SAS_Config.sas_brazil_temp_s3_location, SAS_Config.sas_brazil_temp_file)
			self.load_SAS_data_to_redshift(SAS_Config.program_name_brazil)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred while uploading Brazil csv file to S3 bucket. {}".format(err))
			sys.exit(1)

		try:
			S3_Operations.uploadDataToS3(ODIN.S3_ODIN, Common_Constants.S3_Bucket, SAS_Config.sas_pipeline_target_temp_s3_location, SAS_Config.sas_pipeline_target_temp_file)
			self.load_SAS_data_to_redshift(SAS_Config.program_name_pipeline_targets)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred while uploading Pipeline Target csv file to S3 bucket. {}".format(err))
			sys.exit(1)

		try:

			Logger.printLogs("INFO","Starting Rolling up of SAS metrics")
			roll_return_code = self.roll_up_metrics(distinct_managers)

			Logger.printLogs("INFO","Starting Aggregating SAS metrics")
			self.aggregate_metrics(distinct_managers)

			Logger.printLogs("INFO","Updating refresh status of SAS metrics")
			self.insert_success_refresh_status()

			Logger.printLogs("INFO","Updating duplicate tables of SAS metrics for Manual update of OE Risks")
			self.insert_sas_data_into_duplicate_table()

		except Exception as err:
			Logger.printLogs("ERROR","SAS Plug-in failed with error {}".format(err))
			sys.exit(1)

	def process_high_level_data(self, result, resource_type):
		'''
		Function is responsible to get all environments or resources from result and return consolidated list.
		Input: Result, metric_snapshot and type of resource
		Output: Consolidated list of Environments
		'''
		final_list_environments = []
		for environment in result:
			environment_json_info = self.coral_to_python(environment)
			env_name = environment_json_info['ResourceName']
			Logger.printLogs("[DEBUG]","{} environment {} added to list".format(resource_type, env_name))
			final_list_environments.append(env_name)

		return final_list_environments

	def get_apollo_data_and_write_in_file(self, data, file, metric_snapshot):
		'''
		Fetches all the details from processed Apollo data and writes into a csv file.
		'''
		list_data = self.coral_to_python(data)
		for filtered_depth_data in list_data['Details']:
			env_name = filtered_depth_data['Name']
			stage_name = env_name.split(':')[1]
			owner_login = filtered_depth_data['OwnerLogin']
			children_count = filtered_depth_data['ChildrenCount']
			consumer_count = filtered_depth_data['ConsumerCount']
			host_count = filtered_depth_data['HostCount']
			owner = filtered_depth_data['Owner']
			pipeline_id = filtered_depth_data['PipelineId']
			pipeline_name = filtered_depth_data['PipelineName']
			oldest_risk_age = filtered_depth_data['OldestRiskAge']
			risk_count = filtered_depth_data['RiskCount']
			visibility = filtered_depth_data['Visibility']
			recommendation_with_action = {}
			for recommendation in filtered_depth_data['Recommendations']:
				for target_resource in recommendation['AssociatedTargetResources']:
					recommendation_with_action[target_resource] = recommendation['Template']

			for software_risk in filtered_depth_data['SoftwareRisks']:
				for risk_details in software_risk['SoftwareRiskDetails']:
					blocked_resources = risk_details['TargetResource']
					action_required = recommendation_with_action[blocked_resources]
					associated_campaigns = software_risk['AssociatedCampaign']['Name'].replace(",","")

					oldest_risk_age = risk_details['RiskAge']
					oldest_recommendation_age = str(risk_details['RecommendationAge'])

					if (not oldest_recommendation_age.isnumeric()):
						oldest_recommendation_age = '-1'

					current_date = date.today()
					snapshot_number = current_date.isocalendar()[1]
					classification = 'None'
					curent_year = current_date.strftime('%Y')
					file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(env_name,
																										 stage_name,
																										 pipeline_id,
																										 pipeline_name,
																										 owner,
																										 owner_login,
																										 consumer_count,
																										 children_count,
																										 risk_count,
																										 visibility,
																										 host_count,
																										 oldest_risk_age,
																										 oldest_recommendation_age,
																										 action_required,
																										 blocked_resources,
																										 associated_campaigns,
																										 current_date,
																										 curent_year,
																										 classification,
																										 metric_snapshot,
																										 snapshot_number))

	def get_brazil_data_and_write_in_file(self, data, file, metric_snapshot):
		'''
		Fetches all the details from processed Brazil data and writes into a csv file.
		'''
		list_data = self.coral_to_python(data)
		for version_set_json_info in list_data['Details']:
			version_set = version_set_json_info['Name']
			owner_login = version_set_json_info['OwnerLogin']
			owner = version_set_json_info['Owner']

			pipeline_id = version_set_json_info['PipelineId']
			pipeline_name = version_set_json_info['PipelineName']
			oldest_recommendation_age = version_set_json_info['OldestRecommendationAge']
			oldest_risk_age = version_set_json_info['OldestRiskAge']
			risk_count = version_set_json_info['RiskCount']

			recommendation_with_action = {}
			for recommendation in version_set_json_info['Recommendations']:
				for target_resource in recommendation['Actions']:
					recommendation_with_action[recommendation['TargetResource']] = target_resource['Template']

			for software_risk in version_set_json_info['SoftwareRisks']:
				for risk_details in software_risk['SoftwareRiskDetails']:
					blocked_resources = risk_details['TargetResource']
					try:
						action_required = recommendation_with_action[blocked_resources]
					except Exception as err:
						action_required = 'None'
					associated_campaigns = software_risk['AssociatedCampaign']['Name']
					oldest_risk_age = risk_details['RiskAge']
					oldest_recommendation_age = str(risk_details['RecommendationAge'])

					if (not oldest_recommendation_age.isnumeric()):
						oldest_recommendation_age = '-1'

					current_date = date.today()
					snapshot_number = current_date.isocalendar()[1]
					classification = 'None'
					curent_year = current_date.strftime('%Y')
					file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(version_set,
																						  pipeline_id,
																						  pipeline_name,
																						  owner,
																						  owner_login,
																						  risk_count,
																						  oldest_risk_age,
																						  oldest_recommendation_age,
																						  blocked_resources,
																						  action_required,
																						  associated_campaigns,
																						  current_date,
																						  curent_year,
																						  classification,
																						  metric_snapshot,
																						  snapshot_number))

	def get_pipeline_target_data_and_write_in_file(self, data, file, metric_snapshot):
		'''
		Fetches all the details from processed pipeline target data and writes into a csv file.
		'''
		list_data = self.coral_to_python(data)
		for pipeline_target_json_info in list_data['Details']:
			target_name = pipeline_target_json_info['Name']
			pipeline_name = pipeline_target_json_info['PipelineName']
			pipeline_id = pipeline_target_json_info['PipelineId']
			owner_name = pipeline_target_json_info['Owner']
			owner_login = pipeline_target_json_info['OwnerLogin']
			oldest_risk_age = pipeline_target_json_info['OldestRiskAge']
			risk_count = pipeline_target_json_info['RiskCount']

			recommendation_with_action = {}

			try:
				for recommendation in pipeline_target_json_info['Recommendations']:
					for target_resource in recommendation['Actions']:
						recommendation_with_action[recommendation['TargetResource']] = target_resource['Template']
			except Exception as err:
				Logger.printLogs("INFO", "Please Refer to Associated campaign urls to follow the recommendation")

			for software_risk in pipeline_target_json_info['SoftwareRisks']:
				for risk_details in software_risk['SoftwareRiskDetails']:
					blocked_resources = risk_details['TargetResource']
				try:
					action_required = recommendation_with_action[blocked_resources]
				except Exception as err:
					action_required = 'None'
				associated_campaigns = software_risk['AssociatedCampaign']['Name']
				oldest_risk_age = risk_details['RiskAge']

				try:
					oldest_recommendations_age = str(risk_details['RecommendationAge'])
				except Exception:
					Logger.printLogs("INFO","Recommendation Age is not configured in Pipeline Target plug-in, setting default value")
					oldest_recommendations_age = '-1'

				current_date = date.today()
				snapshot_number = current_date.isocalendar()[1]
				classification = 'None'
				current_year = current_date.strftime('%Y')

				file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(target_name,
																					  pipeline_id,
																					  pipeline_name,
																					  owner_name,
																					  owner_login,
																					  risk_count,
																					  oldest_risk_age,
																					  oldest_recommendations_age,
																					  blocked_resources,
																					  action_required,
																					  associated_campaigns,
																					  current_date,
																					  current_year,
																					  classification,
																					  metric_snapshot,
																					  snapshot_number
																					  ))

	@retry(wait_fixed=10000, stop_max_attempt_number=3)
	def call_get_high_level_resource_details(self, manager, _client):
		'''
		Makes API call to get the high level SAS risk details for any particular manager. It includes all Blocked SAS risks.
		'''
		try:
			data = _client.get_resource_high_level_risk_details_by_campaign_type(aggregate_id = "Person:"+manager, campaign_type='BLOCKED')

		except ThrottlingException as e:
			Logger.printLogs("EXCEPTION","ThrottlingException occurred, Sleeping! {}".format(e))
			time.sleep(5)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred {}".format(err))
			return 1

		return data

	def call_batch_details_wrapper(self, manager, _client, final_list_of_environments, type_of_env):
		'''
		This is the wrapper function, which is responsible to limit the batch size to 100 of every request.
		'''
		batch_iterations = len(final_list_of_environments)// SAS_Config.BATCH_SIZE +1
		Logger.printLogs("[INFO]","Inside Call Batch Wrapper function: Total Batches required {}".format(batch_iterations))

		try:
			for start_range in range(0,len(final_list_of_environments),SAS_Config.BATCH_SIZE):
				end_range = start_range + SAS_Config.BATCH_SIZE

				consolidated_data = self.call_batch_details(manager, _client, final_list_of_environments[start_range:end_range], type_of_env)

				if type_of_env == SAS_Config.program_name_apollo and consolidated_data != 1:
					self.get_apollo_data_and_write_in_file(consolidated_data, self.apollo_file, self.metric_snapshot)
				elif type_of_env == SAS_Config.program_name_brazil and consolidated_data != 1:
					self.get_brazil_data_and_write_in_file(consolidated_data, self.brazil_file, self.metric_snapshot)
				elif type_of_env == SAS_Config.program_name_pipeline_targets and consolidated_data != 1:
					self.get_pipeline_target_data_and_write_in_file(consolidated_data, self.pipeline_file, self.metric_snapshot)

		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred {}".format(err))
			sys.exit(1)

		return


	@retry(wait_fixed=10000, stop_max_attempt_number=3)
	def call_batch_details(self, manager, _client, final_list_of_environments, type_of_env):
		'''
		This function takes list of environments as input and fetches all detailed information.
		'''
		Logger.printLogs("[DEBUG]","Batch Length is {}".format(len(final_list_of_environments)))
		try:
			if (type_of_env == SAS_Config.program_name_apollo):
				data = _client.batch_get_apollo_environment_stage_details(requester_login = manager, entity_id_list=final_list_of_environments, campaign_type='BLOCKED')
			elif (type_of_env == SAS_Config.program_name_brazil):
				data = _client.batch_get_brazil_version_set_details(requester_login = manager, entity_id_list=final_list_of_environments, campaign_type='BLOCKED')
			elif (type_of_env == SAS_Config.program_name_pipeline_targets):
				data = _client.batch_get_pipeline_target_resource_details(requester_login = manager, entity_id_list = final_list_of_environments, campaign_type = 'BLOCKED')
			else:
				Logger.printLogs("ERROR","Wrong type_of_env is passed, Please check! ")

		except ThrottlingException as error:
			Logger.printLogs("EXCEPTION","ThrottlingException occurred, Sleeping! {}".format(error))
			time.sleep(5)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred {}".format(err))
			return 1

		return data

	def roll_up_metrics(self, distinct_managers):
		'''
		Function responsible for Rolling up the metrics by using Rawdata tables.
		'''
		Logger.printLogs("[INFO]","Starting rolling up of SAS metrics")

		for manager in distinct_managers:


			insert_apollo_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.roll_upto_manager_query.format(SAS_Config.rolled_redshift_apollo_table,
																										  manager,
																										  SAS_Config.redshift_apollo_table))
			Logger.printLogs("[DEBUG]","Apollo Data rolled upto query for {} manager result {}".format(manager, insert_apollo_result))


			insert_brazil_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.roll_upto_manager_query.format(SAS_Config.rolled_redshift_brazil_table,
																										  manager,
																										  SAS_Config.redshift_brazil_table))
			Logger.printLogs("[DEBUG]","Brazil Data rolled upto query for {} manager result {}".format(manager, insert_brazil_result))


			insert_pipeline_target_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.roll_upto_manager_query.format(SAS_Config.rolled_redshift_pipeline_target_table,
																										  manager,
																										  SAS_Config.redshift_pipeline_target_table))
			Logger.printLogs("[DEBUG]","Pipeline Target Data rolled upto query for {} manager result {}".format(manager, insert_pipeline_target_result))

			if (insert_apollo_result != 0 or insert_brazil_result != 0 or insert_pipeline_target_result != 0):
				Logger.printLogs("ERROR","Query failed to roll the data for manager {}".format(manager))
				sys.exit(1)



	def aggregate_metrics(self, distinct_managers):
		'''
		This function creates Aggregate metrics from the Rolled up data
		'''
		Logger.printLogs("[INFO]","Starting Aggregate Rolled up of SAS metrics")
		manager_processed = 0
		for manager in distinct_managers:

			manager_processed = manager_processed + 1

			Logger.printLogs("[DEBUG]","Manager processed [{}]/[{}]".format(manager_processed,len(distinct_managers)))


			insert_total_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_total_query.format(SAS_Config.sas_aggregate_table, manager))
			Logger.printLogs("[DEBUG]","Total Metrics rolled upto {} manager query result {}".format(manager, insert_total_result))


			insert_recommended_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_recommendation_query.format(SAS_Config.sas_aggregate_table, manager))
			Logger.printLogs("[DEBUG]","Recommendations Metrics rolled upto {} manager query result {}".format(manager, insert_recommended_result))


			insert_older_recommendation_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_long_recommendation_query.format(SAS_Config.sas_aggregate_table, manager))
			Logger.printLogs("[DEBUG]","Recommendations Metrics rolled upto {} manager query result {}".format(manager, insert_older_recommendation_result))


			insert_recommended_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_near_sla_query.format(SAS_Config.sas_aggregate_table, manager))
			Logger.printLogs("[DEBUG]","SLA near missed Metrics rolled upto {} manager query result {}".format(manager, insert_recommended_result))

			if (insert_total_result !=0 or insert_recommended_result != 0 or insert_older_recommendation_result != 0):
				Logger.printLogs("ERROR","Aggregation Query Execution failed for manager {}".format(manager))
				sys.exit(1)



	def insert_success_refresh_status(self):
		'''
		Updates the refresh status when plug-in runs successfullly.
		'''

		try:
			next_run_date = date.today() + relativedelta(days=7)
			insert_status = self.rs_upsert.execute_redshift_sql_query(SAS_Config.insert_status_of_sas_run.format(date.today(), date.today(),next_run_date))
			Logger.printLogs("DEBUG","Status Insertion Query result {}".format(insert_status))

		except Exception as error:
			Logger.printLogs("ERROR","Inserting refresh status failed with message: {}".format(error))
			sys.exit(1)

	def insert_sas_data_into_duplicate_table(self):
		'''
        Inserting data into respective SAS tables for Manual OE risk update in Amazon Arik portal.
        '''
		try:
			#Adding SAS Apollo data to duplicate table for Manual update of SAS Risks [AmazonArik OE Risk]
			insert_status_apollo = self.rs_upsert.execute_redshift_sql_query(SAS_Config.sas_temp_apollo_insert_query)
			Logger.printLogs("INFO","Data update to duplicate apollo sas table '{}' completed successfully {}.".format(SAS_Config.redshift_apollo_table_v2, insert_status_apollo))

			#Adding SAS Brazil data to duplicate table for Manual update of SAS Risks [AmazonArik OE Risk]
			insert_status_brazil = self.rs_upsert.execute_redshift_sql_query(SAS_Config.sas_temp_brazil_insert_query)
			Logger.printLogs("INFO","Data update to duplicate brazil sas table '{}' completed successfully {}.".format(SAS_Config.redshift_brazil_table_v2, insert_status_brazil))

			#Adding SAS Pipeline Target data to duplicate table for Manual update of SAS Risks [AmazonArik OE Risk]
			insert_status_pipeline_target = self.rs_upsert.execute_redshift_sql_query(SAS_Config.sas_temp_pipeline_target_insert_query)
			Logger.printLogs("INFO","Data update to duplicate Pipeline target sas table '{}' completed successfully {}.".format(SAS_Config.redshift_pipeline_target_table_v2, insert_status_pipeline_target))

		except Exception as error:
			Logger.printLogs("ERROR","Inserting into duplicate table status failed with message: {}".format(error))
			sys.exit(1)

def main():
	obj = SAS_Plugin()
	_client = obj.SAS_Init()
	parser = argparse.ArgumentParser(description='Getting the Snapshot type of SAS Plug-in')
	parser.add_argument('-s', '--snapshot',  nargs="*", help='Plugin Snapshot Type: WoW / MoM', required=True)
	args = vars(parser.parse_args())
	metric_snapshot = args['snapshot'][0]
	obj.create_metrics(_client, metric_snapshot)

if __name__ == "__main__":
	main()


class SAS_Config:

	region = "us-west-2"

	security_token = None

	service = "sas-rags-frontend-prod"

	endpoint = "https://sas-rags-frontend-prod-global-pdx.pdx.proxy.amazon.com"
	
	sas_aggregate_table = "aggregate_schema.sas_risk_summary"

	sas_apollo_temp_s3_location = "P0_metrics_raw_data/SAS/SAS_Apollo_File.csv"

	sas_brazil_temp_s3_location = "P0_metrics_raw_data/SAS/SAS_Brazil_File.csv"

	sas_pipeline_target_temp_s3_location = "P0_metrics_raw_data/SAS/SAS_Pipeline_Target_File.csv"

	sas_apollo_temp_file = "/apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Apollo_File.csv"

	sas_brazil_temp_file = "/apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Brazil_File.csv"

	sas_pipeline_target_temp_file = "/apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Pipeline_Target_File.csv"

	sas_status_query = """insert into rawdata_schema.page0_plugin_run_status values('{}','{}','{}','{}','{}','{}','{}','{}','{}','{}')"""

	redshift_apollo_table = "rawdata_schema.raw_sas_apollo_metrics"

	redshift_brazil_table = "rawdata_schema.raw_sas_brazil_metrics"

	redshift_pipeline_target_table = "rawdata_schema.raw_sas_pipeline_target_metrics"

	redshift_apollo_table_v2 = "rawdata_schema.sas_apollo_metrics_v2"

	redshift_brazil_table_v2 = "rawdata_schema.sas_brazil_metrics_v2"

	redshift_pipeline_target_table_v2 = "rawdata_schema.sas_pipeline_target_metrics_v2"

	temp_redshift_apollo_table = "rawdata_schema.temp_sas_apollo_metrics"

	temp_redshift_brazil_table = "rawdata_schema.temp_sas_brazil_metrics"

	temp_redshift_pipeline_target_table = "rawdata_schema.temp_sas_pipeline_target_metrics"

	rolled_redshift_apollo_table = "rawdata_schema.sas_apollo_metrics"

	rolled_redshift_brazil_table = "rawdata_schema.sas_brazil_metrics"

	rolled_redshift_pipeline_target_table = "rawdata_schema.sas_pipeline_target_metrics"

	program_name_apollo = 'APOLLO'

	program_name_brazil = 'BRAZIL'

	program_name_pipeline_targets = 'PIPELINE_TARGET'

	pipeline_target_function_flag = 'PIPELINE_TARGET_FUNCTION'

	BATCH_SIZE = 100

	query_for_getting_required_data = """insert into {} select distinct * from {}"""

	get_distinct_leader = "select distinct leader from relationship_schema.p0_clients_primary"

	insert_status_of_sas_run = """ 
	insert into rawdata_schema.page0_plugin_run_status 
	values('ALLORG','ALLManager','','SAS','SAS Risks',0,'{}','{}','Weekly','{}');
	"""

	manager_to_manager_aggregate_mapping_query = """
	select relation_entity,relation_manager FROM 
	relationship_schema.entity_manager_relationship WHERE relation_entity IN (SELECT DISTINCT relation_manager FROM 
	relationship_schema.entity_manager_relationship WHERE relation_entity IN {0}) and 
	relation_rundate in (select max(relation_rundate) from relationship_schema.entity_manager_relationship)"""

	roll_upto_manager_query = """
	insert into {0}
	select *,
		   '{1}' AS rolled_upto
	FROM {2}
	WHERE owner_login IN (SELECT DISTINCT login_name
					  FROM BOOKER.D_EMPLOYEE_DETAILS
					  WHERE supervisor_login_name IN (SELECT DISTINCT relation_manager
													  FROM relationship_schema.entity_manager_relationship
													  WHERE relation_entity = '{1}'))
	UNION all
	SELECT *,
		   '{1}' AS rolled_upto
	FROM {2}
	WHERE owner_login = '{1}'
	"""

	aggregate_roll_upto_total_query ="""
	insert into {0} (
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'total' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from rawdata_schema.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from rawdata_schema.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_brazil_metrics) and aggregate_roll_up_to in
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))
	union all
	select pipeline_target_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,pipeline_target_action_type as action_type
	from rawdata_schema.sas_pipeline_target_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_pipeline_target_metrics) and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}')))
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_rolled_upto,metric_year,metric_snapshot_number
	union all
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'total' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from rawdata_schema.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from rawdata_schema.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_brazil_metrics) and aggregate_roll_up_to in ('{1}')
	union all
	select pipeline_target_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,pipeline_target_action_type as action_type
	from rawdata_schema.sas_pipeline_target_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_pipeline_target_metrics) and aggregate_roll_up_to in ('{1}'))
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_roll_up_to,metric_year,metric_snapshot_number)
	"""

	aggregate_roll_upto_near_sla_query = """
	insert into {0} (
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'75DaySASRisk' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from rawdata_schema.sas_apollo_metrics where metric_run_date = (select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))  and 
		cast(days_non_compliant_recommendation as int) >= 20 and 
		cast(days_non_compliant_recommendation as int) < 30
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from rawdata_schema.sas_brazil_metrics where metric_run_date = (select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}')) and 
		cast(days_non_compliant_recommendation as int) >= 20 and 
		cast(days_non_compliant_recommendation as int) < 30)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_rolled_upto,metric_year,metric_snapshot_number
	union all
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'75DaySASRisk' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from rawdata_schema.sas_apollo_metrics where metric_run_date = (select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) >= 20 and 
		cast(days_non_compliant_recommendation as int) < 30
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from rawdata_schema.sas_brazil_metrics where metric_run_date = (select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) >= 20 and 
		cast(days_non_compliant_recommendation as int) < 30)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_roll_up_to,metric_year,metric_snapshot_number)
	"""

	aggregate_roll_upto_recommendation_query = """
	insert into {0} (
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'recommendations' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from rawdata_schema.sas_apollo_metrics where metric_run_date = (select max(metric_run_date) from rawdata_schema.sas_apollo_metrics)
	and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))  and 
		cast(days_non_compliant_recommendation as int) > 0
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from rawdata_schema.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}')) and 
		cast(days_non_compliant_recommendation as int) > 0)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_rolled_upto,metric_year,metric_snapshot_number
	union all
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'recommendations' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from rawdata_schema.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) > 0
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from rawdata_schema.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) > 0)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_roll_up_to,metric_year,metric_snapshot_number)
	"""

	aggregate_roll_upto_long_recommendation_query = """
	insert into {0} (
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'90DaySASRisk' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from rawdata_schema.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))  and 
		cast(days_non_compliant_recommendation as int) > 30
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from rawdata_schema.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_brazil_metrics) and aggregate_roll_up_to in
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}')) and 
		cast(days_non_compliant_recommendation as int) > 30)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_rolled_upto,metric_year,metric_snapshot_number
	union all
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'90DaySASRisk' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from rawdata_schema.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) > 30
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from rawdata_schema.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_brazil_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) > 30)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_roll_up_to,metric_year,metric_snapshot_number)
	"""

	'''
	Insert query to copy retrieve data from parent table to new table with Max(rundate)
	for Manual update of OE Risks by SE/Dev in Arik portal project.

	Master ticket - https://issues.amazon.com/issues/AEESE-3612
	'''

	sas_temp_apollo_insert_query = """
    INSERT INTO rawdata_schema.sas_apollo_metrics_v2 (apollo_environment_name, apollo_environment_stage, pipeline_id,
    pipeline_name, owner, owner_login, consumers, children, software_count, visibility, host_count, days_non_compliant,
    days_non_compliant_recommendation, apollo_action, blocked_resources, associated_campaigns, metric_run_date, metric_year,
    classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to) 
    SELECT apollo_environment_name, apollo_environment_stage, pipeline_id, pipeline_name, owner, owner_login, consumers,
    children, software_count, visibility, host_count, days_non_compliant, days_non_compliant_recommendation, apollo_action,
    blocked_resources, associated_campaigns, metric_run_date, metric_year, classification, metric_snapshot, metric_snapshot_number,
    aggregate_roll_up_to FROM rawdata_schema.sas_apollo_metrics
    WHERE metric_run_date = (SELECT MAX(metric_run_date) FROM rawdata_schema.sas_apollo_metrics)
    """

	sas_temp_brazil_insert_query = """
	INSERT INTO rawdata_schema.sas_brazil_metrics_v2 (brazil_versionset_name, pipeline_id, pipeline_name, owner, owner_login,
	software_count, days_non_compliant, days_non_compliant_recommendation, blocked_resource, brazil_action_type, associated_campaigns,
	metric_run_date, metric_year, classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to) 
    SELECT brazil_versionset_name, pipeline_id, pipeline_name, owner, owner_login, software_count, days_non_compliant,
    days_non_compliant_recommendation, blocked_resource, brazil_action_type, associated_campaigns, metric_run_date, metric_year,
    classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to FROM rawdata_schema.sas_brazil_metrics
    WHERE metric_run_date = (SELECT MAX(metric_run_date) FROM rawdata_schema.sas_brazil_metrics)
    """

	sas_temp_pipeline_target_insert_query = """
	INSERT INTO rawdata_schema.sas_pipeline_target_metrics_v2 (pipeline_target_name, pipeline_id, pipeline_name, owner, owner_login,
	software_count, days_non_compliant, days_non_compliant_recommendation, blocked_resource, pipeline_target_action_type, associated_campaigns,
	metric_run_date, metric_year, classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to) 
    SELECT pipeline_target_name, pipeline_id, pipeline_name, owner, owner_login, software_count, days_non_compliant,
    days_non_compliant_recommendation, blocked_resource, pipeline_target_action_type, associated_campaigns, metric_run_date, metric_year,
    classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to FROM rawdata_schema.sas_pipeline_target_metrics
    WHERE metric_run_date = (SELECT MAX(metric_run_date) FROM rawdata_schema.sas_pipeline_target_metrics)
    """


explain the workflow of sas plugin in detail with each function and call hiererachy and what it returns and from where 

------

Cedric: I'll explain the workflow of the SAS (Security Automation Service) Plugin in detail:

Main Workflow Hierarchy:

1. **Entry Point: main()**
- Creates SAS_Plugin instance
- Initializes SAS client via SAS_Init()
- Parses command line arguments for metric snapshot type (WoW/MoM)
- Calls create_metrics() with client and snapshot type

2. **create_metrics()**
Key orchestration function that manages the entire workflow:

```
create_metrics()
 get_entities() -> Gets client names from config
 get_distinct_managers() -> Gets employees under those clients
 Process data in batches for each manager:
    call_get_high_level_resource_details() -> Gets high-level SAS risk data
    process_high_level_data() -> Processes data for each resource type
    call_batch_details_wrapper() -> Handles detailed data collection
 Upload data to S3 and RedShift
 roll_up_metrics() -> Rolls up metrics by manager
 aggregate_metrics() -> Creates aggregate metrics
 insert_success_refresh_status() -> Updates refresh status
 insert_sas_data_into_duplicate_table() -> Updates duplicate tables
```

Key Function Details:

3. **Data Collection Functions:**

- **call_get_high_level_resource_details()**
  - Makes API call to get high-level SAS risk details
  - Returns data about blocked risks for a manager
  - Uses retry mechanism for throttling

- **process_high_level_data()**
  - Processes results for each resource type (Apollo/Brazil/Pipeline)
  - Returns consolidated list of environments
  - Extracts environment names from JSON response

- **call_batch_details_wrapper()**
  - Manages batch processing (100 items per batch)
  - Calls call_batch_details() for actual API calls
  - Routes data to appropriate processing function

4. **Data Processing Functions:**

- **get_apollo_data_and_write_in_file()**
- **get_brazil_data_and_write_in_file()**
- **get_pipeline_target_data_and_write_in_file()**
  - Process specific resource type data
  - Write formatted data to respective CSV files
  - Handle different data structures for each type

5. **Data Storage Functions:**

- **load_SAS_data_to_redshift()**
  - Manages RedShift table operations
  - Truncates existing tables
  - Loads new data from S3
  - Updates final tables with unique data

6. **Metric Aggregation Functions:**

- **roll_up_metrics()**
  - Rolls up metrics by manager hierarchy
  - Updates rolled-up tables for each resource type
  - Returns status of roll-up operations

- **aggregate_metrics()**
  - Creates aggregate metrics from rolled-up data
  - Handles different metric types (total, recommendations, etc.)
  - Updates aggregate tables

7. **Status Update Functions:**

- **insert_success_refresh_status()**
  - Updates plugin run status
  - Sets next run date
  - Records successful completion

- **insert_sas_data_into_duplicate_table()**
  - Updates duplicate tables for manual OE risk updates
  - Maintains separate copies for Apollo/Brazil/Pipeline data

Data Flow:
1. Raw data from SAS API
2. Processed into CSV files
3. Uploaded to S3
4. Loaded into RedShift raw tables
5. Rolled up by manager hierarchy
6. Aggregated into final metrics
7. Status updates and duplicate table maintenance

The plugin uses various configuration settings from SAS_Config class and handles errors with retries and logging throughout the process.

------

venry: explain data collection functions in details

------

Cedric: Let's break down the data collection functions in detail:

1. **call_get_high_level_resource_details()**
```python
@retry(wait_fixed=10000, stop_max_attempt_number=3)
def call_get_high_level_resource_details(self, manager, _client):
    try:
        # Makes API call to get BLOCKED campaign type risks for a manager
        data = _client.get_resource_high_level_risk_details_by_campaign_type(
            aggregate_id = "Person:" + manager, 
            campaign_type='BLOCKED'
        )
    except ThrottlingException as e:
        Logger.printLogs("EXCEPTION", "ThrottlingException occurred, Sleeping!")
        time.sleep(5)
    except Exception as err:
        Logger.printLogs("[ERROR]", "Error occurred {}".format(err))
        return 1
    return data
```
Key aspects:
- Uses retry decorator for resilience
- Takes manager ID and client as input
- Gets high-level blocked risks data
- Handles throttling with sleep
- Returns raw API response data

2. **process_high_level_data()**
```python
def process_high_level_data(self, result, resource_type):
    final_list_environments = []
    for environment in result:
        environment_json_info = self.coral_to_python(environment)
        env_name = environment_json_info['ResourceName']
        Logger.printLogs("[DEBUG]", 
            "{} environment {} added to list".format(resource_type, env_name))
        final_list_environments.append(env_name)
    return final_list_environments
```
Key aspects:
- Processes data for specific resource type (Apollo/Brazil/Pipeline)
- Converts Coral format to Python using coral_to_python()
- Extracts environment names
- Returns consolidated list of environment names

3. **call_batch_details_wrapper()**
```python
def call_batch_details_wrapper(self, manager, _client, final_list_of_environments, type_of_env):
    batch_iterations = len(final_list_of_environments)// SAS_Config.BATCH_SIZE +1
    
    try:
        for start_range in range(0, len(final_list_of_environments), SAS_Config.BATCH_SIZE):
            end_range = start_range + SAS_Config.BATCH_SIZE
            
            # Get detailed data for current batch
            consolidated_data = self.call_batch_details(
                manager, 
                _client, 
                final_list_of_environments[start_range:end_range], 
                type_of_env
            )
            
            # Route data to appropriate processor based on type
            if type_of_env == SAS_Config.program_name_apollo and consolidated_data != 1:
                self.get_apollo_data_and_write_in_file(
                    consolidated_data, 
                    self.apollo_file, 
                    self.metric_snapshot
                )
            elif type_of_env == SAS_Config.program_name_brazil and consolidated_data != 1:
                self.get_brazil_data_and_write_in_file(
                    consolidated_data, 
                    self.brazil_file, 
                    self.metric_snapshot
                )
            elif type_of_env == SAS_Config.program_name_pipeline_targets and consolidated_data != 1:
                self.get_pipeline_target_data_and_write_in_file(
                    consolidated_data, 
                    self.pipeline_file, 
                    self.metric_snapshot
                )
    except Exception as err:
        Logger.printLogs("[ERROR]", "Error occurred {}".format(err))
        sys.exit(1)
```
Key aspects:
- Manages batch processing (100 items per batch)
- Coordinates data collection and processing
- Routes data to appropriate file writer
- Handles errors with logging and exit

4. **call_batch_details()**
```python
@retry(wait_fixed=10000, stop_max_attempt_number=3)
def call_batch_details(self, manager, _client, final_list_of_environments, type_of_env):
    try:
        if (type_of_env == SAS_Config.program_name_apollo):
            data = _client.batch_get_apollo_environment_stage_details(
                requester_login = manager, 
                entity_id_list=final_list_of_environments, 
                campaign_type='BLOCKED'
            )
        elif (type_of_env == SAS_Config.program_name_brazil):
            data = _client.batch_get_brazil_version_set_details(
                requester_login = manager, 
                entity_id_list=final_list_of_environments, 
                campaign_type='BLOCKED'
            )
        elif (type_of_env == SAS_Config.program_name_pipeline_targets):
            data = _client.batch_get_pipeline_target_resource_details(
                requester_login = manager, 
                entity_id_list = final_list_of_environments, 
                campaign_type = 'BLOCKED'
            )
    except ThrottlingException as error:
        Logger.printLogs("EXCEPTION", "ThrottlingException occurred, Sleeping!")
        time.sleep(5)
    except Exception as err:
        Logger.printLogs("[ERROR]", "Error occurred {}".format(err))
        return 1
    return data
```
Key aspects:
- Makes specific API calls based on resource type
- Uses retry mechanism for resilience
- Handles throttling with sleep
- Returns detailed data for batch of environments

5. **Data Writing Functions**

Example for Apollo (similar pattern for Brazil and Pipeline):
```python
def get_apollo_data_and_write_in_file(self, data, file, metric_snapshot):
    list_data = self.coral_to_python(data)
    for filtered_depth_data in list_data['Details']:
        # Extract data points
        env_name = filtered_depth_data['Name']
        stage_name = env_name.split(':')[1]
        owner_login = filtered_depth_data['OwnerLogin']
        # ... more field extractions ...
        
        # Process recommendations
        recommendation_with_action = {}
        for recommendation in filtered_depth_data['Recommendations']:
            for target_resource in recommendation['AssociatedTargetResources']:
                recommendation_with_action[target_resource] = recommendation['Template']
        
        # Process software risks
        for software_risk in filtered_depth_data['SoftwareRisks']:
            for risk_details in software_risk['SoftwareRiskDetails']:
                # Extract risk specific details
                blocked_resources = risk_details['TargetResource']
                action_required = recommendation_with_action[blocked_resources]
                # ... more risk processing ...
                
                # Write formatted data to file
                file.write("{},{},{},{},...\n".format(
                    env_name,
                    stage_name,
                    pipeline_id,
                    # ... more fields ...
                ))
```
Key aspects:
- Processes complex nested data structures
- Extracts relevant metrics and details
- Handles recommendations and risks
- Writes formatted CSV data
- Includes metadata like dates and snapshots

The data collection functions work together to:
1. Get high-level risk data for managers
2. Process and organize environment information
3. Collect detailed data in batches
4. Transform data into required format
5. Write to appropriate files for later processing

This creates a robust pipeline that handles API throttling, retries, and different data structures while maintaining data integrity and proper error handling.

------

venry: what other keydetails that i should know about this plugin

------

Cedric: Here are several key details about the SAS Plugin that are important to understand:

1. **Configuration and Dependencies**
```python
# Key External Dependencies
from coral import coralrpc
from pyodinhttp import odin_retrieve_pair
from retrying import retry
from dateutil.relativedelta import relativedelta

# Important Config Classes
- SAS_Config: Contains all configuration parameters
- Common_Constants: General constants
- ODIN: Authentication and access details
```

2. **Security & Authentication**
```python
def SAS_Init(Self):
    # Secure credential handling
    aws_access_key_id, aws_secret_access_key = odin_retrieve_pair(ODIN.AEE_ODIN)
    # Uses AWS v4 signing
    _orchestrator = coralrpc.new_orchestrator(
        endpoint=SAS_Config.endpoint,
        signature_algorithm='v4',
        aws_access_key=aws_access_key_id,
        aws_secret_key=aws_secret_access_key
    )
```

3. **Data Storage Architecture**
```python
# Multiple Table Types
- Raw Data Tables (temp_redshift_*_table)
- Rolled-up Tables (rolled_redshift_*_table)
- Aggregate Tables (sas_aggregate_table)
- Duplicate Tables (*_v2) for manual updates

# Storage Flow
Raw Data  S3  Temp Tables  Main Tables  Rolled-up  Aggregated
```

4. **Error Handling & Resilience**
```python
# Retry Mechanism
@retry(wait_fixed=10000, stop_max_attempt_number=3)
def call_batch_details(...)

# Throttling Handling
except ThrottlingException as error:
    Logger.printLogs("EXCEPTION", "ThrottlingException occurred")
    time.sleep(5)

# Critical Error Exit
if select_result == 1:
    Logger.printLogs("EXCEPTION", "Query resulted in exception")
    sys.exit(1)
```

5. **Performance Optimizations**
```python
# Batch Processing
BATCH_SIZE = 100  # Limits API calls
# Efficient Data Loading
- Uses TRUNCATE before loading
- Bulk inserts via S3
- Manages duplicate data
```

6. **Metric Types and Calculations**
```python
# Different Risk Categories
- Total Risks
- 75-Day SAS Risks
- 90-Day SAS Risks
- Recommendations

# Time-based Metrics
- WoW (Week over Week)
- MoM (Month over Month)
```

7. **Important Business Rules**
```python
# Risk Age Classifications
- Regular Risks
- Near SLA (75-day) risks: 20-30 days
- Long-term (90-day) risks: > 30 days

# Aggregation Levels
- Individual Resources
- Manager Level
- Organization Level
```

8. **File Management**
```python
# Temporary Files
self.apollo_file = open(SAS_Config.sas_apollo_temp_file,"w")
self.brazil_file = open(SAS_Config.sas_brazil_temp_file,"w")
self.pipeline_file = open(SAS_Config.sas_pipeline_target_temp_file,"w")

# S3 Locations
sas_apollo_temp_s3_location = "P0_metrics_raw_data/SAS/SAS_Apollo_File.csv"
```

9. **Monitoring & Logging**
```python
# Status Tracking
- Runs status updates
- Next run date calculations
- Success/failure logging

# Debug Information
Logger.printLogs("DEBUG", "Query result {}".format(select_result))
```

10. **Integration Points**
```python
# External Systems
- SAS Risk Aggregation Service
- RedShift Database
- S3 Storage
- ODIN Authentication
- Booker Employee Database
```

11. **Important Limitations**
- API throttling constraints
- Batch size limitations
- Processing time windows
- Data retention policies

12. **Maintenance Considerations**
```python
# Regular Tasks
- Table cleanup
- Status monitoring
- Error log review
- Performance monitoring

# Update Requirements
- Config updates
- API version changes
- Schema modifications
```

13. **Best Practices**
```python
# Data Integrity
- Validate input data
- Check for duplicates
- Maintain data consistency
- Handle NULL values

# Performance
- Use batch processing
- Implement retries
- Handle throttling
- Optimize queries
```

14. **Common Issues and Solutions**
```python
# Common Problems
- API throttling
- Data inconsistencies
- Processing delays
- Authentication failures

# Solutions
- Retry mechanisms
- Data validation
- Error logging
- Monitoring alerts
```

15. **Future Considerations**
- Scalability requirements
- API version updates
- New risk types
- Performance improvements
- Additional metrics

Understanding these aspects is crucial for:
- Maintaining the plugin
- Troubleshooting issues
- Making modifications
- Understanding dependencies
- Ensuring reliable operation
- Planning updates

------

venry: issue :

selrm
2023-11-07 11:49:22 AM (UTC+05:30)
As discussed in oncall handover, created backlog https://sim.amazon.com/issues/AEESE-7406 , resolving ticket
Correspondence
selrm
2023-11-03 2:14:21 PM (UTC+05:30)
Discussed on handover and page0 call , this task will be taken as independent page0 task and is already tracked in https://quip-amazon.com/p9QDApGd1RmH/aee-se-secondary-On-Call-Handover-2023, keeping this ticket open until this is assigned to someone
Correspondence
********
2023-10-16 2:42:27 PM (UTC+05:30)

@vbalired, One cannot identify the if the resource is NAWS/MAWS based on the SAS response.
Adding external API's to confirm the same will increase the runtime.
Correspondence
********
2023-10-16 2:40:34 PM (UTC+05:30)

Adding Sample response from SAS API for a respective owner -

{"individualCountList": 
  [
    {"entityRiskCountMap": 
       "BRAZIL_VERSION_SET": 
        {"blockingRiskCount": 0, 
         "containingEntitiesTotalCount": 0, 
         "containingEntitiesWithRecommendation": 0, 
         "monthOverMonth": 0, 
         "totalRiskCount": 0, 
         "totalRiskCountWithRecommendation": 0, 
         "weekOverWeek": 0, 
         "yearToDate": 0}, 
       "AWS_LAMBDA_FUNCTION": 
        {"blockingRiskCount": 0, 
         "containingEntitiesTotalCount": 0, 
         "containingEntitiesWithRecommendation": 0, 
         "monthOverMonth": 0, 
         "totalRiskCount": 0, 
         "totalRiskCountWithRecommendation": 0, 
         "weekOverWeek": 0, 
         "yearToDate": 0},
       "AMAZON_PIPELINE_TARGET":    
        {"blockingRiskCount": 0, 
         "containingEntitiesTotalCount": 0, 
         "containingEntitiesWithRecommendation": 0, 
         "monthOverMonth": 0, 
         "totalRiskCount": 0, 
         "totalRiskCountWithRecommendation": 0, 
         "weekOverWeek": 0, 
         "yearToDate": 0}, 
       "APOLLO_ENVIRONMENT_NAME_AND_STAGE": 
        {"blockingRiskCount": 0, 
         "containingEntitiesTotalCount": 1, 
         "containingEntitiesWithRecommendation": 0, 
         "monthOverMonth": -3, 
         "totalRiskCount": 1, 
         "totalRiskCountWithRecommendation": 0, 
         "weekOverWeek": 0, 
         "yearToDate": -3}
      }, 
     "ownerLogin": "********", 
     "ownerName": "******* Joshi"}
  ], 
 "managerLogin": "spotnis", 
 "managerName": "Suyog Potnis", 
 "ownerLogin": "********", 
 "ownerName": "*************", 
 "totalCount": 
   "BRAZIL_VERSION_SET": 
    {"blockingRiskCount": 0, 
     "containingEntitiesTotalCount": 0, 
     "containingEntitiesWithRecommendation": 0, 
     "monthOverMonth": 0, 
     "totalRiskCount": 0, 
     "totalRiskCountWithRecommendation": 0, 
     "weekOverWeek": 0, 
     "yearToDate": 0}, 
    "AWS_LAMBDA_FUNCTION": 
        {"blockingRiskCount": 0, 
         "containingEntitiesTotalCount": 0, 
         "containingEntitiesWithRecommendation": 0, 
         "monthOverMonth": 0, 
         "totalRiskCount": 0, 
         "totalRiskCountWithRecommendation": 0, 
         "weekOverWeek": 0, 
         "yearToDate": 0},
    "AMAZON_PIPELINE_TARGET":    
        {"blockingRiskCount": 0, 
         "containingEntitiesTotalCount": 0, 
         "containingEntitiesWithRecommendation": 0, 
         "monthOverMonth": 0, 
         "totalRiskCount": 0, 
         "totalRiskCountWithRecommendation": 0, 
         "weekOverWeek": 0, 
         "yearToDate": 0}, 
   "APOLLO_ENVIRONMENT_NAME_AND_STAGE": 
    {"blockingRiskCount": 0, 
     "containingEntitiesTotalCount": 1, 
     "containingEntitiesWithRecommendation": 0, 
     "monthOverMonth": -3, 
     "totalRiskCount": 1, 
     "totalRiskCountWithRecommendation": 0, 
     "weekOverWeek": 0, 
     "yearToDate": -3}
  }
}

adding High level data for a risk -

{
    "highLevelResourceDetailsByTypeMap": {
        "ApolloEnvironmentNameAndStage": [
            {
                "resourceName": "GG/PDX:Prod",
                "resourceType": "ApolloEnvironmentNameAndStage",
                "totalRiskCountWithAction": 2,
                "totalRiskCountWithoutAction": 0,
                "owner": "cheryl",
                "ownerName": "Cheryl Chen",
                "riskAgeWithAction": 36,
                "riskAgeWithoutAction": 0
            },
            {
                "resourceName": "SUS/PDX:Prod",
                "resourceType": "ApolloEnvironmentNameAndStage",
                "totalRiskCountWithAction": 3,
                "totalRiskCountWithoutAction": 1,
                "owner": "********",
                "ownerName": "Ricky Quan",
                "riskAgeWithAction": 30,
                "riskAgeWithoutAction": 10
            }
        ],
        "BrazilVersionSet": [
            {
                "resourceName": "GG/development",
                "resourceType": "BrazilVersionSet",
                "totalRiskCountWithAction": 0,
                "totalRiskCountWithoutAction": 1,
                "owner": "******",
                "ownerName": "Cheryl Chen",
                "riskAgeWithAction": 0,
                "riskAgeWithoutAction": 15
            }
        ],
        "CodeDeployDeploymentGroupName": [
            {
                "resourceName": "9243453641545:ca-central-1:SomeCodeDeployService-m-CodeDeployApp-3ZSCNEYM3HO0:SomeCodeDeployService-m-DeploymentGroup1-1440FEBJKB7GE",
                "resourceType": "CodeDeployDeploymentGroup",
                "totalRiskCountWithAction": 0,
                "totalRiskCountWithoutAction": 1,
                "owner": "******",
                "ownerName": "***********",
                "riskAgeWithAction": 0,
                "riskAgeWithoutAction": 15
            }
        ],
        "ECSTask": [
            {
                "resourceName": "3153921211059:us-west-2:Cluster:4058b32371fb41c7b1d81b642bdff2ad",
                "resourceType": "ECSTask",
                "totalRiskCountWithAction": 0,
                "totalRiskCountWithoutAction": 1,
                "owner": "******",
                "ownerName": "***********",
                "riskAgeWithAction": 0,
                "riskAgeWithoutAction": 15
            }
        ],
        "AmazonPipelineTarget": [
            {
                "resourceName": "us-west-2:210380261923:AWSSeerWorkflows-CodeDeployApp-3AFUSXCOXZ5G:AWSSeerWorkflows-DeploymentGroup1-CWJ0NGBUFW65:CD",
                "resourceType": "AmazonPipelineTarget",
                "totalRiskCountWithAction": 0,
                "totalRiskCountWithoutAction": 1,
                "owner": "******",
                "ownerName": "***********",
                "riskAgeWithAction": 0,
                "riskAgeWithoutAction": 15
            }
        ]
    }
} 

Correspondence
aws:sts::493682679357:assumed-role/SnowServiceRole-v3/snow-engine-fleet-dub-12012.dub2.amazon.com
2023-09-28 6:33:37 PM (UTC+05:30)

Resolver Group Page0 failed the Status Resolved SLA. Resolver Groups can configure their SLAs: https://w.amazon.com/bin/view/SIMTicketing/UserGuide/Sla/
Correspondence
atulusai
2023-09-26 6:26:49 PM (UTC+05:30)
We are not handling NAWS use case in SAS metrics. 
We can track this task separately.
Correspondence
aws:sts::493682679357:assumed-role/SnowServiceRole-v3/snow-engine-fleet-dub-14019.dub4.amazon.com
2023-09-26 4:38:30 PM (UTC+05:30)

Resolver Group Page0 failed the Status Out of Assigned SLA. Resolver Groups can configure their SLAs: https://w.amazon.com/bin/view/SIMTicketing/UserGuide/Sla/
Correspondence
vbalired
2023-09-26 4:11:18 PM (UTC+05:30)

NAWS campaigns have recommendation but are shown as suggestions in SAS portal 

issue: 

aishrav
2024-10-30 5:22:12 PM (UTC+05:30)
Time updated to absolute format: 2024-10-30 05:22:12 PM
(edited
109 days ago
)
Backlog Sim created for this ticket - https://sim.amazon.com/issues/AEESE-12096. Closing the ticket as discussed in oncall meeting.
Correspondence
ekaraja
2024-10-21 12:12:07 PM (UTC+05:30)
@thirusub
Thank you for sharing new dashboard and change in campaign tracking. 
we need to discuss and have scorecard and SAS metrics gets updated with Emergent, Critical and High instead of Blocked. 
Also will share  with the team. 

AI - 
1. check with Preetha/Sanjay if IST team are aligned to track this SAS risk as they dont include SAS metrics now
2. If yes, We will schedule a meeting and discuss to have on IST and Page0
3. If no, we will have a meeting within our ourself to have additional metrics for AEE  
Correspondence
thirusub
2024-10-18 3:56:47 PM (UTC+05:30)
(edited
121 days ago
)
Seems their definition of Blocked SAS Campaigns have now changed (i.e. no longer called as "Blocked Campaigns" that is what our Page-0 was reporting). Now they have different severity attached to the SAS risks. Refer the SAS team's quick-sight dashboard
https://us-east-1.quicksight.aws.amazon.com/sn/dashboards/7d092299-0571-48f8-a5be-d2f9ac8b1ef1?#p.Userlogin=thirusub&p.CP=2.%20Critical&p.CP=3.%20High&p.CP=1.%20Emergency

Suggestions:
1. We should have the team to focus on the top 3 Severity  - 1. Emergent, 2. Critical and 3. High (same as PE risks)
2. Secondly, for any severity, we should report and track the actionable risks i.e. SAS risk with recommended fix available. Otherwise, the team can't act or resolve them and it would defeat the purpose of reporting and mentioning OverSLA.

Next Steps:
However, we are moving to IST Page-0 starting 2025, the IST Page-0 don't include the SAS Risks metrics. So, first we would need this checked with Preetha/Sanjay i.e. IST teams are aligned to track this SAS risks as-well. If so, we could plan and add it for all IST teams, not just for AEE alone (and in the IST Page-0). But if there is no alignment at IST level, then we should re-group to figure out how we could have additional metrics for AEE.
Correspondence
system
2024-10-17 11:06:23 PM (UTC+05:30)

This issue will automatically resolve if we don't hear back from you soon.

More Info
Correspondence
gurumz
2024-10-16 7:33:06 PM (UTC+05:30)
(edited
123 days ago
)
In page-0 dashboard, we need to show only what is expected from SDMs as mandatory OE goal. Earlier we were expected to resolve only Sas risks that has recommendations. Teams can feel free to go overboard and resolve risks without recommendations. 

@thirusub/@sajitha to confirm if the service owners are expected to resolve risks without recommendations as well? If no, then lets not add this to Page-0 scorecard. This will bring so much noise with no action to be taken.
Correspondence
ekaraja
2024-10-16 6:03:52 PM (UTC+05:30)
(edited
123 days ago
)

As Discussed with @atulusai, it is working as expected.
SAS Plugin shows Recommended sas risk count in score card for all the manager which is true as per cell definition - SAS Risks - Open (Recommended fixes in Blocked Campaign)
If any manager wants all blocked SAS Risk count including non recommended one, it is available in SAS trend.

If there is a request to have all blocked count in score card, we will have separate backlog to work on.
@******** shared mail yesterday to get suggestion from team. we will follow up on that and will create backlog if it is require.

As per below definition:

What are Recommended SAS risks?
SAS has identified recommendations to address software risks. The risks have immediate actions that you can take.

What are Non Recommended SAS risks?
No recommendation means there is no one-click recommended action you can take to fix the risk. We will need to perform additional research to determine how to remove the risks.

@thirusub
could you please confirm if we need to have all blocked count instead of recommended alone.
Correspondence
********
2024-10-14 9:00:43 PM (UTC+05:30)

I will discuss this during handover.
Correspondence
atulusai
2024-10-14 8:40:48 PM (UTC+05:30)
(edited
125 days ago
)
Current Behavior

    Page 0 scorecard - ********@

    As per logic we are showing data for SAS risks which are part of Blocked campaign and have Recommendation in scorecard tab. As all the 8 risks are part of Blocked campaign but with no recommendations, so data is appearing as '0'.

    SAS Plugin Tab in Page 0 scorecard - ********@

As per the above screenshot, we have 2 metrics in SAS risk tab:

    Metrics 1 showing data for SAS risks which are part of Blocked campaign and without any filter of Recommendation.
    Metrics 2 showing data for SAS risks which are part of Blocked campaign and have Recommendation filter.

As all the 8 risks are part of Blocked campaign but without recommendation so we don't have data/metrics in Metrics 2.
Expected Behavior.

I believe we should display data for SAS risks which are part of Blocked campaign without any filter of Recommendation.
As both risks are part of blocked campaigns

As per below definition:

    What are Recommended SAS risks?
    SAS has identified recommendations to address software risks. The risks have immediate actions that you can take.

    What are Non Recommended SAS risks?
    No recommendation means there is no one-click recommended action you can take to fix the risk. We will need to perform additional research to determine how to remove the risks.

Correspondence
********
********** 9:36:32 PM (UTC+05:30)
(edited
130 days ago
)
Data is matching for ******** and ******** with what we have in table below. 
https://sas.corp.amazon.com/summary/all/kottakat?campaignid=blocked
https://sas.corp.amazon.com/summary/all/zhaochen?campaignid=blocked
Correspondence
********
********** 6:45:41 PM (UTC+05:30)
(edited
130 days ago
)

Thank you @krispalk for analysis.

Also we have a ticket in past - https://t.corp.amazon.com/D83262213/communication, As per this the plugin is working as expected, the scorecard have the monthly data.

We can further verify this data

Correspondence
krispalk
********** 11:23:27 AM (UTC+05:30)

The code for the SAS Plugin is working as expected as mentioned below correspondence. We are good to resolve this ticket if there are no further action-items
Correspondence
krispalk
********** 10:18:05 PM (UTC+05:30)

From this code-pointer

    The call_get_high_level_resource_details(self, manager, _client) function makes an API call to retrieve high-level SAS risk details for a specific manager.

    This API is responsible for making an API call to retrieve high-level SAS risk details for a specific manager. The function fetches all risks marked as BLOCKED for the given manager via _client.get_resource_high_level_risk_details_by_campaign_type, passing the aggregate_id of the manager and the campaign_type as BLOCKED

    Based on the code logic, the function retrieves all SAS risks categorized as blocked for that particular manager.

    https://sas.corp.amazon.com/summary/blocked/amandeb, which is 0 for the blocked campaigns
    https://sas.corp.amazon.com/summary/blocked/nchitira, which is 0 for the blocked campaigns.

    Permission was only available to view and verify data for these two SDMs.

Conclusion

    The call_get_high_level_resource_details function effectively retrieves SAS risks marked as BLOCKED for a particular manager or SDM.
    However, during the verification process, no blocked risks were found for the tested managers, amandeb and nchitira.

Correspondence
vbalired
********** 2:28:02 PM (UTC+05:30)

@krispalk to update the findings
Correspondence
thirusub
2024-09-30 10:38:47 AM (UTC+05:30)
What is the latest update and next steps pls?
Correspondence
krispalk
2024-09-27 9:11:32 AM (UTC+05:30)

AEE-SE Primary checking-in

Ticket will be updated with more findings
Correspondence
aws:sts::968761973157:assumed-role/da-recon-ops-user-role/da-recon-ops-session
2024-09-09 12:29:17 PM (UTC+05:30)
TRI BOT Ticket Analysis
Summary

You are a bot responsible for adding helpful information to an open ticket related to verifying the SAS plugin data reflecting in Page 0 for most managers. Currently, the data is showing as 0.
At a glance

    Issue: The SAS plugin data is not reflecting correctly in Page 0 for most managers, showing as 0 instead.
    Current status: Open
    Root cause: The root cause is not provided in the ticket information.

Probable Resolution

Based on the limited information provided, there are no clear resolution steps from past similar tickets. However, some potential next steps could be:

    Investigate logs and recent deployments/configuration changes related to the SAS plugin or Page 0 data population to identify any potential issues or regressions.
    Verify if the issue is specific to certain managers, regions, or environments, and narrow down the scope.
    Check if there are any known issues or ongoing incidents related to the SAS plugin or Page 0 data population.
    Engage with the relevant teams responsible for the SAS plugin and Page 0 data population to collaborate on troubleshooting and resolution.

Since the root cause is not provided, it is difficult to suggest a specific resolution step. Further investigation and collaboration with the relevant teams would be necessary to identify the root cause and determine the appropriate resolution.
---
Screenshot 2024-10-09 at 6.41.57 PM.png

------

Cedric: Based on the ticket discussion and images provided, here's a summary of the SAS Plugin issue and changes needed:

Key Issues:
1. **Data Reporting Discrepancy:**
- Currently only showing SAS risks with recommendations in scorecard
- Not showing all blocked campaign risks
- Some managers showing 0 despite having blocked risks

2. **Campaign Classification Change:**
- SAS team has changed "Blocked Campaigns" classification
- Now using severity levels: Emergency, Critical, and High
- Similar to PE risk classification

3. **Current vs Expected Behavior:**
```python
# Current
- Shows only risks with recommendations in scorecard
- Separate metrics for:
  - Blocked campaign risks (total)
  - Blocked campaign risks with recommendations

# Expected
- Should show all blocked campaign risks without recommendation filter
- Need to update to track new severity levels:
  1. Emergency
  2. Critical 
  3. High
```

Required Changes:
1. **Plugin Code Updates:**
```python
# Current Risk Filter
campaign_type='BLOCKED'

# Needed Updates
- Add support for new severity levels
- Update risk classification logic
- Modify aggregation queries
- Update reporting metrics
```

2. **Metrics Collection:**
```python
def get_high_level_resource_details():
    # Need to modify to include:
    - Emergency risks
    - Critical risks  
    - High risks
    # Remove blocked campaign filter
```

3. **Integration Requirements:**
- Check IST team alignment for tracking SAS risks
- Update Page-0 scorecard metrics
- Modify aggregation logic
- Update dashboard reporting

Next Steps:
1. Create backlog for implementation (AEESE-12096)
2. Verify alignment with IST team for risk tracking
3. Plan metrics update for:
   - Emergency risks
   - Critical risks
   - High risks
4. Update plugin to remove recommendation-only filter
5. Modify dashboard to show new severity classifications

The changes will require significant updates to the SAS Plugin's data collection, processing, and reporting mechanisms to align with the new risk classification system.

------

venry: 
RiskAggregationService(RAGS) customer facing API
Primary Owner builder-tools (POSIX)
Last modified 1 year ago by jruzzi.

Software Assurance Services(SAS) is going to make changes to RAGS APIs related to NAWS risks as we are going to move NAWS(CodeDeploy and LambdaFunction) risk reporting to be Pipeline Target based reporting. See more details here

NOTICE

This RAGS API reference is now being published on BuilderHub. See https://builderhub.corp.amazon.com/docs/sas/api-guide/.

    RAGS customer facing API blog post
    https://builderhub.corp.amazon.com/blog/sasriskaggregationservicecustomerfacingapi/
    Request access to use RAGS Read APIs
    Calling RAGS Read APIs programmatically
        Using our Python/Ruby clients:
    Calling RAGS Read APIs programmatically - NAWS
    Calling RAGS Read APIs via coral explorer
    What are the throttling limits?
    Read API examples
        BatchGetAWSLambdaFunctionDetails
        BatchGetAWSLambdaFunctionVersionDetails
        BatchGetApolloEnvironmentStageDetails
        BatchGetBrazilVersionSetDetails
        BatchGetPipelineTargetResourceDetails
        BatchGetCodeDeployDeploymentGroupNameDetails
        BatchGetDownstreamAggregatesWithRiskByCampaignType
        BatchGetDownstreamAggregatesWithRiskByCampaignIds
        BatchGetElasticContainerServiceDetails
        BatchGetRiskSummaryCountByCampaignType
        BatchGetRiskSummaryCountByCampaignIds
        BatchGetWeeklyRiskSummaryCountByCampaignType
        GetPipelineTargetResourceDetails
        GetAWSLambdaFunctionDetails
        GetAWSLambdaFunctionVersionDetails
        GetAllCampaignsOfAggregate
        GetApolloEnvironmentStageDetails
        GetBrazilVersionSetDetails
        GetCodeDeployDeploymentGroupNameDetails
        GetDownstreamAggregatesWithRisk
        GetElasticContainerServiceDetails
        GetResourceHighLevelDetails
        GetResourceHighLevelRiskDetailsByCampaignType
        GetRiskSummaryCount
        GetWeeklyRiskSummaryCount
        GetResourceHighLevelRiskDetailsByPipelineId
    Common Use Cases
        List all containing entities (e.g. Apollo environment stages, Brazil version sets, etc.) that contains risks with recommendations in NON_ADVISORY campaign for my director's organization (directorlogin@) and the owner login
        List all Apollo environment with details (e.g recommendation, risk age, etc.) in NON_ADVISORY campaign for my director's orgnization (*************@) and the owner login
        Upcoming changes for NAWS risk related API

RAGS customer facing API blog post
https://builderhub.corp.amazon.com/blog/sasriskaggregationservicecustomerfacingapi/
Request access to use RAGS Read APIs

Please add your team to subscribe sas-rags-api-users email list so that we can reach out to you when there is new APIs available, or when changes in existing APIs.
Go to the permissions section of the SAS Dashboard. In your delegation request, be sure to request for the highest login your AWS account will need access for. Then enter the AWS accounts separated by comma under the Requester text field. Then, enter the employee logins under Requestee for which your AWS account will need risk data for. Next, provide a valid reason and expiry time and then click request. Once approved, your AWS account will be able to fetch risk data using RAGS for the requested employees and those below them.

Do I have to own the AWS account I want to request access for?
No, you do not need to own the AWS account you request access for. Ideally, your team should be the owner so that you can make calls using it.

How do we request permanent access for our AWS account?
You cannot request permanent access for any account or user. This is a part of our ongoing initiative to make sure requested resources are always in use. Once your delegation approaches its expiry time, you will remind you to renew it for uninterrupted access.

When my request gets approved, what do I get access to?
You will be able to make API to RAGS either programmatically and or by using the coral explorer for the requested employees risk data using the AWS accounts you provided.

If I have access to a manager, do I also have access to those under them?
Yes, if you have access to a manager, then you will have access to everyone who reports up to them.

I want to revoke the access I approved. Is there a way to do that?
No. In the current iteration, we do not have such functionality. However, in the next update, we will allow users to revoke any approved access to their data anytime. If you accidentally gave access and want to revoke it, please cut us a ticket here.

What if I want Andy Jassy level data?
If you would like Andy Jassy level data, a security review approved by app-sec would also be needed, the related approval should be included in the reason field of the request. Your request may take more time to be approved as compared to other requests because of the scope of the data requested.

I have successfully submitted the request, whats next?
After a request is submitted, an Amazon Approval request will be sent to the approvers via email, and they will need to approve the request for it to take effect.

Who will be my requests approvers?
If the persons job level you have requested data for to is lower than L10, the requested person will be assigned as the approver of the request. If the persons job level is L10 or higher, SAS-Access-Delegation-Approver group will be assigned as the approver of the request. You can also find the approvers and details in the Amazon Approval that is created for the request.

I have requested access to multiple employees at the same time, and I see that some of them have approved the delegation. Why dont I have access to those who accepted?
When you request delegation from multiple employees at once, they all have to approve the given delegation. For example, if you request access to both employee A and B in the same request, then both A and B have to approve the related delegation. Partial approval will not give access to either employee. If your delegation does not rely on approval from all requested parties, please make separate requests to each employee.

My request was approved but using the approved AWS account in the coral explorer says I don't have access. What do I do?
When you get access via https://sas.corp.amazon.com/v2/permissions, you get permission to access RAGS production API which does not have a coral explorer. The coral explorer you are likely using is the one for gamma. Gamma coral explorer cannot be used to access RAGS production API. To access RAGS production API, you must do so programmatically. If you would like to use the gamma coral explorer, you must get gamma permission on the gamma version of SAS website which can be found here: https://sas-gamma.corp.amazon.com/v2/

Need more help?
Feel free to reach out to SAS team for assistance by cutting us a SIM here.
Calling RAGS Read APIs programmatically

    Add the following to your service's dependency list:

    dependencies = {
        1.0 = {
                SASRiskAggregationServiceJavaClient = 1.0;
                SASRiskAggregationServiceClientConfig = 1.0;
        };
    }; 

    Build the RAGS Frontend Client with AwsCredentialsProvider and clientConfig:

    clientConfig values for different environments:
        For test usage: "Base.Frontend.AwsAuth.Gamma" (it's our gamma test stack which maps data on sas-gamma.corp.amazon.com)
        Prod: "Base.Frontend.AwsAuth.Prod" (real prod data, which maps data on sas.corp.amazon.com)

    public SASRiskAggregationServiceClient getRAGSFEClient(final String clientConfig,
                                                           final Identity identity) { // Aws account Identity 
        final CallVisitor authVisitor = call -> call.setIdentity(identity); 
        final ClientBuilder coralClientBuilder = new ClientBuilder(); 
        return coralClientBuilder .remoteOf(SASRiskAggregationServiceClient.class) 
            .withConfiguration(clientConfig) 
            .withCallVisitors(authVisitor) 
            .newClient(); 
    } 

    Use RAGS Frontend Client to make requests:

    final GetAllCampaignsOfAggregateRequest request = new GetAllCampaignsOfAggregateRequest
            .Builder()
            .withAggregateId("Person:somebody")
            .build();
    final GetAllCampaignsOfAggregateResponse response = frontendClient.callGetAllCampaignsOfAggregate(request); 

Using our Python/Ruby clients:

https://code.amazon.com/packages/SASRiskAggregationServicePythonClient/trees/mainline
https://code.amazon.com/packages/SASRiskAggregationServiceRubyClient/trees/mainline

Note that we do not have example calls and do not have plans to provide them.
Calling RAGS Read APIs programmatically - NAWS

RAGS has VPC-enabled via Allegiance and it can be accessed by clients in native AWS VPCs. Please follow Allegiance onboarding wiki to setup a VPC interface to call RAGS.

AWS team needs to get approval from AppSec engineer to use Allegiance and ask InfoSec to allowlist aws account. Here are example steps in AWS Builder Tools orgnization.

    Gamma: https://allegiance.corp.amazon.com/vpc_enable_status?targetDns=sas-rags-frontend-gamma-global-pdx.pdx.proxy.amazon.com&targetPort=443

Service Name: SASRiskAggregationService
Service Instance Name: SASRiskAggregationService-PDX-Gamma

    Prod: https://allegiance.corp.amazon.com/vpc_enable_status?targetDns=sas-rags-frontend-prod-global-pdx.pdx.proxy.amazon.com&targetPort=443

Service Name: SASRiskAggregationService
Service Instance Name: SASRiskAggregationService-PDX-Prod

Here is an example of one of our customers calling RAGS from Lambda: https://code.amazon.com/packages/JerrytuTestG2S2Caller/blobs/6748ce574144f13626cd48ad5639db0b4d535d61/--/src/com/amazon/jerrytutestg2s2caller/lambda/calculator/modules/ClientModule.java
Calling RAGS Read APIs via coral explorer

Here are steps to call RAGS Read API via coral explorer:

    Open coral explorer:
        Gamma: https://sas-rags-frontend-gamma-global-pdx.pdx.proxy.amazon.com/explorer/index.html
        Prod: Prod coral explorer endpoint is disabled due to security reason
    Choose one of the API on the left side and add input values;
    Choose correct signature and pass signature value:
        Choose "HttpSigning/v4" in the "Signature Version" choice;
        Access Key Id: access key of the odin material set of the allowlisted aws account;
        Secret Key : and secret key of the odin material set of the allowlisted aws account;
        service: "sas-rags-frontend-[evironment stage]";
        region: "us-west-2";
    Click "Call ***" at the bottom and view the output;

Please note: The response from our Gamma stack will be different from what you might see on your SAS dashboard as our Gamma stack is used for testing and have test data.
What are the throttling limits?

1 TPS per API per AWS account
Read API examples
BatchGetAWSLambdaFunctionDetails

NOTE: AWS Lamdba Function APIs are deprecated. Please use BatchGetPipelineTargetResourceDetails.

The API batch size limit is 100

Get details for the given list of AWSLambdaFunction entity Ids in the format: [awsAccountId]:[region]:[functionName]

Required input:

    entityIdList: A list of AWS Lambda function name; (Max entityIdList size we can support is 200)
    requesterLogin: requester login (usually use the same supervisor login in allowlisting SIM);

Optional input:

    campaignType: BLOCK/NON_ADVISORY/ADVISORY/ALL;
        BLOCK -- all blocked software campaigns;
        NON_ADVISORY -- all campaigns on the default landing page (including all BLOCK campaigns);
        ADVISORY -- all campaigns on the Advisory Insights tab;
        ALL -- all campaigns;
    campaignId: campaign id, for example "******************-*****************" for "Blocked Software Campaign - Dec 2018";

Note: if campaignType and campaignId are both null,  it will return risk details associsted with all campaigns; If campaignId is not null, it will return risk details only associated with the given campaign id; If campaignType is not null and campaignId is null, it will return risk details only associated with the given campaign type;

Note: It will return empty list if given entity ids don't have any risk associsted with given campaign id or campaign type;

Example Input:

{
  "entityIdList": [
    "6541234564:us-west-2:SomeLambdaFunctionName-12M4HB61C83TE"
  ],
  "requesterLogin": "login"
}

Example Output:

{
  "details": [
    {
      "account": {
        "accountId": "6541234564",
        "org": "AWS"
      },
      "associatedVersions": [
        "6541234564:us-west-2:SomeLambdaFunctionName-12M4HB61C83TE:99",
        "6541234564:us-west-2:SomeLambdaFunctionName-12M4HB61C83TE:95",
        "6541234564:us-west-2:SomeLambdaFunctionName-12M4HB61C83TE:96"
      ],
      "name": "6541234564:us-west-2:SomeLambdaFunctionName-12M4HB61C83TE",
      "owner": "Owner Name",
      "ownerLogin": "ownerLogin",
      "pipelineId": "12345",
      "pipelineName": "SomePipelineName"
    }
  ]
}

BatchGetAWSLambdaFunctionVersionDetails

NOTE: AWS Lamdba Function APIs are deprecated. Please use BatchGetPipelineTargetResourceDetails.

The API batch size limit is 100

Get version details for the given list of AWSLambdaFunction entity Ids in the format: [awsAccountId]:[region]:[functionName]:[functionVersion]

Required input:

    entityIdList: A list of AWS Lambda function version name; (Max size we can support is 200)
    requesterLogin: requester login (usually use the same supervisor login in allowlisting SIM);

Optional input:

    campaignType: BLOCK/NON_ADVISORY/ADVISORY/ALL;
        BLOCK -- all blocked software campaigns;
        NON_ADVISORY -- all campaigns on the default landing page (including all BLOCK campaigns);
        ADVISORY -- all campaigns on the Advisory Insights tab;
        ALL -- all campaigns;
    campaignId: campaign id, for example "******************-*****************" for "Blocked Software Campaign - Dec 2018";

Note: if campaignType and campaignId are both null,  it will return risk details associsted with all campaigns; If campaignId is not null, it will return risk details only associated with the given campaign id; If campaignType is not null and campaignId is null, it will return risk details only associated with the given campaign type;

Note: It will return empty list if given entity ids don't have any risk associsted with given campaign id or campaign type;

Example Input:

{
  "entityIdList": [
    "6541234564:us-west-2:SomeLambdaFunctionName-12M4HB61C83TE:99"
  ],
  "requesterLogin": "*****"
}

Example Output:

{
  "details": [
    {
      "account": {
        "accountId": "6541234564",
        "org": "AWS"
      },
      "name": "6541234564:us-west-2:SomeLambdaFunctionName-12M4HB61C83TE:99",
      "oldestRiskAge": 65,
      "owner": "Owner Name",
      "ownerLogin": "ownerName",
      "pipelineId": "12345",
      "pipelineName": "SomePipelineName",
      "riskCount": 3,
      "softwareRisks": [
        {
          "associatedCampaign": {
            "contact": "NA",
            "name": "OdinAWSCredential - Dry run campaign",
            "url": "NA"
          },
          "softwareRiskDetails": [
            {
              "riskAge": 63,
              "targetResource": "OdinAWSCredentials:1.4"
            }
          ]
        },
        {
          "associatedCampaign": {
            "contact": "contactInfo",
            "name": "campaignName",
            "url": "url about the campaign"
          },
          "softwareRiskDetails": [
            {
              "riskAge": 63,
              "targetResource": "RiskyPackage:1.0"
            },
            {
              "riskAge": 63,
              "targetResource": "RiskyPackage2:1.1"
            }
          ]
        }
      ]
    }
  ]
}

BatchGetApolloEnvironmentStageDetails

The API batch size limit is 100

Get environment details for the given Apollo environment stage entityIds in format: [EnvironmentName]:[Stage]

Required input:

    entityIdList: A list of apollo environment stage name; (Max size we can support is 200)
    requesterLogin: requester login (usually use the same supervisor login in allowlisting SIM);

Optional input:

    campaignType: BLOCK/NON_ADVISORY/ADVISORY/ALL;
        BLOCK -- all blocked software campaigns;
        NON_ADVISORY -- all campaigns on the default landing page (including all BLOCK campaigns);
        ADVISORY -- all campaigns on the Advisory Insights tab;
        ALL -- all campaigns;
    campaignId: campaign id, for example "******************-*****************" for "Blocked Software Campaign - Dec 2018";

Note: if campaignType and campaignId are both null,  it will return risk details associsted with all campaigns; If campaignId is not null, it will return risk details only associated with the given campaign id; If campaignType is not null and campaignId is null, it will return risk details only associated with the given campaign type;

Note: It will return empty list if given entity ids don't have any risk associsted with given campaign id or campaign type;

Example input:

{
  "entityIdList": [
    "ApolloEnvironmentName:Prod"
  ],
  "requesterLogin": "wyep"
}

Example Output:

{
  "details": [
    {
      "childrenCount": 0,
      "consumerCount": 0,
      "hostCount": 2,
      "name": "ApolloEnvironmentName:Prod",
      "oldestRiskAge": 117,
      "owner": "Owner name",
      "ownerLogin": "ownerLogin",
      "pipelineId": "12345",
      "pipelineName": "SomePipelineName",
      "recommendations": [
        {
          "associatedTargetResources": [
            "CoralRubyClientBuildLogic:1.0",
            "CoralTibcoValue:1.0",
            "CoralTransmutingProxy:1.0",
            "CoralBatchingSupport:1.0",
          ],
          "details": "{\"entity\":\"ApolloEnvironmentName:Prod\"}",
          "template": "APOLLO_NO_ACTION"
        },
        {
          "associatedTargetResources": [
            "Jackson-databind:2.8.x"
          ],
          "details": "{\"consumableEnvironmentsToUpdate\":[\"SomeConsumableEnvironmentName:Prod:105326079224\"],\"entity\":\"ApolloEnvironmentName:Prod\"}",
          "template": "APOLLO_CAN_UPDATE"
        }
      ],
      "riskCount": 1,
      "softwareRisks": [
        {
          "associatedCampaign": {
            "contact": "https://sim.amazon.com/issues/create?assignedFolder=dfio1291-5fcc-47fd-8089-eeb19123f46f",
            "name": "CDO Blocked Software Campaign",
            "url": "https://w.amazon.com/index.php/somepages"
          },
          "softwareRiskDetails": [
            {
              "consumers": [
                "ApolloEnvironmentStageRevision:SomeConsumableEnvironmentName:Prod:105326079224"
              ],
              "recommendationAge": 27,
              "riskAge": 117,
              "targetResource": "Jackson-databind:2.8.x"
            }
          ]
        }
      ],
      "visibility": "Internal"
    }
  ]
}

BatchGetBrazilVersionSetDetails

The API batch size limit is 100

Required input:

    entityIdList: A list of Brazil version set name; (Max size we can support is 200)
    requesterLogin: requester login (usually use the same supervisor login in allowlisting SIM);

Optional input:

    campaignType: BLOCK/NON_ADVISORY/ADVISORY/ALL;
        BLOCK -- all blocked software campaigns;
        NON_ADVISORY -- all campaigns on the default landing page (including all BLOCK campaigns);
        ADVISORY -- all campaigns on the Advisory Insights tab;
        ALL -- all campaigns;
    campaignId: campaign id, for example "******************-*****************" for "Blocked Software Campaign - Dec 2018";

Note: if campaignType and campaignId are both null,  it will return risk details associsted with all campaigns; If campaignId is not null, it will return risk details only associated with the given campaign id; If campaignType is not null and campaignId is null, it will return risk details only associated with the given campaign type;

Note: It will return empty list if given entity ids don't have any risk associsted with given campaign id or campaign type;

Example input:

{
  "entityIdList": [
    "SomeBrazilVersionSet/development"
  ],
  "requesterLogin": "wyep"
}

Example Output:

{
  "details": [
    {
      "name": "SomeBrazilVersionSet/development",
      "oldestRiskAge": 56,
      "owner": "Owenr Name",
      "ownerLogin": "ownerLogin",
      "recommendations": [
        {
          "actions": [],
          "brazilActionCount": 0,
          "hasFullActions": false,
          "targetResource": "CoralJavaBuildLogic:1.0"
        },
        {
          "actions": [],
          "brazilActionCount": 0,
          "hasFullActions": false,
          "targetResource": "Jackson-databind:2.8.x"
        }
      ],
      "riskCount": 1,
      "softwareRisks": [
        {
          "associatedCampaign": {
            "contact": "https://sim.amazon.com/issues/create?assignedFolder=ddba1fg1-5fcc-47fd-12389-eeb19sdfc46f",
            "name": "CDO Blocked Software Campaign",
            "url": "https://w.amazon.com/index.php/somePage"
          },
          "softwareRiskDetails": [
            {
              "dependencies": [],
              "riskAge": 56,
              "targetResource": "Jackson-databind:2.8.x"
            }
          ]
        }
      ]
    }
  ]
}

BatchGetPipelineTargetResourceDetails

The API batch size limit is 100

Required input:

    entityIdList: A list of Amazon Pipeline Target name; (Max size we can support is 200)
    requesterLogin: requester login (usually use the same supervisor login in allowlisting SIM);

Optional input:

    campaignType: BLOCK/NON_ADVISORY/ADVISORY/ALL;
        BLOCK -- all blocked software campaigns;
        NON_ADVISORY -- all campaigns on the default landing page (including all BLOCK campaigns);
        ADVISORY -- all campaigns on the Advisory Insights tab;
        ALL -- all campaigns;
    campaignId: campaign id, for example "******************-*****************" for "Blocked Software Campaign - Dec 2018";

Note: if campaignType and campaignId are both null,  it will return risk details associsted with all campaigns; If campaignId is not null, it will return risk details only associated with the given campaign id; If campaignType is not null and campaignId is null, it will return risk details only associated with the given campaign type;

Note: It will return empty list if given entity ids don't have any risk associsted with given campaign id or campaign type;

Example input:

{
  "entityIdList": [
    "us-west-2:4099375:alphaApplication:DG"
  ],
  "requesterLogin": "wyep"
}

Example Output:

{ "details": [ 
  { 
    "name": "us-west-2:4099375:alphaApplication:DG", 
    "pipelineId" : 4099375, 
    "pipelineName" : "SomePipelineName", 
    "pipelineTargetType" : "DeploymentGroup", 
    "pipelineStage" : "alpha",
    "oldestRiskAge": 56, 
    "owner": "Owner Name", 
    "ownerLogin": "ownerLogin",  
    "riskCount": 1, 
    "softwareRisks": [
      { 
        "associatedCampaign": 
          { 
            "contact": "https://sim.amazon.com/issues/create?assignedFolder=ddba1fg1-5fcc-47fd-12389-eeb19sdfc46f", 
            "name": "CDO Blocked Software Campaign", 
            "url": "https://w.amazon.com/index.php/somePage" 
          }, 
        "softwareRiskDetails": [ 
          { 
            "dependencies": [], 
            "riskAge": 56, 
            "targetResource": "Jackson-databind:2.8.x" 
          } 
        ] 
      } 
    ] 
  } 
] }

BatchGetCodeDeployDeploymentGroupNameDetails

NOTE: CodeDeployDeploymentGroup APIs are deprecated. Please use BatchGetPipelineTargetResourceDetails.

The API batch size limit is 100

Get CodeDeployDeploymentGroup details for given entityIdList. entityId format:  [AccountId]:[Region]:[ApplicationName]:[DeploymentGroupName]

Required input:

    entityIdList: A list of AWS CodeDeploy deployment group name;
    requesterLogin: requester login (usually use the same supervisor login in allowlisting SIM);

Optional input:

    campaignType: BLOCK/NON_ADVISORY/ADVISORY/ALL;
        BLOCK -- all blocked software campaigns;
        NON_ADVISORY -- all campaigns on the default landing page (including all BLOCK campaigns);
        ADVISORY -- all campaigns on the Advisory Insights tab;
        ALL -- all campaigns;
    campaignId: campaign id, for example "******************-*****************" for "Blocked Software Campaign - Dec 2018";

Note: if campaignType and campaignId are both null,  it will return risk details associsted with all campaigns; If campaignId is not null, it will return risk details only associated with the given campaign id; If campaignType is not null and campaignId is null, it will return risk details only associated with the given campaign type;

Note: It will return empty list if given entity ids don't have any risk associsted with given campaign id or campaign type;

Example Input:

{
  "entityIdList": [
    "9243453641545:us-west-2:SomeCodeDeployService-m-CodeDeployApp-3GBSCNEYM3GO0:SomeCodeDeployService-m-DeploymentGroup1-1440FEBIOJK7GE"
  ],
  "requesterLogin": "wyep"
} 

Example Output:

{
  "details": [
    {
      "account": {
        "accountId": "9243453641545",
        "org": "AWS"
      },
      "name": "9243453641545:ca-central-1:SomeCodeDeployService-m-CodeDeployApp-3ZSCNEYM3HO0:SomeCodeDeployService-m-DeploymentGroup1-1440FEBJKB7GE",
      "oldestRiskAge": 75,
      "owner": "OwnerName",
      "ownerLogin": "ownerlogin",
      "riskCount": 2,
      "softwareRisks": [
        {
          "associatedCampaign": {
            "contact": "https://issues.amazon.com/issues/create?assignedFolder=b867b140-a87d-1234-9d40-408195ebf5a6",
            "name": "Load Test Campaign Coral 1.0 Deprecation",
            "url": "https://w.amazon.com/index.php/somePage"
          },
          "softwareRiskDetails": [
            {
              "riskAge": 64,
              "targetResource": "Bobcat:2.1"
            },
            {
              "riskAge": 64,
              "targetResource": "CoralActivity:1.0"
            }
          ]
        }
      ]
    }
  ]
} 

BatchGetDownstreamAggregatesWithRiskByCampaignType

For a given aggregateId, campaign type, get list of direct downstream aggregates whose total risk count is non-zero. AggregateId format: Person:[login]

Campaign Type could be: NON_ADVISORY (all campaigns on SAS Dashboard landing page, including BLOCKED campaigns), ADVISORY(all campaigns on SAS Dashboard Advisory page), BLOCKED (https://w.amazon.com/index.php/BuilderTools/SoftwareAssurance/BlockedSoftwareCampaign)

Example Input:

{
  "aggregateId": "Person:someLogin",
  "campaignType": "NON_ADVISORY"
} 

Example Output:

{
  "downstreamAggregateIdList": [
    "Person:employeeLoginA",
    "Person:employeeLoginB",
    "Person:employeeLoginC"
  ]
} 

BatchGetDownstreamAggregatesWithRiskByCampaignIds

For a given aggregateId, list of campaignIds, get list of direct downstream aggregates whose total risk count is non-zero. AggregateId format: Person:[login]

Example Input:

{
  "aggregateId": "Person:someLogin",
  "campaignIds": ["******************-*****************"]
} 

Example Output:

{
  "downstreamAggregateIdList": [
    "Person:employeeLoginA",
    "Person:employeeLoginB",
    "Person:employeeLoginC"
  ]
} 

BatchGetElasticContainerServiceDetails

NOTE: ECS APIs are deprecated, please switch to using BatchGetPipelineTargetResourceDetails

Get Elastic Container Service Details for a list of Elastic Container Service entity. entityId format: [AccountId]:[Region]:[Cluster]:[TaskId]

Required input:

    entityIdList: A list of AWS elastic container service name;
    requesterLogin: requester login (usually use the same supervisor login in allowlisting SIM);

Optional input:

    campaignType: BLOCK/NON_ADVISORY/ADVISORY/ALL;
        BLOCK -- all blocked software campaigns;
        NON_ADVISORY -- all campaigns on the default landing page (including all BLOCK campaigns);
        ADVISORY -- all campaigns on the Advisory Insights tab;
        ALL -- all campaigns;
    campaignId: campaign id, for example "******************-*****************" for "Blocked Software Campaign - Dec 2018";

Note: if campaignType and campaignId are both null,  it will return risk details associsted with all campaigns; If campaignId is not null, it will return risk details only associated with the given campaign id; If campaignType is not null and campaignId is null, it will return risk details only associated with the given campaign type;

Note: It will return empty list if given entity ids don't have any risk associsted with given campaign id or campaign type;

Example Input:

{
  "entityIdList": [
    "3153921211059:us-west-2:Cluster:4058b32371fb41c7b1d81b642bdff2ad"
  ],
  "requesterLogin": "wyep"
} 

Example Output:

{
  "details": [
    {
      "account": {
        "accountId": "3153921211059",
        "org": "AWS"
      },
      "name": "3153921211059:us-west-2:Cluster:4058b32371fb41c7b1d81b642bdff2ad",
      "oldestRiskAge": 65,
      "owner": "Owner Name",
      "ownerLogin": "ownerName",
      "pipelineId": "12345",
      "pipelineName": "SomePipelineName",
      "riskCount": 3,
      "softwareRisks": [
        {
          "associatedCampaign": {
            "contact": "NA",
            "name": "OdinAWSCredential - Dry run campaign",
            "url": "NA"
          },
          "softwareRiskDetails": [
            {
              "riskAge": 63,
              "targetResource": "OdinAWSCredentials:1.4"
            }
          ]
        },
        {
          "associatedCampaign": {
            "contact": "contactInfo",
            "name": "campaignName",
            "url": "url about the campaign"
          },
          "softwareRiskDetails": [
            {
              "riskAge": 63,
              "targetResource": "RiskyPackage:1.0"
            },
            {
              "riskAge": 63,
              "targetResource": "RiskyPackage2:1.1"
            }
          ]
        }
      ]
    }
  ]
} 

BatchGetRiskSummaryCountByCampaignType

For a given aggregateId and campaign type, get the summary of total risk count, total risk with recommendation, WoW, MoM and YTD risks for the given login and his/her direct reports

Campaign Type could be: NON_ADVISORY (all campaigns on SAS Dashboard landing page, including BLOCKED campaigns), ADVISORY(all campaigns on SAS Dashboard Advisory page), BLOCKED (https://w.amazon.com/index.php/BuilderTools/SoftwareAssurance/BlockedSoftwareCampaign)

Example Input:

 {
    "campaignType": "ADVISORY",
    "aggregateId": "Person:*****"
} 

Example output:

 {
    "owner": "*****",
    "ownerName": "***** ARISOY",
    "manager": "ramensky",
    "managerName": "Peter Ramensky",
    "totalCount": {
        "APOLLO_ENVIRONMENT_NAME_AND_STAGE": {
            "totalRiskCount": 10,
            "totalRiskCountWithRecommendation": 8,
            "weekOverWeek": -5,
            "monthOverMonth": -10,
            "yearToDate": -10
        },
        "BRAZIL_VERSION_SET": {
            "totalRiskCount": 2,
            "totalRiskCountWithRecommendation": 0,
            "weekOverWeek": -3,
            "monthOverMonth": -3,
            "yearToDate": -3
        }
    },
    "individualCountList": [
        {
            "ownerLogin": "*****",
            "ownerName": "AYTUL ARISOY",
            "entityRiskCountMap": {
                "APOLLO_ENVIRONMENT_NAME_AND_STAGE": {
                    "totalRiskCount": 8,
                    "totalRiskCountWithRecommendation": 6,
                    "weekOverWeek": -4,
                    "monthOverMonth": -9,
                    "yearToDate": -9
                },
                "BRAZIL_VERSION_SET": {
                    "totalRiskCount": 2,
                    "totalRiskCountWithRecommendation": 0,
                    "weekOverWeek": -3,
                    "monthOverMonth": -3,
                    "yearToDate": -3
                }
            }
        },
        {
            "ownerLogin": "******",
            "ownerName": "Cheryl Chen",
            "entityRiskCountMap": {
                "APOLLO_ENVIRONMENT_NAME_AND_STAGE": {
                    "totalRiskCount": 2,
                    "totalRiskCountWithRecommendation": 2,
                    "weekOverWeek": -1,
                    "monthOverMonth": -1,
                    "yearToDate": -1
                }
            }
        }
    ]
} 

BatchGetRiskSummaryCountByCampaignIds

For a given aggregateId and list of campaign ids, get the summary of total risk count, total risk with recommendation, WoW, MoM and YTD risks for the given login and his/her direct reports

Example Input:

 {
    "aggregateId": "Person:*****"
    "campaignIds": ["df7448de-02a0-4b1c-b836-03f39750cf4d"]
} 

Example output:

 {
    "ownerLogin": "*****",
    "ownerName": "***** ARISOY",
    "managerLogin": "ramensky",
    "managerName": "Peter Ramensky",
    "totalCount": {
        "APOLLO_ENVIRONMENT_NAME_AND_STAGE": {
            "totalRiskCount": 10,
            "totalRiskCountWithRecommendation": 8,
            "weekOverWeek": -5,
            "monthOverMonth": -10,
            "yearToDate": -10
        },
        "BRAZIL_VERSION_SET": {
            "totalRiskCount": 2,
            "totalRiskCountWithRecommendation": 0,
            "weekOverWeek": -3,
            "monthOverMonth": -3,
            "yearToDate": -3
        }
    },
    "individualCountList": [
        {
            "ownerLogin": "*****",
            "ownerName": "***** ARISOY",
            "entityRiskCountMap": {
                "APOLLO_ENVIRONMENT_NAME_AND_STAGE": {
                    "totalRiskCount": 8,
                    "totalRiskCountWithRecommendation": 6,
                    "weekOverWeek": -4,
                    "monthOverMonth": -9,
                    "yearToDate": -9
                },
                "BRAZIL_VERSION_SET": {
                    "totalRiskCount": 2,
                    "totalRiskCountWithRecommendation": 0,
                    "weekOverWeek": -3,
                    "monthOverMonth": -3,
                    "yearToDate": -3
                }
            }
        },
        {
            "ownerLogin": "******",
            "ownerName": "Cheryl Chen",
            "entityRiskCountMap": {
                "APOLLO_ENVIRONMENT_NAME_AND_STAGE": {
                    "totalRiskCount": 2,
                    "totalRiskCountWithRecommendation": 2,
                    "weekOverWeek": -1,
                    "monthOverMonth": -1,
                    "yearToDate": -1
                }
            }
        }
    ]
} 

BatchGetWeeklyRiskSummaryCountByCampaignType

For a given aggregateId, campaign type and week number (default 6 weeks), get the total risks of past weeks for the given login. AggregateId format: Person:[login]

Campaign Type could be: NON_ADVISORY (all campaigns on SAS Dashboard landing page, including BLOCKED campaigns), ADVISORY(all campaigns on SAS Dashboard Advisory page), BLOCKED (https://w.amazon.com/index.php/BuilderTools/SoftwareAssurance/BlockedSoftwareCampaign)

Example Input:

{
  "aggregateId": "Person:employeeA",
  "campaignType": "ADVISORY",
  "weekNumber": 3
} 

Example Output:

{
  "ownerLogin": "employeeA",
  "ownerName": "employeeName",
  "weeklyTotalCount": {
    "BRAZIL_VERSION_SET": {
      "2020-12-23": 271,
      "**********": 241,
      "2020-12-30": 246
    },
    "AMAZON_PIPELINE_TARGET": {
      "2020-12-23": 2,
      "**********": 2,
      "2020-12-30": 4
    },
    "APOLLO_ENVIRONMENT_NAME_AND_STAGE": {
      "2020-12-23": 816,
      "**********": 792,
      "2020-12-30": 820
    }
  }
} 

GetPipelineTargetResourceDetails

Required input:

    entityIdList: Amazon Pipeline Target name; (Max size we can support is 200)
    requesterLogin: requester login (usually use the same supervisor login in allowlisting SIM);

Optional input:

    campaignType: BLOCK/NON_ADVISORY/ADVISORY/ALL;
        BLOCK -- all blocked software campaigns;
        NON_ADVISORY -- all campaigns on the default landing page (including all BLOCK campaigns);
        ADVISORY -- all campaigns on the Advisory Insights tab;
        ALL -- all campaigns;
    campaignId: campaign id, for example "******************-*****************" for "Blocked Software Campaign - Dec 2018";

Note: if campaignType and campaignId are both null,  it will return risk details associsted with all campaigns; If campaignId is not null, it will return risk details only associated with the given campaign id; If campaignType is not null and campaignId is null, it will return risk details only associated with the given campaign type;

Note: It will return empty list if given entity ids don't have any risk associsted with given campaign id or campaign type;

Example input:

{
  "entityIdList": "us-west-2:4099375:alphaApplication:DG",
  "requesterLogin": "wyep"
}

Example Output:

  { 
    "name": "us-west-2:4099375:alphaApplication:DG", 
    "pipelineId" : 4099375, 
    "pipelineName" : "SomePipelineName", 
    "pipelineTargetType" : "DeploymentGroup", 
    "pipelineStage" : "alpha",
    "oldestRiskAge": 56, 
    "owner": "Owner Name", 
    "ownerLogin": "ownerLogin",  
    "riskCount": 1, 
    "softwareRisks": [
      { 
        "associatedCampaign": 
          { 
            "contact": "https://sim.amazon.com/issues/create?assignedFolder=ddba1fg1-5fcc-47fd-12389-eeb19sdfc46f", 
            "name": "CDO Blocked Software Campaign", 
            "url": "https://w.amazon.com/index.php/somePage" 
          }, 
        "softwareRiskDetails": [ 
          { 
            "dependencies": [], 
            "riskAge": 56, 
            "targetResource": "Jackson-databind:2.8.x" 
          } 
        ] 
      } 
    ] 
  } 

GetAWSLambdaFunctionDetails

NOTE: AWS Lamdba Function APIs are deprecated. Please use GetPipelineTargetResourceDetails.

Get AWS Lambda Function Details for given entityId in format: [AccountId]:[Region]:[FunctionName]

Required input:

    entityId: AWS Lambda function name;
    requesterLogin: requester login (usually use the same supervisor login in allowlisting SIM);

Optional input:

    campaignType: BLOCK/NON_ADVISORY/ADVISORY/ALL;
        BLOCK -- all blocked software campaigns;
        NON_ADVISORY -- all campaigns on the default landing page (including all BLOCK campaigns);
        ADVISORY -- all campaigns on the Advisory Insights tab;
        ALL -- all campaigns;
    campaignId: campaign id, for example "******************-*****************" for "Blocked Software Campaign - Dec 2018";

Note: if campaignType and campaignId are both null,  it will return risk details associsted with all campaigns; If campaignId is not null, it will return risk details only associated with the given campaign id; If campaignType is not null and campaignId is null, it will return risk details only associated with the given campaign type;

Note: It will throw coral InvalidRequestException if given entity id doesn't have any risk associsted with given campaign id or campaign type;

Example Input:

{
  "entityId": "0390295230123:us-west-1:AwsSomeServiceLambda-LambdaFunction-IXIJ4432PFBIO",
  "requesterLogin": "wyep"
} 

Example Output:

{
  "account": {
    "accountId": "0390295230123",
    "org": "AWS"
  },
  "associatedVersions": [
    "0390295230123:us-west-1:AwsSomeServiceLambda-LambdaFunction-IXIJ4432PFBIO:99",
    "0390295230123:us-west-1:AwsSomeServiceLambda-LambdaFunction-IXIJ4432PFBIO:95",
    "0390295230123:us-west-1:AwsSomeServiceLambda-LambdaFunction-IXIJ4432PFBIO:96",
    "0390295230123:us-west-1:AwsSomeServiceLambda-LambdaFunction-IXIJ4432PFBIO:97",
    "0390295230123:us-west-1:AwsSomeServiceLambda-LambdaFunction-IXIJ4432PFBIO:98"
  ],
  "name": "0390295230123:us-west-1:AwsSomeServiceLambda-LambdaFunction-IXIJ4432PFBIO",
  "owner": "OwnerName",
  "ownerLogin": "ownerLogin",
  "pipelineId": "123145",
  "pipelineName": "AwsSomeServiceLambda"
} 

GetAWSLambdaFunctionVersionDetails

NOTE: AWSLamdbaFunction APIs are deprecated. Please use GetPipelineTargetResourceDetails.

Get AWS Lambda Function Version Details for given entityId in format: [AccountId]:[Region]:[FunctionName]:[VersionNumber]

Required input:

    entityId: AWS Lambda function version name;
    requesterLogin: requester login (usually use the same supervisor login in allowlisting SIM);

Optional input:

    campaignType: BLOCK/NON_ADVISORY/ADVISORY/ALL;
        BLOCK -- all blocked software campaigns;
        NON_ADVISORY -- all campaigns on the default landing page (including all BLOCK campaigns);
        ADVISORY -- all campaigns on the Advisory Insights tab;
        ALL -- all campaigns;
    campaignId: campaign id, for example "******************-*****************" for "Blocked Software Campaign - Dec 2018";

Note: if campaignType and campaignId are both null,  it will return risk details associsted with all campaigns; If campaignId is not null, it will return risk details only associated with the given campaign id; If campaignType is not null and campaignId is null, it will return risk details only associated with the given campaign type;

Note: It will throw coral InvalidRequestException if given entity id doesn't have any risk associsted with given campaign id or campaign type;

Example Input:

{
  "entityId": "0390295230123:us-west-1:AwsSomeServiceLambda-LambdaFunction-IXIJ4432PFBIO:98",
  "requesterLogin": "wyep"
} 

Example Output:

{
  "account": {
    "accountId": "0390295230123",
    "org": "AWS"
  },
  "name": "0390295230123:us-west-1:AwsSomeServiceLambda-LambdaFunction-IXIJ4432PFBIO:98",
  "oldestRiskAge": 67,
  "owner": "Owner Name",
  "ownerLogin": "ownerlogin",
  "pipelineId": "123455",
  "pipelineName": "AwsSomeServiceLambda",
  "riskCount": 1,
  "softwareRisks": [
    {
      "associatedCampaign": {
        "contact": "https://issues.amazon.com/issues/create?template=123124-4642-4a56-9c7d-81231236",
        "name": "Security Jackson campaign",
        "url": "https://w.amazon.com/index.php/somePage"
      },
      "softwareRiskDetails": [
        {
          "riskAge": 56,
          "targetResource": "Jackson-dataformat-cbor:2.9.x"
        }
      ]
    }
  ]
} 

GetAllCampaignsOfAggregate

This API get all campaigns which this employee and all employees finally reports to them have risks. Campaigns are divided by different campaign type: NON_ADVISORY, ADVISORY, BLOCKED and NON_TYPE;

    CampaignType is defined here: https://code.amazon.com/packages/SASRiskAggregationServiceModel/blobs/c747bd3b68333554f75149dbbac790d89ea481b2/--/model/data-types.xml#L243
    NO_TYPE should be usually null, it only has value when there is something wrong getting the campaign data;

Example Input:

 { "aggregateId": "Person:employeeLogin" } 

Example Output:

 {
    "getAllCampaignsOfAggregateResponseMap": {
        "NON_ADVISORY": [
            {
                "id": "912507f7-3d5e-4afe-a05c-2gi438aabe5e8",
                "url": "https://w.amazon.com/somepage",
                "name": "NSCampaign"

            },
            {
                "id": "a523080a-5a08-4f13-b200-d49174ddt3250",
                "url": "https://w.amazon.com/someotherPage",
                "name": "PlatformTest2"
            },
            {
                "id": "9291a12166-1462-48bc-af29-b31322512c5bb1",
                "url": "https://w.amazon.com/anotherPage",
                "name": "GGQSCampaign"
            }
        ],
        "ADVISORY": [
            {
                "id": "22cd247b-9e30-4763-9bc5-e33578763e8d",
                "url": "https://w.amazon.com/bin/view/SUSAdvisoryCampaign",
                "name": "SUSAdvisoryCampaign"
            }
        ],
        "BLOCKED": [
            {
                "id": "******************-*****************",
                "url": "https://w.amazon.com/bin/view/BLOCKEDCampaign",
                "name": "Blocked Software Campaign - Dec 2018"
            }
        ]
    }
 } 

GetApolloEnvironmentStageDetails

Get Apollo environment stage details

Required input:

    entityId: Apollo environment stage name;
    requesterLogin: requester login (usually use the same supervisor login in allowlisting SIM);

Optional input:

    campaignType: BLOCK/NON_ADVISORY/ADVISORY/ALL;
        BLOCK -- all blocked software campaigns;
        NON_ADVISORY -- all campaigns on the default landing page (including all BLOCK campaigns);
        ADVISORY -- all campaigns on the Advisory Insights tab;
        ALL -- all campaigns;
    campaignId: campaign id, for example "******************-*****************" for "Blocked Software Campaign - Dec 2018";

Note: if campaignType and campaignId are both null,  it will return risk details associsted with all campaigns; If campaignId is not null, it will return risk details only associated with the given campaign id; If campaignType is not null and campaignId is null, it will return risk details only associated with the given campaign type;

Note: It will throw coral InvalidRequestException if given entity id doesn't have any risk associsted with given campaign id or campaign type;

Example input:

{
  "entityId": "SomeApolloEnvironment/IAD:Gamma",
  "requesterLogin": "wyep"
} 

Example Output:

{
  "childrenCount": 1,
  "consumerCount": 0,
  "hostCount": 3,
  "name": "SomeApolloEnvironment/IAD:Gamma",
  "oldestRiskAge": 118,
  "owner": "OwnerName",
  "pipelineId": "203511",
  "pipelineName": "pipelineName",
  "recommendations": [
    {
      "associatedTargetResources": [
        "CoralRubyClientBuildLogic:1.0",
        "CoralTibcoValue:1.0",
        "CoralTransmutingProxy:1.0"
      ],
      "details": "{\"entity\":\"SomeApolloEnvironment/IAD:Gamma\"}",
      "template": "APOLLO_NO_ACTION"
    }
  ],
  "riskCount": 61,
  "softwareRisks": [
    {
      "associatedCampaign": {
        "contact": "https://sim.amazon.com/issues/create?assignedFolder=ddb12314-5fcc-47fd-8089-eeb123123d46f",
        "name": "CDO Blocked Software Campaign - Aug 2020 Gamma",
        "url": "https://w.amazon.com/index.php/somepage"
      },
      "softwareRiskDetails": [
        {
          "consumers": [
            "ApolloEnvironmentStageRevision:Consumer1:Prod:636268584"
          ],
          "riskAge": 118,
          "targetResource": "Jackson-databind:2.9.x"
        },
        {
          "consumers": [
            "ApolloEnvironmentStageRevision:Consumer2/IAD:Gamma:638358788"
          ],
          "riskAge": 118,
          "targetResource": "RubyGem-json:1.4"
        }
      ]
    },
    {
      "associatedCampaign": {
        "contact": "https://sim.amazon.com/issues/create?assignedFolder=12312213-7e0a-4d28-bb8a-12312312",
        "name": "SUSAdvisoryCampaign",
        "url": "https://w.amazon.com/index.php/somePage"
      },
      "softwareRiskDetails": [
        {
          "consumers": [
            "ApolloEnvironmentStageRevision:Consumer2/IAD:Gamma:638358788",
            "ApolloEnvironmentStageRevision:Consumer1:Prod:636268584"
          ],
          "riskAge": 76,
          "targetResource": "Jackson-dataformat-cbor:2.6.x"
        }
      ]
    }
  ],
  "visibility": "Internal"
} 

GetBrazilVersionSetDetails

Retrieve summary information for Brazil VersionSet

Required input:

    entityId: Brazil version set name;
    requesterLogin: requester login (usually use the same supervisor login in allowlisting SIM);

Optional input:

    campaignType: BLOCK/NON_ADVISORY/ADVISORY/ALL;
        BLOCK -- all blocked software campaigns;
        NON_ADVISORY -- all campaigns on the default landing page (including all BLOCK campaigns);
        ADVISORY -- all campaigns on the Advisory Insights tab;
        ALL -- all campaigns;
    campaignId: campaign id, for example "******************-*****************" for "Blocked Software Campaign - Dec 2018";

Note: if campaignType and campaignId are both null,  it will return risk details associsted with all campaigns; If campaignId is not null, it will return risk details only associated with the given campaign id; If campaignType is not null and campaignId is null, it will return risk details only associated with the given campaign type;

Note: It will throw coral InvalidRequestException if given entity id doesn't have any risk associsted with given campaign id or campaign type;

Example input:

{
  "entityId": "someVersionSet/development",
  "requesterLogin": "wyep"
} 

Example Output:

{
  "name": "someVersionSet/development",
  "oldestRecommendationAge": 79,
  "oldestRiskAge": 238,
  "owner": "OwnerName",
  "pipelineId": "1607",
  "pipelineName": "somePipeline",
  "recommendations": [
    {
      "actions": [
        {
          "details": "[{\"consumingPMV\":\"somePackage-1.0\",\"dependentPMV\":\"AmazonCoralMetricsThreadPoolSensor-1.0\"}]",
          "template": "BRAZIL_CAN_REMOVE_DEPENDENCY"
        }
      ],
      "brazilActionCount": 1,
      "hasFullActions": true,
      "targetResource": "AmazonCoralMetricsThreadPoolSensor:1.0"
    },
    {
      "actions": [],
      "brazilActionCount": 0,
      "hasFullActions": false,
      "targetResource": "CoralMetricsReporter:1.0"
    }
  ],
  "riskCount": 1,
  "softwareRisks": [
    {
      "associatedCampaign": {
        "contact": "https://issues.amazon.com/issues/create?template=12312412-4642-4a56-9c7d-8d12312124236",
        "name": "PlatformTest2",
        "url": "https://w.amazon.com/index.php/somepage"
      },
      "softwareRiskDetails": [
        {
          "dependencies": [],
          "recommendationAge": 79,
          "riskAge": 238,
          "targetResource": "NotAL2"
        }
      ]
    }
  ]
} 

GetCodeDeployDeploymentGroupNameDetails

NOTE: CodeDeployDeploymentGroup APIs are deprecated. Please use BatchGetPipelineTargetResourceDetails.

Required input:

    entityId: AWS CodeDeploy deployment group name;
    requesterLogin: requester login (usually use the same supervisor login in allowlisting SIM);

Optional input:

    campaignType: BLOCK/NON_ADVISORY/ADVISORY/ALL;
        BLOCK -- all blocked software campaigns;
        NON_ADVISORY -- all campaigns on the default landing page (including all BLOCK campaigns);
        ADVISORY -- all campaigns on the Advisory Insights tab;
        ALL -- all campaigns;
    campaignId: campaign id, for example "******************-*****************" for "Blocked Software Campaign - Dec 2018";

Note: if campaignType and campaignId are both null,  it will return risk details associsted with all campaigns; If campaignId is not null, it will return risk details only associated with the given campaign id; If campaignType is not null and campaignId is null, it will return risk details only associated with the given campaign type;

Note: It will throw coral InvalidRequestException if given entity id doesn't have any risk associsted with given campaign id or campaign type;

Example Input:

{
  "entityId": "*********:ca-central-1:SomeCodeDeployService-m-CodeDeployApp-3ZSCNEYM3HO0:SomeCodeDeployService-m-DeploymentGroup1-1080FEBJKB7GE",
  "requesterLogin": "wyep"
} 

Example Output:

{
  "account": {
    "accountId": "123123123",
    "org": "AWS"
  },
  "name": "*********:ca-central-1:SomeCodeDeployService-m-CodeDeployApp-3ZSCNEYM3HO0:SomeCodeDeployService-m-DeploymentGroup1-1080FEBJKB7GE",
  "oldestRiskAge": 76,
  "owner": "owenrName",
  "riskCount": 51,
  "softwareRisks": [
    {
      "associatedCampaign": {
        "contact": "https://sim.amazon.com/issues/create?assignedFolder=5fe0c312312312553759",
        "name": "SUSAdvisoryCampaign",
        "url": "https://w.amazon.com/index.php/somepage"
      },
      "softwareRiskDetails": [
        {
          "riskAge": 76,
          "targetResource": "Jackson-dataformat-cbor:2.6.x"
        }
      ]
    },
    {
      "associatedCampaign": {
        "contact": "https://issues.amazon.com/issues/create?assignedFolder=b867b140-a123123125a6",
        "name": "Load Test Campaign Coral 1.0 Deprecation",
        "url": "https://w.amazon.com/index.php/somepage1"
      },
      "softwareRiskDetails": [
        {
          "riskAge": 64,
          "targetResource": "Bobcat:2.1"
        },
        {
          "riskAge": 64,
          "targetResource": "CoralActivity:1.0"
        }
      ]
    }
  ]
} 

GetDownstreamAggregatesWithRisk

For a given aggregateId, campaignId, get list of direct downstream aggregates whose total risk count is non-zero

Example Input:

{
  "aggregateId": "Person:employeeLogin",
  "campaignId": "12312312-5172-4063-a64d-9b1f9b5df64a"
} 

Example Output:

{
  "downstreamAggregateIdList": [
    "Person:employeeLoginA",
    "Person:employeeLoginB",
    "Person:employeeLoginC"
  ]
}  

GetElasticContainerServiceDetails

NOTE: ECS APIs are deprecated. Please use GetPipelineTargetResourceDetails.

NOTE: ECS calls are currently disabled. They will be re-enabled in the future and this box will be removed.

Get Elastic Container Service Details for an entity. entityId format: [AccountId]:[Region]:[Cluster]:[TaskId]

Required input:

    entityId: AWS Elastic container service name;
    requesterLogin: requester login (usually use the same supervisor login in allowlisting SIM);

Optional input:

    campaignType: BLOCK/NON_ADVISORY/ADVISORY/ALL;
        BLOCK -- all blocked software campaigns;
        NON_ADVISORY -- all campaigns on the default landing page (including all BLOCK campaigns);
        ADVISORY -- all campaigns on the Advisory Insights tab;
        ALL -- all campaigns;
    campaignId: campaign id, for example "******************-*****************" for "Blocked Software Campaign - Dec 2018";

Note: if campaignType and campaignId are both null,  it will return risk details associsted with all campaigns; If campaignId is not null, it will return risk details only associated with the given campaign id; If campaignType is not null and campaignId is null, it will return risk details only associated with the given campaign type;

Note: It will throw coral InvalidRequestException if given entity id doesn't have any risk associsted with given campaign id or campaign type;

Example Input:

{
  "entityId": "3153921211059:us-west-2:Cluster:4058b32371fb41c7b1d81b642bdff2ad",
  "requesterLogin": "wyep"
} 

Example Output:

{
  "account": {
    "accountId": "3153921211059",
    "org": "AWS"
  },
  "name": "3153921211059:us-west-2:Cluster:4058b32371fb41c7b1d81b642bdff2ad",
  "oldestRiskAge": 65,
  "owner": "Owner Name",
  "ownerLogin": "ownerName",
  "pipelineId": "12345",
  "pipelineName": "SomePipelineName",
  "riskCount": 3,
  "softwareRisks": [
    {
      "associatedCampaign": {
        "contact": "NA",
        "name": "OdinAWSCredential - Dry run campaign",
        "url": "NA"
      },
      "softwareRiskDetails": [
        {
          "riskAge": 63,
          "targetResource": "OdinAWSCredentials:1.4"
        }
      ]
    },
    {
      "associatedCampaign": {
        "contact": "contactInfo",
        "name": "campaignName",
        "url": "url about the campaign"
      },
      "softwareRiskDetails": [
        {
          "riskAge": 63,
          "targetResource": "RiskyPackage:1.0"
        },
        {
          "riskAge": 63,
          "targetResource": "RiskyPackage2:1.1"
        }
      ]
    }
  ]
} 

GetResourceHighLevelDetails

For a given campaign and aggregateId, get associated resource(s) high level details of given employee and all their direct reportee. e.g. resource type, name, owner, risk counts etc.

Example Input:

{
    "campaignId": "ec3a359d-91bf-4feb-a47d-c3de0d2508d9",
    "aggregateId": "Person:*****"
} 

Example output:

 {
    "highLevelResourceDetailsByTypeMap": {
        "ApolloEnvironmentNameAndStage": [
            {
                "resourceName": "GG/PDX:Prod",
                "resourceType": "ApolloEnvironmentNameAndStage",
                "totalRiskCountWithAction": 2,
                "totalRiskCountWithoutAction": 0,
                "owner": "******",
                "ownerName": "Cheryl Chen",
                "riskAgeWithAction": 36,
                "riskAgeWithoutAction": 0
            },
            {
                "resourceName": "SUS/PDX:Prod",
                "resourceType": "ApolloEnvironmentNameAndStage",
                "totalRiskCountWithAction": 3,
                "totalRiskCountWithoutAction": 1,
                "owner": "********",
                "ownerName": "Ricky Quan",
                "riskAgeWithAction": 30,
                "riskAgeWithoutAction": 10
            }
        ],
        "BrazilVersionSet": [
            {
                "resourceName": "GG/development",
                "resourceType": "BrazilVersionSet",
                "totalRiskCountWithAction": 0,
                "totalRiskCountWithoutAction": 1,
                "owner": "******",
                "ownerName": "Cheryl Chen",
                "riskAgeWithAction": 0,
                "riskAgeWithoutAction": 15
            }
        ]
    }
} 

GetResourceHighLevelRiskDetailsByCampaignType

For a given campaign type and aggregateId, get all associated resource(s) high level details of given employee and all their direct reports. e.g. resource type, name, owner, risk counts etc.

Campaign Type could be:

    campaignType: BLOCK/NON_ADVISORY/ADVISORY/ALL;
        BLOCK -- all blocked software campaigns;
        NON_ADVISORY -- all campaigns on the default landing page (including all BLOCK campaigns);
        ADVISORY -- all campaigns on the Advisory Insights tab;
        ALL -- all campaigns;

Example Input:

 {
    "campaignType": "BLOCKED",
    "aggregateId": "Person:aytul"
} 

Example output:

 {
    "highLevelResourceDetailsByResourceTypeMap": {
        "ApolloEnvironmentNameAndStage": [
            {
                "resourceName": "GG/PDX:Prod",
                "resourceType": "ApolloEnvironmentNameAndStage",
                "totalRiskCountWithAction": 2,
                "totalRiskCountWithoutAction": 0,
                "owner": "cheryl",
                "ownerName": "Cheryl Chen",
                "riskAgeWithAction": 3,
                "recommendationAge": 24
                "riskAgeWithoutAction": 0
            },
            {
                "resourceName": "SUS/PDX:Prod",
                "resourceType": "ApolloEnvironmentNameAndStage",
                "totalRiskCountWithAction": 3,
                "totalRiskCountWithoutAction": 1,
                "owner": "********",
                "ownerName": "Ricky Quan",
                "riskAgeWithAction": 30,
                "riskAgeWithoutAction": 10
            }
        ],
        "BrazilVersionSet": [
            {
                "resourceName": "GG/development",
                "resourceType": "BrazilVersionSet",
                "totalRiskCountWithAction": 0,
                "totalRiskCountWithoutAction": 1,
                "owner": "cheryl",
                "ownerName": "Cheryl Chen",
                "riskAgeWithAction": 3,
                "riskAgeWithAction": 0,
                "riskAgeWithoutAction": 15
            }
        ]
    }
} 

GetRiskSummaryCount

For a given aggregateId and campaignId, get the summary of total risk count, total risk with recommendation, WoW, MoM and YTD risks for the given login and his/her direct reports

Example Input:

{
    "campaignId": "ec3a359d-91bf-4feb-a47d-c3de0d2508d9",
    "aggregateId": "Person:*****"
} 

Example output:

 {
 "owner": "*****", 
 "ownerName": "***** ARISOY",
 "manager": "********",
 "managerName": "Peter Ramensky",
 "totalCount": {
   "APOLLO_ENVIRONMENT_NAME_AND_STAGE": {
      "totalRiskCount": 10,
      "totalRiskCountWithRecommendation": 8,
      "weekOverWeek": -5,
      "monthOverMonth": -10,
      "yearToDate": -10}
   "BRAZIL_VERSION_SET":{
      "totalRiskCount": 2,
      "totalRiskCountWithRecommendation": 0,
      "weekOverWeek": -3,
      "monthOverMonth": -3,
      "yearToDate": -3}
 },
 "individualCountList": [{
    "ownerLogin": "*****",
    "ownerName": "***** ARISOY",
    "entityRiskCountMap": {
      "APOLLO_ENVIRONMENT_NAME_AND_STAGE": {
        "totalRiskCount": 8,
        "totalRiskCountWithRecommendation": 6,
        "weekOverWeek": -4,
        "monthOverMonth": -9,
        "yearToDate": -9}
      "BRAZIL_VERSION_SET":{
        "totalRiskCount": 2,
        "totalRiskCountWithRecommendation": 0,
        "weekOverWeek": -3,
        "monthOverMonth": -3,
        "yearToDate": -3}
      }
    },
    {
    "ownerLogin": "******",
    "ownerName": "***********",
    "entityRiskCountMap": {
      "APOLLO_ENVIRONMENT_NAME_AND_STAGE": {
        "totalRiskCount": 2,
        "totalRiskCountWithRecommendation": 2,
        "weekOverWeek": -1,
        "monthOverMonth": -1,
        "yearToDate": -1}
     }
   }
 ]
} 

GetWeeklyRiskSummaryCount

For a given aggregateId and campaignId, get the weekly summary of total risk count.

Example Input:

 {
    "campaignId": "9291b966-1462-48bc-af29-b362259c5bb1",
    "aggregateId": "Person:*****",
    "weekNumber": 6
 } 

Example output:

 {
    "owner": "*****",
    "ownerName": "************",
    "weeklyTotalCount": {
        "APOLLO_ENVIRONMENT_NAME_AND_STAGE": {
            "2020-03-01": 10,
            "2020-03-08": 8,
            "2020-03-15": 8,
            "2020-03-23": 7,
            "2020-03-30": 5,
            "2020-04-06": 4
        },
        "BRAZIL_VERSION_SET": {
            "2020-03-01": 2,
            "2020-03-08": 2,
            "2020-03-15": 1,
            "2020-03-23": 0,
            "2020-03-30": 0,
            "2020-04-06": 0
        }
    }
} 

GetResourceHighLevelRiskDetailsByPipelineId

For a given campaignType and pipelineId, get high level resource details for all entities within the pipeline.

Example Input:

 {
    "campaignType": "BLOCKED",
    "pipelineId": "123456"
 } 

Example output:

 {"highLevelResourceDetailsByResourceTypeMap": 
  {"ApolloEnvironmentNameAndStage": 
    [
      {"blockingTimeInEpochMilli": 1655445893000, 
       "containsBlockedResource": true, 
       "owner": "harmesod", 
       "ownerName": "Harmeet Sodhi", 
       "pipelineId": "123456", 
       "recommendationAge": 29, 
       "resourceName": "FooWebsite/PDX:Beta", 
       "resourceType": "ApolloEnvironmentNameAndStage", 
       "riskAgeWithAction": 937, 
       "riskAgeWithoutAction": 461, 
       "riskSeverityMapWithRecommendations": 
        {"High": 8, 
         "Low": 13, 
         "Medium": 0, 
         "Critical": 0}, 
       "riskSeverityMapWithoutRecommendations": 
        {"High": 0, 
         "Low": 5, 
         "Medium": 0, 
         "Critical": 0}, 
       "totalRiskCountWithAction": 21, 
       "totalRiskCountWithoutAction": 5}, 
      {"blockingTimeInEpochMilli": 1655449098000, 
       "containsBlockedResource": true, 
       "owner": "harmesod", 
       "ownerName": "Harmeet Sodhi", 
       "pipelineId": "123456", 
       "recommendationAge": 29, 
       "resourceName": "FooWebsite/PDX:Prod", 
       "resourceType": "ApolloEnvironmentNameAndStage", 
       "riskAgeWithAction": 937, 
       "riskAgeWithoutAction": 461, 
       "riskSeverityMapWithRecommendations": 
        {"High": 8, 
         "Low": 14, 
         "Medium": 0, 
         "Critical": 0}, 
       "riskSeverityMapWithoutRecommendations": 
        {"High": 0, 
         "Low": 5, 
         "Medium": 0, 
         "Critical": 0}, 
       "totalRiskCountWithAction": 22, 
       "totalRiskCountWithoutAction": 5}
    ], 
   "BrazilVersionSet": 
    [
      {"blockingTimeInEpochMilli": 1655366550941, 
       "containsBlockedResource": true, 
       "owner": "harmesod", 
       "ownerName": "Harmeet Sodhi", 
       "pipelineId": "123456", 
       "recommendationAge": 670, 
       "resourceName": "FooWebsite/development", 
       "resourceType": "BrazilVersionSet", 
       "riskAgeWithAction": 937, 
       "riskAgeWithoutAction": 461, 
       "riskSeverityMapWithRecommendations": 
        {"High": 0, 
         "Low": 13, 
         "Medium": 0, 
         "Critical": 8}, 
       "riskSeverityMapWithoutRecommendations": 
        {"High": 0, 
         "Low": 5, 
         "Medium": 0, 
         "Critical": 0}, 
       "totalRiskCountWithAction": 21, 
       "totalRiskCountWithoutAction": 5}
    ]
  }, 
 "pipelineName": "FooWebsite", 
 "pipelineOwnerLogin": "harmesod", 
 "pipelineOwnerName": "Harmeet Sodhi"}

Common Use Cases
List all containing entities (e.g. Apollo environment stages, Brazil version sets, etc.) that contains risks with recommendations in NON_ADVISORY campaign for my director's organization (directorlogin@) and the owner login

APIs: BatchGetDownstreamAggregatesWithRiskByCampaignType && GetResourceHighLevelRiskDetailsByCampaignType

1. Call GetResourceHighLevelRiskDetailsByCampaignType API with given login (directlogin@) and campaign type (NON_ADVISORY), this returns containing entities owned by given 
   login and their direct reports;
2. Filter out containing entities which `totalRiskCountWithAction` is 0 which means this containing entities doesn't contain any risks with recommendation,add the filtered
   containing entities to final result;
3. For given login (directlogin@), get direct reports who owns risks for given campaign type NON_ADVISORY by calling BatchGetDownstreamAggregatesWithRiskByCampaignType API;
4. Add direct reports to toBeProcessed list;
5. Repeat step #1-#4 for every item in toBeProcessed list;
6. Return final result;

Result: The final result would look similar as below and containing the following information:

 {
    "highLevelResourceDetailsByResourceTypeMap": {
        "ApolloEnvironmentNameAndStage": [
            {
                "resourceName": "GG/PDX:Prod",
                "resourceType": "ApolloEnvironmentNameAndStage",
                "totalRiskCountWithAction": 2,
                "totalRiskCountWithoutAction": 0,
                "owner": "******",
                "ownerName": "****** Chen",
                "riskAgeWithAction": 36,
                "riskAgeWithoutAction": 0
            },
            {
                "resourceName": "SUS/PDX:Prod",
                "resourceType": "ApolloEnvironmentNameAndStage",
                "totalRiskCountWithAction": 3,
                "totalRiskCountWithoutAction": 1,
                "owner": "********",
                "ownerName": "Ricky Quan",
                "riskAgeWithAction": 30,
                "riskAgeWithoutAction": 10
            }
        ],
        "BrazilVersionSet": [
            {
                "resourceName": "GG/development",
                "resourceType": "BrazilVersionSet",
                "totalRiskCountWithAction": 1,
                "totalRiskCountWithoutAction": 0,
                "owner": "******",
                "ownerName": "Cheryl Chen",
                "riskAgeWithAction": 16,
                "riskAgeWithoutAction": 0
            }
        ]
    }
} 

List all Apollo environment with details (e.g recommendation, risk age, etc.) in NON_ADVISORY campaign for my director's orgnization (directorlogin@) and the owner login

****: BatchGetDownstreamAggregatesWithRiskByCampaignType && GetResourceHighLevelRiskDetailsByCampaignType && BatchGetApolloEnvironmentStageDetails

1. Call GetResourceHighLevelRiskDetailsByCampaignType API with given login (directlogin@) and campaign type (NON_ADVISORY), this returns containing entities owned by given 
   login and their direct reports;
2. Filter out containing entities which is not Apollo environment stage, and call BatchGetApolloEnvironmentStageDetails with the list of apollo environment stages, add the 
   filtered containing entities to final result;
3. For given login (directlogin@), get direct reports who owns risks for given campaign type NON_ADVISORY by calling BatchGetDownstreamAggregatesWithRiskByCampaignType API;
4. Add direct reports to toBeProcessed list;
5. Repeat step #1-#4 for every item in toBeProcessed list;
6. Return final result;

Result: The final result would look similar as below and containing the following information:

{
  "details": [
    {
      "childrenCount": 0,
      "consumerCount": 0,
      "hostCount": 2,
      "name": "ApolloEnvironmentName:Prod",
      "oldestRiskAge": 117,
      "owner": "Owner name",
      "ownerLogin": "ownerLogin",
      "pipelineId": "12345",
      "pipelineName": "SomePipelineName",
      "recommendations": [
        {
          "associatedTargetResources": [
            "CoralRubyClientBuildLogic:1.0",
            "CoralTibcoValue:1.0",
            "CoralTransmutingProxy:1.0",
            "CoralBatchingSupport:1.0",
          ],
          "details": "{\"entity\":\"ApolloEnvironmentName:Prod\"}",
          "template": "APOLLO_NO_ACTION"
        },
        {
          "associatedTargetResources": [
            "Jackson-databind:2.8.x"
          ],
          "details": "{\"consumableEnvironmentsToUpdate\":[\"SomeConsumableEnvironmentName:Prod:105326079224\"],\"entity\":\"ApolloEnvironmentName:Prod\"}",
          "template": "APOLLO_CAN_UPDATE"
        }
      ],
      "riskCount": 1,
      "softwareRisks": [
        {
          "associatedCampaign": {
            "contact": "https://sim.amazon.com/issues/create?assignedFolder=dfio1291-5fcc-47fd-8089-eeb19123f46f",
            "name": "CDO Blocked Software Campaign",
            "url": "https://w.amazon.com/index.php/somepages"
          },
          "softwareRiskDetails": [
            {
              "consumers": [
                "ApolloEnvironmentStageRevision:SomeConsumableEnvironmentName:Prod:105326079224"
              ],
              "recommendationAge": 27,
              "riskAge": 117,
              "targetResource": "Jackson-databind:2.8.x"
            }
          ]
        }
      ],
      "visibility": "Internal"
    }
  ]
}

Upcoming changes for NAWS risk related API

To improve the current NAWS risk reporting, Builder Tools Software Assurance Services(SAS) is making changes to these APIs as we are going to move NAWS(CodeDeploy and LambdaFunction) risk reporting to be Pipeline Target based reporting. Pipeline Target reporting will also report risks in Lambda Functions, CD, ECS tasks present in the AWS account. We will stop tracking Lambda Functions and CodeDeploy Deployment Groups separately. So the SAS Dashboard and RAGS APIs will also stop reporting on the Lambda Functions and CodeDeploy Deployment Groups and start reporting on AmazonPipelineTargets instead. These changes would happen in two phases:

    On 4/26/2023, we will add AmazonPipelineTarget entity to RAGS APIs. We will also keep reporting the Lambda Functions and CodeDeploy Deployment Groups data, so that we do not immediately introduce a breaking change to these APIs. The following new APIs will be added:
        GetPipelineTargetResourceDetails
        BatchGetPipelineTargetResourceDetails
     The following APIs will return a new key for pipeline target entity in the risk map:
        GetResourceHighLevelRiskDetailsByCampaignType
        GetResourceHighLevelDetailsGroupByPipeline
        GetResourceHighLevelRiskDetailsByCampaignTypeGroupByPipeline
        GetResourceHighLevelRiskDetailsByPipelineId
        BatchGetRiskSummaryCountByCampaignType
        BatchGetRiskSummaryCountByCampaignIds
        BatchGetWeeklyRiskSummaryCountByCampaignType
        GetRiskSummaryCount
        GetWeeklyRiskSummaryCount

    On 6/5/2023 we will remove Lambda Functions and CodeDeploy Deployment Groups from the RAGS APIs - We will remove the usage of these NAWS entities completely from the RAGS APIs which also includes deprecating the following APIs and calling these APIs will throw an NotImplementedException after 6/5/2023.
        GetAWSLambdaFunctionDetails
        GetAWSLambdaFunctionVersionDetails
        GetCodeDeployDeploymentGroupNameDetails
        BatchGetAWSLambdaFunctionDetails
        BatchGetAWSLambdaFunctionVersionDetails
        BatchGetCodeDeployDeploymentGroupNameDetails
    The CodeDeploy/LambdaFunction key in the risk map will be removed for the following APIs:
        GetResourceHighLevelDetails
        GetResourceHighLevelRiskDetailsByCampaignType
        GetResourceHighLevelDetailsGroupByPipeline
        GetResourceHighLevelRiskDetailsByCampaignTypeGroupByPipeline
        GetResourceHighLevelRiskDetailsByPipelineId
        BatchGetRiskSummaryCountByCampaignType
        BatchGetRiskSummaryCountByCampaignIds
        BatchGetWeeklyRiskSummaryCountByCampaignType
        GetRiskSummaryCount
        GetWeeklyRiskSummaryCount

Here are the changes that will happen to these RAGS APIs - [Sample doc].

==== WHAT ARE MY ACTIONS? ====

No actions needed by 4/26/2023.

Between 4/26/2023 and 6/5/2023,
       If you are currently using one of the following APIs:

    GetAWSLambdaFunctionDetails
    GetAWSLambdaFunctionVersionDetails
    GetCodeDeployDeploymentGroupNameDetails
    BatchGetAWSLambdaFunctionDetails
    BatchGetAWSLambdaFunctionVersionDetails
    BatchGetCodeDeployDeploymentGroupNameDetails

       you should migrate to use GetPipelineTargetResourceDetails and BatchGetPipelineTargetResourceDetails APIs between 4/26 and 6/5.

       If you are currently using one of the following APIs:

    GetResourceHighLevelDetails
    GetResourceHighLevelRiskDetailsByCampaignType
    GetResourceHighLevelDetailsGroupByPipeline
    GetResourceHighLevelRiskDetailsByCampaignTypeGroupByPipeline
    GetResourceHighLevelRiskDetailsByPipelineId
    BatchGetRiskSummaryCountByCampaignType
    BatchGetRiskSummaryCountByCampaignIds
    BatchGetWeeklyRiskSummaryCountByCampaignType
    GetRiskSummaryCount
    GetWeeklyRiskSummaryCount

     you should make code changes to read the new risk data for AmazonPipelineTarget returned in these API responses between 4/26 and 5/26.

If you have any questions, please reach out to SAS by cutting a ticket: https://tiny.amazon.com/1gfih1esj/issuamazissucrea. or signing up for SAS office hours: https://w.amazon.com/index.php/BuilderTools/OfficeHours/SoftwareAssurance.

==== ADDITIONAL INFORMATION ====

What does SAS report in Pipeline Target based NAWS Reporting?
Pipeline targets including CodeDeploy(CD), CloudFormation(CF), and DeploymentGroup(DG) targets will be reported instead of individual CodeDeploy DeploymentGroup and Lambda Function. Customers will be able to get risks contained in currently deployed target revisions within a pipeline. Individual CodeDeploy DeploymentGroup risks will be covered in CD and DG target risks and Lambda Function risks will be covered in CF and DG risks if they are created via CloudFormation. CF target risks will also cover other artifacts deployed via the CloudFormation stack besides Lambda Functions. For example, an ECS task containing risks could be identified for a CF target if the ECS task is defined in the CloudFormation stack.

Why does SAS move NAWS reporting to Pipeline Target based reporting?
Software Assurance Services (SAS) currently enumerates NAWS entities like Code Deploy and Lambda functions that are impacted by vulnerable or outdated software. Classification of these entities is based on runtime closure graphs of Version Sets. This process however can result in false positives or false negatives due to multiple reasons - missing targets in Version Sets, inability to know what dependencies end up being actually deployed to the entities and GatedGarden not tracking NAWS artifacts in AWS regions like opt-in, china, govcloud, and ADC. Additionally, SAS had to stop reporting ECS entities in 2021 due to tracking of ECS being incomplete. To address these issues, improve coverage and track dependencies packaged by BATS, SAS will move NAWS reporting to Pipeline Targets based reporting.

Post 4/26, why don't I see Lambda/CD risks on SAS dashboard but I see risks in API results of GetLambdaDetails/GetCDDetails?
We will stop showing Lambda/CD risks on SAS dashboard from 4/26/2023 and campaign owners will not be able to make Lambda/CD risks dashboard visible or ticket Lambda/CD risks. We will be able to show and ticket pipeline target risks instead. To make the transition process smoother for RAGS API users, we will still classify Lambda/CD risks at backend and support GetLambda/CDDetails API for one month after 4/26 so that we do not immediately introduce a breaking change to these APIs and customers will have enough time to migrate to use new GetPipelineTargetResourceDetails API.

What happens if I dont migrate by the date given?
Call for the APIs to get CodeDeploy/LambdaFunction details will throw InvalidRequestException after 5/19/2023. Call for get high level risk details/risk summary count related APIs will not return CodeDeploy/LambdaFunction key after 5/26/2023.

How do I identify which RAGS APIs our service is using?

You could do a code search for your packages related to the specific AWS account that received this ticket.

these are the API available

------

Cedric: Based on the documentation provided, here are the key SAS RAGS (Risk Aggregation Service) APIs that are relevant to the plugin and recent changes:

1. **Main Risk Detail APIs**
```python
# Core APIs used by the plugin:
BatchGetApolloEnvironmentStageDetails
BatchGetBrazilVersionSetDetails
BatchGetPipelineTargetResourceDetails
GetResourceHighLevelRiskDetailsByCampaignType
```

2. **Important Changes to Note**:

- **NAWS Risk Reporting Changes**:
```python
# Deprecated APIs (No longer available):
- GetAWSLambdaFunctionDetails
- GetCodeDeployDeploymentGroupNameDetails
- BatchGetAWSLambdaFunctionDetails
- BatchGetCodeDeployDeploymentGroupNameDetails

# New Pipeline Target Based Reporting:
- GetPipelineTargetResourceDetails
- BatchGetPipelineTargetResourceDetails
```

3. **Risk Classification Changes**:
```python
# Previous Classification
campaign_type='BLOCKED'

# New Severity Levels:
- Emergency
- Critical 
- High
```

4. **API Batch Limits**:
```python
BATCH_SIZE = 100  # Maximum items per batch for all batch APIs
```

5. **Required API Parameters**:
```python
Required:
- entityIdList: List of entities to query
- requesterLogin: Requester's login

Optional:
- campaignType: BLOCK/NON_ADVISORY/ADVISORY/ALL
- campaignId: Specific campaign identifier
```

6. **API Response Structure**:
```python
{
    "details": [
        {
            "name": "entity_name",
            "oldestRiskAge": int,
            "owner": "owner_name",
            "ownerLogin": "owner_login",
            "riskCount": int,
            "softwareRisks": [
                {
                    "associatedCampaign": {...},
                    "softwareRiskDetails": [...]
                }
            ]
        }
    ]
}
```

7. **Authentication Requirements**:
```python
# AWS v4 signing required
- aws_access_key
- aws_secret_key 
- aws_region
- aws_service
```

8. **Throttling Limits**:
```python
# Rate Limits
1 TPS per API per AWS account
```

These APIs form the core of how the SAS plugin retrieves and processes risk data. The plugin needs to be updated to:

1. Use new Pipeline Target APIs instead of deprecated NAWS APIs
2. Handle new severity classifications
3. Maintain batch size limits
4. Handle proper authentication
5. Implement retry logic for throttling
6. Process complex nested response structures

The API changes should be reflected in both data collection and processing parts of the plugin.

------

venry: Create a me a High level design dcoument to address this issue with 2 approaches

------

Cedric: # High Level Design Document
## SAS Metrics Plugin Enhancement for New Risk Classification

### Problem Statement
The SAS (Security Automation Service) plugin currently tracks only "Blocked" campaign risks. However, SAS has changed their risk classification from "Blocked" to severity levels (Emergency, Critical, High). The plugin needs to be updated to reflect this change while maintaining existing functionality.

### Current Architecture
```mermaid
graph TD
    A[SAS Plugin] --> B[Get High Level Risk Details]
    B --> C[Process Blocked Risks]
    C --> D[Apollo Data]
    C --> E[Brazil Data]
    C --> F[Pipeline Data]
    D --> G[RedShift Tables]
    E --> G
    F --> G
    G --> H[Aggregate Metrics]
    H --> I[Page-0 Dashboard]
```

### Requirements
1. Track new severity levels (Emergency, Critical, High)
2. Maintain historical data compatibility
3. Update dashboard metrics
4. Minimal disruption to existing workflows
5. Support both risk types during transition
6. Update aggregation logic

---

## Approach 1: Parallel Processing Architecture
### Overview
Create parallel processing paths for both old and new classification systems, allowing gradual migration while maintaining backward compatibility.

### Design
```mermaid
graph TD
    A[SAS Plugin] --> B[Risk Classification Router]
    B --> C[Legacy Processor Blocked]
    B --> D[New Severity Processor]
    C --> E[Legacy Tables]
    D --> F[New Tables]
    E --> G[Aggregation Layer]
    F --> G
    G --> H[Combined Metrics]
    H --> I[Page-0 Dashboard]
```

### Components
1. **Risk Classification Router**
```python
class RiskClassificationRouter:
    def route_risk_data(self, data):
        legacy_data = self.filter_blocked_risks(data)
        severity_data = self.filter_severity_risks(data)
        return {
            'legacy': legacy_data,
            'severity': severity_data
        }
```

2. **Dual Processing System**
```python
class DualRiskProcessor:
    def process_risks(self, data):
        # Process legacy blocked risks
        legacy_metrics = self.process_legacy_risks(data['legacy'])
        
        # Process new severity risks
        severity_metrics = self.process_severity_risks(data['severity'])
        
        return self.combine_metrics(legacy_metrics, severity_metrics)
```

3. **Schema Changes**
```sql
-- New Tables
CREATE TABLE severity_risks (
    risk_id VARCHAR,
    severity VARCHAR,
    resource_name VARCHAR,
    risk_age INT,
    recommendation_age INT,
    metric_type VARCHAR
);

-- Mapping Table
CREATE TABLE risk_classification_mapping (
    legacy_risk_id VARCHAR,
    new_risk_id VARCHAR,
    classification_type VARCHAR
);
```

### Advantages
1. Zero downtime migration
2. Backward compatibility
3. Gradual transition
4. Risk mitigation

### Disadvantages
1. Increased complexity
2. Duplicate data processing
3. Higher maintenance overhead
4. Additional storage requirements

---

## Approach 2: Direct Migration with Feature Flags
### Overview
Direct migration to new classification system using feature flags to control rollout and maintain temporary backward compatibility through views.

### Design
```mermaid
graph TD
    A[SAS Plugin] --> B[Feature Flag Controller]
    B --> C[Unified Risk Processor]
    C --> D[New Schema Tables]
    D --> E[Compatibility Views]
    D --> F[New Metrics]
    E --> G[Legacy Systems]
    F --> H[Updated Dashboard]
```

### Components
1. **Feature Flag Controller**
```python
class FeatureFlagController:
    def get_active_classification_system(self):
        return self.feature_flags.get('use_severity_classification', False)
    
    def process_with_flags(self, data):
        if self.get_active_classification_system():
            return self.process_severity_based(data)
        return self.process_legacy(data)
```

2. **Unified Schema**
```sql
CREATE TABLE unified_risk_metrics (
    risk_id VARCHAR,
    severity VARCHAR,
    legacy_classification VARCHAR,
    resource_name VARCHAR,
    risk_age INT,
    recommendation_age INT,
    metric_type VARCHAR
);

-- Compatibility View
CREATE VIEW legacy_blocked_risks AS
SELECT * FROM unified_risk_metrics
WHERE legacy_classification = 'BLOCKED';
```

3. **Migration Handler**
```python
class RiskMigrationHandler:
    def migrate_risk_data(self, risk_data):
        severity = self.map_legacy_to_severity(risk_data)
        return {
            'risk_id': risk_data.id,
            'severity': severity,
            'legacy_classification': risk_data.classification,
            'metrics': self.calculate_metrics(risk_data)
        }
```

### Implementation Phases
1. **Phase 1: Schema Update & Data Migration**
   - Deploy new schema
   - Create compatibility views
   - Migrate existing data

2. **Phase 2: Code Deployment**
   - Deploy feature-flagged code
   - Enable new data collection
   - Validate metrics

3. **Phase 3: Feature Flag Rollout**
   - Gradually enable new classification
   - Monitor metrics accuracy
   - Collect feedback

4. **Phase 4: Legacy Cleanup**
   - Remove feature flags
   - Clean up legacy views
   - Update documentation

### Advantages
1. Cleaner final architecture
2. Simplified maintenance
3. Better performance
4. Easier rollback capability

### Disadvantages
1. More complex initial migration
2. Requires careful coordination
3. Potential brief disruption
4. More upfront testing required

### Recommendations
**Recommend Approach 2** for the following reasons:
1. Long-term maintainability
2. Better performance characteristics
3. Cleaner architecture
4. Easier to extend for future changes

### Risk Mitigation
1. Comprehensive testing plan
2. Rollback procedures
3. Monitoring and alerts
4. Phased rollout
5. Data validation checks

### Success Metrics
1. Zero data loss during migration
2. Accurate risk reporting
3. Performance within SLA
4. No disruption to existing workflows
5. Complete migration within timeline

### Timeline
1. Development: 2 weeks
2. Testing: 1 week
3. Staged Rollout: 1 week
4. Monitoring & Stabilization: 1 week
5. Legacy Cleanup: 1 week

### Next Steps
1. Detailed technical specification
2. Test plan development
3. Migration script creation
4. Rollback plan documentation
5. Communication plan
6. Monitoring setup

------

venry: what about these 

Approaches for Risk Categorization

Approach 1: Direct RAGS API Integration

Description:

This approach leverages the existing RAGS API to directly obtain risk severity information. The API provides built-in severity classification, which simplifies the implementation and maintains consistency with Amazon's existing risk classification.


Steps:

1. Use the GetResourceHighLevelRiskDetailsByCampaignType API to get high-level details with severity information.
2. Process the built-in severity mapping.
3. Update the Redshift schema to include severity classification.

Code:

class SAS_Plugin:

def init(self, client):
self.client = client

def get_severity_based_risks(self, aggregate_id):
# Use GetResourceHighLevelRiskDetailsByCampaignType API
high_level_data = self.client.get_resource_high_level_risk_details_by_campaign_type(
aggregate_id=aggregate_id,
campaign_type="BLOCKED"
)
# Process built-in severity mapping
severity_counts = {
"EMERGENCY": high_level_data.riskSeverityMapWithRecommendations.get("Critical", 0),
"CRITICAL": high_level_data.riskSeverityMapWithRecommendations.get("High", 0),
"HIGH": high_level_data.riskSeverityMapWithRecommendations.get("Medium", 0)
}
return severity_counts

def update_redshift_data(self):
# Update Redshift schema to include severity
alter_table_query = """
ALTER TABLE {} ADD COLUMN severity VARCHAR(20) CHECK (severity IN ('EMERGENCY', 'CRITICAL', 'HIGH'))
"""
# Execute the alter table query
self.rs_upsert.execute_redshift_sql_query(alter_table_query.format("raw_sas_apollo_metrics"))
self.rs_upsert.execute_redshift_sql_query(alter_table_query.format("raw_sas_brazil_metrics"))
self.rs_upsert.execute_redshift_sql_query(alter_table_query.format("raw_sas_pipeline_target_metrics"))



Architecture Diagram

+-------------------+
| Managers |
+-------------------+
|
v
+-------------------------+
| SAS Plugin |
+-------------------------+
|
v
+-------------------------+
| AWS API Gateway |
+-------------------------+
/ | \
v v v
+----------------+ +----------------+ +----------------+
| AWS S3 | | AWS Redshift | | AWS EC2 |
+----------------+ +----------------+ +----------------+



Sequence Diagram

Manager SAS Plugin AWS API Gateway AWS Redshift AWS S3
| | | | |
|----Request Data| | | |
| |---Fetch Data------->| | |
| | |---Store Data----->| |
| | | | |
| |Store Processed Data------------------>| |
| | | | |
| | |Data Stored-----| |
| |<---Return Data------| | |
|<---Display Data| | | |



Advantages:

* Uses built-in RAGS severity classification.
* Minimal code changes required.
* Direct integration with existing API.
* Maintains data consistency.

Disadvantages:

* Limited customization options.
* Dependent on RAGS API availability.

Approach 2: Custom Severity Classification

Description:

This approach involves implementing custom logic to classify risks based on various factors such as risk age, recommendation age, and risk count. This provides flexibility in defining severity levels.


Steps:

1. Define custom classification logic.
2. Process each resource type and classify risks based on custom logic.
3. Update the Redshift database with the custom severity classification.

Code:

class SAS_Plugin:

def init(self, client):
self.client = client

def classify_risk_severity(self, risk_data):
# Custom classification logic
def get_severity(risk_age, recommendation_age, risk_count):
if risk_age > 30 or recommendation_age > 20:
return "EMERGENCY"
elif risk_age > 20 or risk_count > 5:
return "CRITICAL"
return "HIGH"

# Process each resource type
for resource_type in ["APOLLO", "BRAZIL", "PIPELINE_TARGET"]:
resource_data = self.get_resource_details(resource_type)
for risk in resource_data:
severity = get_severity(risk.riskAge, risk.recommendationAge, risk.riskCount)
self.update_risk_severity(risk.id, severity)

def update_risk_severity(self, risk_id, severity):
# Update the risk severity in Redshift
update_query = """
UPDATE raw_sas_apollo_metrics SET severity = '{}' WHERE id = '{}'
"""
self.rs_upsert.execute_redshift_sql_query(update_query.format(severity, risk_id))



Architecture Diagram

+-------------------+
| Managers |
+-------------------+
|
v
+-------------------------+
| SAS Plugin |
+-------------------------+
|
v
+-------------------------+
| AWS API Gateway |
+-------------------------+
/ | \
v v v
+----------------+ +----------------+ +----------------+
| AWS S3 | | AWS Redshift | | AWS EC2 |
+----------------+ +----------------+ +----------------+



Sequence Diagram

Manager SAS Plugin AWS API Gateway AWS Redshift AWS S3
| | | | |
|----Request Data| | | |
| |---Fetch Data------->| | |
| | |---Store Data----->| |
| | | | |
| |Store Processed Data------------------>| |
| | | | |
| | |Data Stored-----| |
| |<---Return Data------| | |
|<---Display Data| | | |



Advantages:

* Fully customizable classification logic.
* Independent of RAGS API changes.
* Can incorporate multiple data points.

Disadvantages:

* More complex maintenance.
* Potential inconsistency with RAGS classification.
* Higher processing overhead.

Approach 3: Hybrid Aggregation

Description:

This approach combines data from multiple sources, including high-level details, batch summaries, and historical data, to provide a richer context for severity classification.


Steps:

1. Get data from multiple sources.
2. Combine metrics for classification.
3. Process and store the results.

Code:

from collections import defaultdict

class SAS_Plugin:

def init(self, client):
self.client = client

def aggregate_risk_severity(self, manager):
# Get data from multiple sources
high_level_data = self.get_high_level_risk_details(manager)
batch_summary = self.get_batch_risk_summary(manager)
historical_data = self.get_weekly_risk_summary(manager)

# Combine metrics for classification
def calculate_severity(metrics):
if (metrics.riskAge > 30 or metrics.weekOverWeek > 0 or metrics.riskSeverity == "Critical"):
return "EMERGENCY"
elif (metrics.riskAge > 20 or metrics.monthOverMonth > 0 or metrics.riskSeverity == "High"):
return "CRITICAL"
return "HIGH"

# Process and store results
severity_distribution = defaultdict(int)
for resource in high_level_data:
severity = calculate_severity(resource)
severity_distribution[severity] += 1
return severity_distribution



Architecture Diagram

+-------------------+
| Managers |
+-------------------+
|
v
+-------------------------+
| SAS Plugin |
+-------------------------+
|
v
+-------------------------+
| AWS API Gateway |
+-------------------------+
/ | \
v v v
+----------------+ +----------------+ +----------------+
| AWS S3 | | AWS Redshift | | AWS EC2 |
+----------------+ +----------------+ +----------------+



Sequence Diagram

Manager SAS Plugin AWS API Gateway AWS Redshift AWS S3
| | | | |
|----Request Data| | | |
| |---Fetch Data------->| | |
| | |---Store Data----->| |
| | | | |
| |Store Processed Data------------------>| |
| | | | |
| | |Data Stored-----| |
| |<---Return Data------| | |
|<---Display Data| | | |



Advantages:

* Combines multiple data sources.
* Balances RAGS and custom classification.
* Provides richer context for severity.

Disadvantages:

* Most complex implementation.
* Higher API usage.
* More resource intensive.



------

Cedric: # High Level Design Document: SAS Risk Classification Enhancement

## Executive Summary
This document outlines approaches to enhance the SAS Plugin to handle new severity-based risk classification (Emergency, Critical, High) while maintaining existing functionality.

## Comparison of Approaches

### Approach 1: Direct RAGS API Integration
Leverages native RAGS API severity classification with minimal changes to existing architecture.

#### Technical Architecture
```mermaid
graph TD
    A[SAS Plugin] --> B[RAGS API Client]
    B --> C[GetResourceHighLevelRiskDetails]
    C --> D[Built-in Severity Mapping]
    D --> E[RedShift Tables]
    E --> F[Page-0 Dashboard]
```

#### Implementation Details
```python
class SASRiskProcessor:
    def process_severity_risks(self, aggregate_id):
        # Use native RAGS API severity classification
        response = self.client.get_resource_high_level_risk_details(
            aggregate_id=aggregate_id,
            campaign_type="ALL"  # Get all risks for severity filtering
        )
        
        severity_mapping = {
            "Critical": "EMERGENCY",
            "High": "CRITICAL",
            "Medium": "HIGH"
        }
        
        return self.map_and_store_risks(response, severity_mapping)
```

#### Schema Updates
```sql
-- Add severity columns to existing tables
ALTER TABLE rawdata_schema.raw_sas_apollo_metrics 
ADD COLUMN severity VARCHAR(10);

ALTER TABLE rawdata_schema.raw_sas_brazil_metrics 
ADD COLUMN severity VARCHAR(10);

ALTER TABLE rawdata_schema.raw_sas_pipeline_target_metrics 
ADD COLUMN severity VARCHAR(10);
```

### Approach 2: Hybrid Aggregation with Historical Context
Combines multiple data sources for comprehensive risk assessment and classification.

#### Technical Architecture
```mermaid
graph TD
    A[SAS Plugin] --> B[Data Aggregator]
    B --> C[RAGS API Client]
    B --> D[Historical Data]
    B --> E[Trend Analysis]
    C --> F[Risk Processor]
    D --> F
    E --> F
    F --> G[Classification Engine]
    G --> H[RedShift Tables]
    H --> I[Page-0 Dashboard]
```

#### Implementation Details
```python
class HybridRiskProcessor:
    def __init__(self):
        self.risk_threshold = self.load_risk_thresholds()
        self.trend_analyzer = TrendAnalyzer()
        
    def process_risks(self, manager_id):
        # Collect data from multiple sources
        current_risks = self.get_current_risks(manager_id)
        historical_data = self.get_historical_data(manager_id)
        trends = self.trend_analyzer.analyze(historical_data)
        
        # Comprehensive classification
        classified_risks = self.classify_risks(
            current_risks=current_risks,
            historical_data=historical_data,
            trends=trends
        )
        
        return self.store_classified_risks(classified_risks)

    def classify_risks(self, **data):
        classifications = []
        for risk in data['current_risks']:
            severity = self.determine_severity(
                risk=risk,
                history=data['historical_data'],
                trend=data['trends'].get(risk.id)
            )
            classifications.append({
                'risk_id': risk.id,
                'severity': severity,
                'metrics': self.calculate_metrics(risk)
            })
        return classifications
```

#### Enhanced Schema
```sql
-- New comprehensive risk tracking schema
CREATE TABLE enhanced_risk_metrics (
    risk_id VARCHAR PRIMARY KEY,
    severity VARCHAR(10),
    risk_age INT,
    recommendation_age INT,
    historical_severity VARCHAR(10),
    trend_indicator VARCHAR(10),
    last_updated TIMESTAMP,
    metric_snapshot VARCHAR(10),
    metric_type VARCHAR(20)
);

-- Aggregation view
CREATE VIEW risk_summary AS
SELECT 
    severity,
    COUNT(*) as risk_count,
    AVG(risk_age) as avg_risk_age,
    MAX(risk_age) as max_risk_age
FROM enhanced_risk_metrics
GROUP BY severity;
```

## Recommendation
Recommend implementing **Approach 2: Hybrid Aggregation** for the following reasons:

1. **Comprehensive Risk Assessment**
   - Considers multiple data points
   - Better accuracy in classification
   - Supports future enhancements

2. **Enhanced Metrics**
   - Richer context for risk evaluation
   - Better trend analysis
   - More actionable insights

3. **Future Proofing**
   - Flexible architecture
   - Easier to adapt to changes
   - Better scalability

## Implementation Plan

### Phase 1: Foundation (Week 1-2)
1. Schema updates
2. Data collection framework
3. Basic classification implementation

### Phase 2: Enhancement (Week 3-4)
1. Historical data integration
2. Trend analysis implementation
3. Advanced classification logic

### Phase 3: Migration (Week 5)
1. Data migration
2. Testing and validation
3. Rollout preparation

### Phase 4: Deployment (Week 6)
1. Gradual rollout
2. Monitoring
3. Feedback collection

## Success Metrics
1. Classification accuracy  95%
2. Zero data loss during migration
3. Response time  200ms
4. Successful processing of all risk types
5. Dashboard update time  5 minutes

## Risk Mitigation
1. **Data Integrity**
   - Validation checks
   - Backup procedures
   - Rollback capability

2. **Performance**
   - Batch processing
   - Caching strategy
   - Optimization techniques

3. **Availability**
   - Failover mechanisms
   - Error handling
   - Retry logic

## Monitoring and Maintenance
1. **Metrics**
   - Classification accuracy
   - Processing time
   - Error rates

2. **Alerts**
   - Classification failures
   - Processing delays
   - Data inconsistencies

3. **Documentation**
   - API documentation
   - Operational procedures
   - Troubleshooting guides

This design provides a robust foundation for handling the new severity-based classification while maintaining existing functionality and preparing for future enhancements.

------

venry: how would this and Approach 2: Direct Migration with Feature Flags and Approach 1: Parallel Processing Architecture , Approach 1: Direct RAGS API Integration and Approach 2: Hybrid Aggregation with Historical Context
differ

------

Cedric: # Comprehensive Comparison of All Approaches

## Overview Matrix

| Aspect | Direct Migration with Feature Flags | Parallel Processing | Direct RAGS API Integration | Hybrid Aggregation |
|--------|-------------------------------------|-------------------|---------------------------|-------------------|
| Primary Focus | Controlled rollout with backward compatibility | Dual-track processing | API-driven classification | Comprehensive data analysis |
| Complexity | Medium | High | Low | Very High |
| Risk Level | Medium | Low | Medium | High |
| Implementation Time | 4-5 weeks | 6-8 weeks | 2-3 weeks | 6-8 weeks |
| Resource Usage | Moderate | High | Low | Very High |

## Detailed Comparison

### 1. Architecture Design

**Direct Migration with Feature Flags**
```mermaid
graph TD
    A[SAS Plugin] --> B[Feature Flag Controller]
    B --> C[Unified Processor]
    C --> D[New Schema]
    D --> E[Legacy View]
    D --> F[New View]
```

**Parallel Processing**
```mermaid
graph TD
    A[SAS Plugin] --> B[Router]
    B --> C[Legacy Pipeline]
    B --> D[New Pipeline]
    C --> E[Legacy Storage]
    D --> F[New Storage]
    E --> G[Aggregator]
    F --> G
```

**Direct RAGS API**
```mermaid
graph TD
    A[SAS Plugin] --> B[RAGS API Client]
    B --> C[Severity Mapper]
    C --> D[Storage]
```

**Hybrid Aggregation**
```mermaid
graph TD
    A[SAS Plugin] --> B[Data Collector]
    B --> C[Historical Data]
    B --> D[Current Data]
    B --> E[Trend Analysis]
    C --> F[Classification Engine]
    D --> F
    E --> F
```

## Key Differences

### 1. Data Processing

**Feature Flags**
```python
class FeatureFlagProcessor:
    def process_risk(self, risk_data):
        if self.flags.use_new_classification:
            return self.process_severity_based(risk_data)
        return self.process_legacy(risk_data)
```

**Parallel Processing**
```python
class ParallelProcessor:
    def process_risk(self, risk_data):
        legacy_result = self.legacy_pipeline.process(risk_data)
        new_result = self.new_pipeline.process(risk_data)
        return self.combine_results(legacy_result, new_result)
```

**Direct RAGS**
```python
class RAGSProcessor:
    def process_risk(self, risk_data):
        severity = self.rags_client.get_severity(risk_data)
        return self.map_severity(severity)
```

**Hybrid**
```python
class HybridProcessor:
    def process_risk(self, risk_data):
        historical = self.get_historical_data(risk_data)
        trends = self.analyze_trends(historical)
        current = self.get_current_data(risk_data)
        return self.classify_with_context(current, historical, trends)
```

### 2. Storage Requirements

| Approach | Storage Type | Data Duplication | Migration Needs |
|----------|--------------|------------------|-----------------|
| Feature Flags | Single Schema + Views | No | Incremental |
| Parallel | Dual Schemas | Yes | Parallel |
| Direct RAGS | Single Schema | No | Minimal |
| Hybrid | Enhanced Schema | Partial | Complex |

### 3. Implementation Effort

| Component | Feature Flags | Parallel | Direct RAGS | Hybrid |
|-----------|--------------|----------|-------------|---------|
| Schema Changes | Moderate | High | Low | Very High |
| Code Changes | Moderate | High | Low | Very High |
| Testing Effort | High | Very High | Moderate | Extreme |
| Migration Effort | Moderate | Low | Low | High |

### 4. Risk Management

```python
# Feature Flags
class FeatureFlagRiskManager:
    def rollback(self):
        self.flags.disable_new_classification()
        return self.revert_to_legacy()

# Parallel Processing
class ParallelRiskManager:
    def handle_failure(self):
        return self.legacy_pipeline.process()  # Always have backup

# Direct RAGS
class RAGSRiskManager:
    def handle_api_failure(self):
        return self.use_cached_classification()

# Hybrid
class HybridRiskManager:
    def handle_failure(self):
        if self.historical_data_failed():
            return self.fallback_to_current()
        return self.use_partial_classification()
```

### 5. Performance Characteristics

| Metric | Feature Flags | Parallel | Direct RAGS | Hybrid |
|--------|--------------|----------|-------------|---------|
| Processing Time | Medium | High | Low | Very High |
| Memory Usage | Medium | High | Low | High |
| API Calls | Medium | High | Low | Very High |
| Storage I/O | Medium | High | Low | High |

### 6. Maintenance Requirements

```python
# Feature Flags
class FeatureFlagMaintenance:
    def cleanup(self):
        self.remove_legacy_views()
        self.cleanup_flags()

# Parallel
class ParallelMaintenance:
    def maintain(self):
        self.sync_pipelines()
        self.validate_consistency()

# Direct RAGS
class RAGSMaintenance:
    def update(self):
        self.update_api_mappings()

# Hybrid
class HybridMaintenance:
    def maintain(self):
        self.update_historical_data()
        self.optimize_storage()
        self.update_classification_rules()
```

### 7. Cost Implications

| Cost Factor | Feature Flags | Parallel | Direct RAGS | Hybrid |
|-------------|--------------|----------|-------------|---------|
| Development | Medium | High | Low | Very High |
| Infrastructure | Medium | High | Low | High |
| Maintenance | Medium | High | Low | High |
| Operations | Medium | High | Low | High |

## Recommendation Matrix

| Scenario | Recommended Approach |
|----------|---------------------|
| Quick Implementation Needed | Direct RAGS API |
| High Risk Aversion | Parallel Processing |
| Limited Resources | Feature Flags |
| Complex Requirements | Hybrid Aggregation |

This detailed comparison shows that each approach has its specific use case:

1. **Feature Flags**: Best for controlled rollout with moderate complexity
2. **Parallel Processing**: Best for risk-averse, gradual transitions
3. **Direct RAGS**: Best for quick, simple implementation
4. **Hybrid**: Best for comprehensive, data-driven classification

The choice depends on your specific constraints regarding:
- Implementation timeline
- Resource availability
- Risk tolerance
- Maintenance capacity
- Performance requirements

------

venry: which of these approaches requires the minimal change in the existing structure so that the other dependencies and workflows doesn't have any impact on them

------

Cedric: Based on the current SAS Plugin structure and dependencies, let's analyze each approach's impact on existing structures:

## Impact Analysis Matrix

| Component | Feature Flags | Parallel | Direct RAGS | Hybrid |
|-----------|--------------|----------|-------------|---------|
| Existing Code | Moderate | High | Low | Very High |
| Database Schema | Moderate | High | Low | High |
| API Calls | Minimal | High | Minimal | High |
| Dependencies | Moderate | High | Low | Very High |
| Workflows | Minimal | Moderate | Minimal | High |

## Detailed Analysis

### 1. Direct RAGS API Integration (LOWEST IMPACT)
```python
# Minimal changes to existing code structure
class SAS_Plugin:
    def get_high_level_resource_details(self, manager, _client):
        # Existing method with minor modification
        data = _client.get_resource_high_level_risk_details_by_campaign_type(
            aggregate_id = "Person:" + manager, 
            campaign_type='BLOCKED'  # Remains same for backward compatibility
        )
        
        # Add new severity mapping without changing existing logic
        severity_data = self.map_severity(data)
        return data, severity_data

    def map_severity(self, data):
        # New method that doesn't affect existing flow
        return {
            'EMERGENCY': data.get('Critical', []),
            'CRITICAL': data.get('High', []),
            'HIGH': data.get('Medium', [])
        }
```

Schema Changes:
```sql
-- Only add new columns without modifying existing ones
ALTER TABLE rawdata_schema.raw_sas_apollo_metrics 
ADD COLUMN severity VARCHAR(10) DEFAULT NULL;

-- Existing queries continue to work without modification
```

### Why Direct RAGS API Integration is the Least Disruptive:

1. **Code Structure Preservation**
```python
# Existing methods remain unchanged
def create_metrics(self, _client, metric_snapshot):
    entities = self.get_entities()
    distinct_managers = self.get_distinct_managers(entities)
    
    # Original flow remains the same
    for manager in distinct_managers:
        data = self.call_get_high_level_resource_details(manager, _client)
        # New severity processing doesn't affect existing flow
        severity = self.process_severity(data) if data else None
```

2. **Minimal Database Impact**
```python
def load_SAS_data_to_redshift(self, program_name):
    # Existing data loading remains unchanged
    if (program_name == SAS_Config.program_name_apollo):
        # Original logic stays the same
        self.load_existing_data()
        
        # Optional new severity data
        if self.has_severity_data():
            self.load_severity_data()
```

3. **API Compatibility**
```python
def call_batch_details(self, manager, _client, final_list_of_environments, type_of_env):
    # Existing API calls remain unchanged
    data = self.get_existing_data()
    
    # Optional new severity information
    if self.should_process_severity():
        severity_data = self.get_severity_data()
        data.update({'severity': severity_data})
    
    return data
```

4. **Workflow Preservation**
```python
def process_high_level_data(self, result, resource_type):
    # Existing workflow remains intact
    final_list_environments = []
    for environment in result:
        # Original processing
        env_name = environment_json_info['ResourceName']
        final_list_environments.append(env_name)
        
        # Optional severity processing
        if self.is_severity_enabled():
            self.process_severity_data(environment)
            
    return final_list_environments
```

## Implementation Strategy

1. **Phase 1: Add Severity Support**
```python
def extend_existing_functionality(self):
    # Step 1: Add new columns
    self.add_severity_columns()
    
    # Step 2: Add severity mapping
    self.implement_severity_mapping()
    
    # Step 3: Update data processing
    self.update_data_processing()
```

2. **Phase 2: Data Storage**
```python
def enhance_data_storage(self):
    # Add new columns without affecting existing ones
    self.add_new_columns()
    
    # Create optional views for severity data
    self.create_severity_views()
```

3. **Phase 3: Reporting**
```python
def enhance_reporting(self):
    # Add new severity reports alongside existing ones
    self.add_severity_reports()
    
    # Maintain existing report structure
    self.maintain_existing_reports()
```

## Benefits of This Approach

1. **Minimal Code Changes**
- Adds functionality without modifying existing code
- Preserves current behavior
- Easy to rollback if needed

2. **Database Compatibility**
- No breaking changes to existing schema
- Backward compatible queries
- Optional new functionality

3. **API Consistency**
- Uses existing API endpoints
- Maintains current authentication
- No new dependencies

4. **Workflow Preservation**
- Existing processes remain unchanged
- New functionality is additive
- No disruption to current operations

## Recommendation

Implement the **Direct RAGS API Integration** approach because:

1. Lowest impact on existing structure
2. Minimal changes required
3. Easy to implement and rollback
4. Maintains existing workflows
5. No disruption to dependencies

```python
# Implementation Example
class EnhancedSASPlugin(SAS_Plugin):
    def process_data(self, data):
        # Process existing data normally
        result = super().process_data(data)
        
        # Add severity information if available
        if self.is_severity_enabled():
            result.update(self.process_severity(data))
            
        return result
```

This approach allows for:
- Gradual adoption of new features
- Zero impact on existing functionality
- Easy maintenance and updates
- Minimal risk to current operations
- Simple rollback procedure if needed

The other approaches require more significant changes to the existing structure and would have greater impact on dependencies and workflows.

------

venry: what could the impact on altering the table structure?

------

Cedric: # Impact Analysis of Table Structure Alterations

## Current Table Structure
```sql
-- Example of current tables
1. rawdata_schema.raw_sas_apollo_metrics
2. rawdata_schema.raw_sas_brazil_metrics
3. rawdata_schema.raw_sas_pipeline_target_metrics
4. aggregate_schema.sas_risk_summary
```

## Potential Impacts

### 1. Data Loading Operations
```python
def load_SAS_data_to_redshift(self, program_name):
    # Current Operations
    try:
        # Truncate operations
        os.system(COMMANDS.Redshift_Operations_Command.format(
            ODIN.Redshift_ODIN,
            "truncate table {}".format(SAS_Config.temp_redshift_apollo_table)
        ))
        
        # Data loading operations
        os.system(COMMANDS.Redshift_Update_Comamnd.format(
            ODIN.Redshift_ODIN,
            "csv",
            SAS_Config.temp_redshift_apollo_table,
            Common_Constants.S3_Bucket+'/'+SAS_Config.sas_apollo_temp_s3_location
        ))
        
        # These operations could fail with schema changes
    except Exception as error:
        Logger.printLogs("ERROR", "Data loading failed: {}".format(error))
        sys.exit(1)
```

### 2. Dependent Systems

#### a. Page-0 Dashboard Queries
```sql
-- Existing queries might break
SELECT risk_count, metric_type, metric_snapshot 
FROM aggregate_schema.sas_risk_summary
WHERE metric_run_date = current_date;

-- Need to modify queries to handle new columns
SELECT 
    risk_count, 
    metric_type, 
    metric_snapshot,
    severity -- New column
FROM aggregate_schema.sas_risk_summary
WHERE metric_run_date = current_date;
```

#### b. CSV File Format
```python
def get_apollo_data_and_write_in_file(self, data, file, metric_snapshot):
    # Current CSV structure
    file.write("{},{},{},{},{},{},{},{},{},{},{}\n".format(
        env_name,
        stage_name,
        pipeline_id,
        pipeline_name,
        owner,
        owner_login,
        risk_count,
        # Adding new columns affects CSV structure
        severity,  # New column
        classification  # New column
    ))
```

### 3. Data Migration Requirements

```sql
-- Migration Script Requirements
BEGIN TRANSACTION;

-- 1. Create backup
CREATE TABLE rawdata_schema.raw_sas_apollo_metrics_backup AS 
SELECT * FROM rawdata_schema.raw_sas_apollo_metrics;

-- 2. Add new columns
ALTER TABLE rawdata_schema.raw_sas_apollo_metrics
ADD COLUMN severity VARCHAR(10),
ADD COLUMN risk_priority INT,
ADD COLUMN last_updated TIMESTAMP;

-- 3. Update existing data
UPDATE rawdata_schema.raw_sas_apollo_metrics
SET severity = 'HIGH'
WHERE days_non_compliant > 30;

-- 4. Verify data
SELECT COUNT(*) FROM rawdata_schema.raw_sas_apollo_metrics
WHERE severity IS NULL;

COMMIT;
```

### 4. Performance Implications

```sql
-- Index Requirements
CREATE INDEX idx_severity 
ON rawdata_schema.raw_sas_apollo_metrics(severity);

-- Analyze table statistics
ANALYZE rawdata_schema.raw_sas_apollo_metrics;
```

## Mitigation Strategies

### 1. Temporary Tables Approach
```sql
-- Create new table structure
CREATE TABLE rawdata_schema.raw_sas_apollo_metrics_new (
    -- Existing columns
    apollo_environment_name VARCHAR,
    stage_name VARCHAR,
    pipeline_id VARCHAR,
    -- New columns
    severity VARCHAR(10),
    risk_priority INT
);

-- Migrate data
INSERT INTO rawdata_schema.raw_sas_apollo_metrics_new
SELECT 
    *,
    NULL as severity,
    0 as risk_priority
FROM rawdata_schema.raw_sas_apollo_metrics;

-- Swap tables
ALTER TABLE rawdata_schema.raw_sas_apollo_metrics 
RENAME TO raw_sas_apollo_metrics_old;

ALTER TABLE rawdata_schema.raw_sas_apollo_metrics_new 
RENAME TO raw_sas_apollo_metrics;
```

### 2. View-Based Solution
```sql
-- Create view with new structure
CREATE OR REPLACE VIEW rawdata_schema.v_sas_apollo_metrics AS
SELECT 
    am.*,
    CASE 
        WHEN days_non_compliant > 30 THEN 'EMERGENCY'
        WHEN days_non_compliant > 20 THEN 'CRITICAL'
        ELSE 'HIGH'
    END as severity
FROM rawdata_schema.raw_sas_apollo_metrics am;

-- Update applications to use view instead of table
```

### 3. Parallel Table Strategy
```python
class ParallelTableManager:
    def manage_data_flow(self):
        # Write to both old and new tables
        self.write_to_legacy_table(data)
        self.write_to_new_table(data)
        
        # Verify data consistency
        self.verify_data_consistency()
        
    def verify_data_consistency(self):
        # Compare record counts
        legacy_count = self.get_legacy_count()
        new_count = self.get_new_count()
        assert legacy_count == new_count, "Data mismatch"
```

## Recommended Approach

### Phase 1: Add Nullable Columns
```sql
-- Add new columns as nullable
ALTER TABLE rawdata_schema.raw_sas_apollo_metrics
ADD COLUMN severity VARCHAR(10) NULL;

-- This allows existing processes to continue working
```

### Phase 2: Data Population
```python
class DataPopulator:
    def populate_new_columns(self):
        # Update data in batches
        batch_size = 1000
        offset = 0
        
        while True:
            records = self.get_batch(offset, batch_size)
            if not records:
                break
                
            self.update_batch(records)
            offset += batch_size
            
    def update_batch(self, records):
        try:
            with transaction.atomic():
                for record in records:
                    self.update_record(record)
        except Exception as e:
            Logger.printLogs("ERROR", f"Batch update failed: {e}")
```

### Phase 3: Update Dependencies
```python
class DependencyUpdater:
    def update_dependencies(self):
        # Update CSV generation
        self.update_csv_format()
        
        # Update queries
        self.update_queries()
        
        # Update aggregation logic
        self.update_aggregation()
    
    def update_csv_format(self):
        # Update CSV headers and row format
        self.csv_headers.append('severity')
        self.csv_format = self.csv_format + ',{}'
```

### Phase 4: Monitoring and Validation
```python
class DataValidator:
    def validate_data(self):
        # Check data consistency
        self.check_record_counts()
        
        # Verify data integrity
        self.verify_data_integrity()
        
        # Monitor query performance
        self.monitor_performance()
```

## Timeline and Resource Requirements

1. **Planning Phase** (1 week)
   - Impact analysis
   - Backup strategy
   - Rollback plan

2. **Implementation Phase** (2 weeks)
   - Schema changes
   - Data migration
   - Query updates

3. **Testing Phase** (1 week)
   - Data validation
   - Performance testing
   - Dependency verification

4. **Rollout Phase** (1 week)
   - Gradual deployment
   - Monitoring
   - Issue resolution

This structured approach minimizes risks while ensuring successful table structure modifications.

------

venry: How can the approach be tested with a duplicate table before using it on prod table

------

Cedric: # Testing Strategy with Duplicate Tables

## 1. Testing Environment Setup

### Create Duplicate Schema and Tables
```sql
-- Create test schema
CREATE SCHEMA IF NOT EXISTS test_rawdata_schema;

-- Create duplicate tables with new structure
CREATE TABLE test_rawdata_schema.raw_sas_apollo_metrics (
    LIKE rawdata_schema.raw_sas_apollo_metrics INCLUDING ALL,
    severity VARCHAR(10),
    risk_priority INT,
    last_updated TIMESTAMP
);

CREATE TABLE test_rawdata_schema.raw_sas_brazil_metrics (
    LIKE rawdata_schema.raw_sas_brazil_metrics INCLUDING ALL,
    severity VARCHAR(10),
    risk_priority INT,
    last_updated TIMESTAMP
);

CREATE TABLE test_rawdata_schema.raw_sas_pipeline_target_metrics (
    LIKE rawdata_schema.raw_sas_pipeline_target_metrics INCLUDING ALL,
    severity VARCHAR(10),
    risk_priority INT,
    last_updated TIMESTAMP
);
```

## 2. Test Plugin Implementation

### Create Test Plugin Class
```python
class TestSASPlugin(SAS_Plugin):
    def __init__(self):
        super().__init__()
        self.test_config = self.load_test_config()
        
    def load_test_config(self):
        return {
            'apollo_table': 'test_rawdata_schema.raw_sas_apollo_metrics',
            'brazil_table': 'test_rawdata_schema.raw_sas_brazil_metrics',
            'pipeline_table': 'test_rawdata_schema.raw_sas_pipeline_target_metrics',
            's3_test_bucket': 'sas-test-bucket'
        }
        
    def create_test_metrics(self, _client, metric_snapshot):
        """Override metrics creation for testing"""
        try:
            # Use test tables instead of production
            original_tables = self.backup_table_config()
            self.use_test_tables()
            
            # Process metrics with new severity logic
            self.process_test_metrics(_client, metric_snapshot)
            
        finally:
            # Restore original configuration
            self.restore_table_config(original_tables)
            
    def process_test_metrics(self, _client, metric_snapshot):
        """Process metrics with new severity classification"""
        entities = self.get_entities()
        distinct_managers = self.get_distinct_managers(entities)
        
        for manager in distinct_managers:
            # Process with new severity logic
            self.process_manager_risks(manager, _client)
```

## 3. Data Sampling and Testing

### Sample Data Processor
```python
class SampleDataProcessor:
    def __init__(self, production_data, sample_size=1000):
        self.production_data = production_data
        self.sample_size = sample_size
        
    def get_sample_data(self):
        """Get representative sample from production"""
        return self.production_data.order_by('RANDOM()').limit(self.sample_size)
        
    def process_sample(self):
        """Process sample data with new logic"""
        sample = self.get_sample_data()
        return self.apply_new_classification(sample)
        
    def apply_new_classification(self, sample):
        """Apply new severity classification"""
        classified_data = []
        for record in sample:
            severity = self.classify_severity(record)
            classified_data.append({
                'original': record,
                'new_classification': severity
            })
        return classified_data
```

## 4. Validation Framework

### Test Data Validator
```python
class TestDataValidator:
    def __init__(self, test_plugin):
        self.test_plugin = test_plugin
        self.validation_results = []
        
    def validate_data_integrity(self):
        """Validate test data processing"""
        # Check record counts
        self.validate_record_counts()
        
        # Check data consistency
        self.validate_data_consistency()
        
        # Check severity classification
        self.validate_severity_classification()
        
        return self.validation_results
        
    def validate_record_counts(self):
        """Ensure no data loss"""
        prod_count = self.get_production_count()
        test_count = self.get_test_count()
        
        self.validation_results.append({
            'test': 'record_count',
            'success': prod_count == test_count,
            'prod_count': prod_count,
            'test_count': test_count
        })
```

## 5. Performance Testing

### Performance Test Framework
```python
class PerformanceTestFramework:
    def __init__(self, test_plugin):
        self.test_plugin = test_plugin
        self.metrics = []
        
    def run_performance_tests(self):
        """Run performance tests on test tables"""
        # Test data loading
        self.test_data_loading()
        
        # Test query performance
        self.test_query_performance()
        
        # Test aggregation performance
        self.test_aggregation_performance()
        
        return self.metrics
        
    def test_data_loading(self):
        """Test data loading performance"""
        start_time = time.time()
        self.test_plugin.load_test_data()
        end_time = time.time()
        
        self.metrics.append({
            'test': 'data_loading',
            'duration': end_time - start_time
        })
```

## 6. Comparison Framework

### Results Comparator
```python
class ResultsComparator:
    def __init__(self, prod_data, test_data):
        self.prod_data = prod_data
        self.test_data = test_data
        
    def compare_results(self):
        """Compare production and test results"""
        differences = []
        
        # Compare metrics
        metric_diff = self.compare_metrics()
        
        # Compare classifications
        class_diff = self.compare_classifications()
        
        # Compare performance
        perf_diff = self.compare_performance()
        
        return {
            'metrics': metric_diff,
            'classifications': class_diff,
            'performance': perf_diff
        }
```

## 7. Test Execution Script

```python
def run_test_suite():
    # Initialize test environment
    test_plugin = TestSASPlugin()
    validator = TestDataValidator(test_plugin)
    perf_tester = PerformanceTestFramework(test_plugin)
    
    try:
        # Step 1: Sample production data
        sample_processor = SampleDataProcessor(production_data)
        test_data = sample_processor.process_sample()
        
        # Step 2: Process test data
        test_plugin.create_test_metrics(_client, 'test_snapshot')
        
        # Step 3: Validate results
        validation_results = validator.validate_data_integrity()
        
        # Step 4: Performance testing
        performance_results = perf_tester.run_performance_tests()
        
        # Step 5: Compare results
        comparator = ResultsComparator(production_data, test_data)
        comparison_results = comparator.compare_results()
        
        # Generate report
        generate_test_report(validation_results, performance_results, comparison_results)
        
    except Exception as e:
        Logger.printLogs("ERROR", f"Test suite failed: {e}")
        cleanup_test_environment()
```

## 8. Configuration Management

```python
class TestConfig:
    def __init__(self):
        self.test_settings = {
            'tables': {
                'apollo': 'test_rawdata_schema.raw_sas_apollo_metrics',
                'brazil': 'test_rawdata_schema.raw_sas_brazil_metrics',
                'pipeline': 'test_rawdata_schema.raw_sas_pipeline_target_metrics'
            },
            'sample_size': 1000,
            'performance_thresholds': {
                'data_loading': 60,  # seconds
                'query_execution': 5  # seconds
            }
        }
```

## 9. Test Report Generation

```python
def generate_test_report(validation_results, performance_results, comparison_results):
    report = {
        'timestamp': datetime.now(),
        'validation': validation_results,
        'performance': performance_results,
        'comparison': comparison_results,
        'recommendations': generate_recommendations(
            validation_results,
            performance_results,
            comparison_results
        )
    }
    
    # Save report
    save_test_report(report)
    
    # Send notifications
    notify_test_results(report)
```

This testing approach allows for:
- Safe testing of new functionality
- Performance impact assessment
- Data integrity validation
- Easy rollback if issues are found
- Comprehensive reporting
- Risk-free experimentation

The test results can then be used to make an informed decision about implementing changes in production.

------

venry: write select statements with limit 10 to see structure and results for these ables
redshift_apollo_table = "rawdata_schema.raw_sas_apollo_metrics"        	redshift_brazil_table = "rawdata_schema.raw_sas_brazil_metrics"        	redshift_pipeline_target_table = "rawdata_schema.raw_sas_pipeline_target_metrics"        	redshift_apollo_table_v2 = "rawdata_schema.sas_apollo_metrics_v2"        	redshift_brazil_table_v2 = "rawdata_schema.sas_brazil_metrics_v2"        	redshift_pipeline_target_table_v2 = "rawdata_schema.sas_pipeline_target_metrics_v2"        	temp_redshift_apollo_table = "rawdata_schema.temp_sas_apollo_metrics"        	temp_redshift_brazil_table = "rawdata_schema.temp_sas_brazil_metrics"        	temp_redshift_pipeline_target_table = "rawdata_schema.temp_sas_pipeline_target_metrics"        	rolled_redshift_apollo_table = "rawdata_schema.sas_apollo_metrics"        	rolled_redshift_brazil_table = "rawdata_schema.sas_brazil_metrics"        rolled_redshift_pipeline_target_table = "rawdata_schema.sas_pipeline_target_metrics"

------

Cedric: Here are the SELECT statements to examine the structure and sample data for each table:

1. **Raw SAS Apollo Metrics Tables**
```sql
-- Raw Apollo Metrics
SELECT * FROM rawdata_schema.raw_sas_apollo_metrics LIMIT 10;

-- Apollo Metrics V2
SELECT * FROM rawdata_schema.sas_apollo_metrics_v2 LIMIT 10;

-- Temp Apollo Metrics
SELECT * FROM rawdata_schema.temp_sas_apollo_metrics LIMIT 10;

-- Rolled Apollo Metrics
SELECT * FROM rawdata_schema.sas_apollo_metrics LIMIT 10;
```

2. **Raw SAS Brazil Metrics Tables**
```sql
-- Raw Brazil Metrics
SELECT * FROM rawdata_schema.raw_sas_brazil_metrics LIMIT 10;

-- Brazil Metrics V2
SELECT * FROM rawdata_schema.sas_brazil_metrics_v2 LIMIT 10;

-- Temp Brazil Metrics
SELECT * FROM rawdata_schema.temp_sas_brazil_metrics LIMIT 10;

-- Rolled Brazil Metrics
SELECT * FROM rawdata_schema.sas_brazil_metrics LIMIT 10;
```

3. **Raw SAS Pipeline Target Metrics Tables**
```sql
-- Raw Pipeline Target Metrics
SELECT * FROM rawdata_schema.raw_sas_pipeline_target_metrics LIMIT 10;

-- Pipeline Target Metrics V2
SELECT * FROM rawdata_schema.sas_pipeline_target_metrics_v2 LIMIT 10;

-- Temp Pipeline Target Metrics
SELECT * FROM rawdata_schema.temp_sas_pipeline_target_metrics LIMIT 10;

-- Rolled Pipeline Target Metrics
SELECT * FROM rawdata_schema.sas_pipeline_target_metrics LIMIT 10;
```

To examine table structures:

```sql
-- Apollo Tables Structure
SELECT column_name, data_type, character_maximum_length 
FROM information_schema.columns 
WHERE table_schema = 'rawdata_schema' 
AND table_name = 'raw_sas_apollo_metrics'
ORDER BY ordinal_position;

SELECT column_name, data_type, character_maximum_length 
FROM information_schema.columns 
WHERE table_schema = 'rawdata_schema' 
AND table_name = 'sas_apollo_metrics_v2'
ORDER BY ordinal_position;

-- Brazil Tables Structure
SELECT column_name, data_type, character_maximum_length 
FROM information_schema.columns 
WHERE table_schema = 'rawdata_schema' 
AND table_name = 'raw_sas_brazil_metrics'
ORDER BY ordinal_position;

SELECT column_name, data_type, character_maximum_length 
FROM information_schema.columns 
WHERE table_schema = 'rawdata_schema' 
AND table_name = 'sas_brazil_metrics_v2'
ORDER BY ordinal_position;

-- Pipeline Target Tables Structure
SELECT column_name, data_type, character_maximum_length 
FROM information_schema.columns 
WHERE table_schema = 'rawdata_schema' 
AND table_name = 'raw_sas_pipeline_target_metrics'
ORDER BY ordinal_position;

SELECT column_name, data_type, character_maximum_length 
FROM information_schema.columns 
WHERE table_schema = 'rawdata_schema' 
AND table_name = 'sas_pipeline_target_metrics_v2'
ORDER BY ordinal_position;
```

To compare data between original and v2 tables:
```sql
-- Apollo Comparison
SELECT 
    a.apollo_environment_name,
    a.stage_name,
    a.pipeline_id,
    a.owner_login,
    a.risk_count,
    b.apollo_environment_name as v2_name,
    b.stage_name as v2_stage,
    b.pipeline_id as v2_pipeline,
    b.owner_login as v2_owner,
    b.risk_count as v2_risk_count
FROM rawdata_schema.raw_sas_apollo_metrics a
LEFT JOIN rawdata_schema.sas_apollo_metrics_v2 b
    ON a.apollo_environment_name = b.apollo_environment_name
LIMIT 10;

-- Brazil Comparison
SELECT 
    a.brazil_versionset_name,
    a.pipeline_id,
    a.owner_login,
    a.risk_count,
    b.brazil_versionset_name as v2_name,
    b.pipeline_id as v2_pipeline,
    b.owner_login as v2_owner,
    b.risk_count as v2_risk_count
FROM rawdata_schema.raw_sas_brazil_metrics a
LEFT JOIN rawdata_schema.sas_brazil_metrics_v2 b
    ON a.brazil_versionset_name = b.brazil_versionset_name
LIMIT 10;

-- Pipeline Target Comparison
SELECT 
    a.pipeline_target_name,
    a.pipeline_id,
    a.owner_login,
    a.risk_count,
    b.pipeline_target_name as v2_name,
    b.pipeline_id as v2_pipeline,
    b.owner_login as v2_owner,
    b.risk_count as v2_risk_count
FROM rawdata_schema.raw_sas_pipeline_target_metrics a
LEFT JOIN rawdata_schema.sas_pipeline_target_metrics_v2 b
    ON a.pipeline_target_name = b.pipeline_target_name
LIMIT 10;
```

To check for recent data:
```sql
-- Recent Apollo Data
SELECT * 
FROM rawdata_schema.raw_sas_apollo_metrics 
WHERE metric_run_date >= CURRENT_DATE - 7
LIMIT 10;

-- Recent Brazil Data
SELECT * 
FROM rawdata_schema.raw_sas_brazil_metrics 
WHERE metric_run_date >= CURRENT_DATE - 7
LIMIT 10;

-- Recent Pipeline Target Data
SELECT * 
FROM rawdata_schema.raw_sas_pipeline_target_metrics 
WHERE metric_run_date >= CURRENT_DATE - 7
LIMIT 10;
```

To check data distribution:
```sql
-- Apollo Risk Distribution
SELECT 
    metric_snapshot,
    COUNT(*) as record_count,
    AVG(risk_count) as avg_risks,
    MAX(risk_count) as max_risks
FROM rawdata_schema.raw_sas_apollo_metrics
GROUP BY metric_snapshot
LIMIT 10;

-- Brazil Risk Distribution
SELECT 
    metric_snapshot,
    COUNT(*) as record_count,
    AVG(risk_count) as avg_risks,
    MAX(risk_count) as max_risks
FROM rawdata_schema.raw_sas_brazil_metrics
GROUP BY metric_snapshot
LIMIT 10;

-- Pipeline Target Risk Distribution
SELECT 
    metric_snapshot,
    COUNT(*) as record_count,
    AVG(risk_count) as avg_risks,
    MAX(risk_count) as max_risks
FROM rawdata_schema.raw_sas_pipeline_target_metrics
GROUP BY metric_snapshot
LIMIT 10;
```

These queries will help you:
1. Understand the table structures
2. Compare data between original and v2 tables
3. Check recent data
4. Analyze data distribution
5. Verify data consistency

You can modify the WHERE clauses and add additional conditions based on your specific needs.

------

venry: SELECT * FROM rawdata_schema.raw_sas_apollo_metrics LIMIT 10;
apollo_environment_name	apollo_environment_stage	pipeline_id	pipeline_name	owner	owner_login	consumers	children	software_count	visibility	host_count	days_non_compliant	days_non_compliant_recommendation	apollo_action	blocked_resources	associated_campaigns	metric_run_date	metric_year	classification	metric_snapshot	snapshot_number
GlidePlatformHorizonte/EU/InAppBilling:Gamma	Gamma	None	None	Ravindran Rethinam	raviret	0	0	1	Internal	1	360	-1	APOLLO_NO_ACTION	Jetty:8.1.12	CDO Blocked Software Campaign - Jan 2022	2025-02-17	2025	None	WoW	8
TPSGenerator/SSA/FE:Gamma	Gamma	None	None	Mashkoor Alam	alammash	0	0	5	Internal	1	88	-1	APOLLO_NO_ACTION	Netty:3.10	CDO Blocked Software Campaign - Sep 2021	2025-02-17	2025	None	WoW	8
TPSGenerator/ATSTPSGeneratorControllerEU:Prod	Prod	None	None	Vidhya Prakash Sivashanmugam	vsivasha	0	0	5	Internal	1	264	208	APOLLO_CAN_SYNC	SnakeYAML:1.16	CDO Blocked Software Campaign - Jan 2022	2025-02-17	2025	None	WoW	8
GlidePlatformHorizonte/NA/OneBox:Prod	Prod	127929	GlidePlatform-master	Ravindran Rethinam	raviret	0	0	1	Internal	2	1130	-1	APOLLO_NO_ACTION	Jetty:8.1.12	CDO Blocked Software Campaign - Jan 2022	2025-02-17	2025	None	WoW	8
KindleConversionDevUtils:Alpha	Alpha	None	None	Arunkumar Subramanian	sznaru	0	0	2	Internal	20	260	-1	APOLLO_NO_ACTION	JakartaCommons-beanutils:1.0	CDO Blocked Software Campaign - Jan 2022	2025-02-17	2025	None	WoW	8
ShipTrack2Service/EU:Beta	Beta	563	ShipTrack2Service	Abhijeet Jha	abhijeej	0	2	2	Internal	1	1634	5	APOLLO_CAN_UPDATE_VERSION_SET	JettyBSFHTTPServer:4.1	CDO Blocked Software Campaign - Aug 2020	2025-02-17	2025	None	WoW	8
GordianKnot/jcrites:Prod	Prod	None	None	Daniel Lloyd	danlloyd	0	0	2	None	0	1130	1130	APOLLO_CAN_DELETE	Graphviz:2.26.3	CDO Blocked Software Campaign - Jan 2022	2025-02-17	2025	None	WoW	8
SDETools/lunadong:Prod	Prod	None	None	Daniel Lloyd	danlloyd	0	0	5	Internal	1	1840	1647	APOLLO_CAN_DELETE	Sleepycat:4.6	Blocked Software Campaign - Feb 2020	2025-02-17	2025	None	WoW	8
MLEnvImprovement/markliu:Prod	Prod	None	None	Daniel Lloyd	danlloyd	0	0	16	Internal	1	1130	1130	APOLLO_CAN_DELETE	Dom4j:1.0	CDO Blocked Software Campaign - Jan 2022	2025-02-17	2025	None	WoW	8
MLEnvImprovement/markliu:Prod	Prod	None	None	Daniel Lloyd	danlloyd	0	0	16	Internal	1	2434	1319	APOLLO_CAN_DELETE	JDK:1.6_64	Blocked Software Campaign - Dec 2018	2025-02-17	2025	None	WoW	8SELECT * FROM rawdata_schema.sas_apollo_metrics_v2 LIMIT 10;
apollo_environment_name	apollo_environment_stage	pipeline_id	pipeline_name	owner	owner_login	consumers	children	software_count	visibility	host_count	days_non_compliant	days_non_compliant_recommendation	apollo_action	blocked_resources	associated_campaigns	metric_run_date	metric_year	classification	metric_snapshot	metric_snapshot_number	aggregate_roll_up_to	resolved_user_updated_date	resolved_user
HedwigPipelineTestNotifications/NA:Prod	Prod	418127	HedwigPipelineTestNotifications	Vidhi Bhagdev	vidhi	0	0	5	None	0	220	36	APOLLO_CAN_DEPLOY_VERSION_SET_REVISION	Jackson-databind:2.8.x	CDO Blocked Software Campaign - Aug 2020	2021-03-31	2021	None	MoM	13	adji
YourOrdersDisplayService/Integ/EU/Devo:Beta	Beta	185122	YourOrdersDisplayService	Pooja Sardesai	poojasar	0	0	7	Internal	2	11	11	APOLLO_CAN_UPDATE_VERSION_SET	XStream:1.4.8	[Security][Internal-Important]- CDO Blocked Software Campaign - March 2021	2021-03-31	2021	None	MoM	13	adji
ShiptrackShortStatusLoader/CN:Beta	Beta	7219	ShiptrackShortStatusLoader-release	Arti Kansal	artiag	0	1	9	Internal	1	163	45	APOLLO_CAN_UPDATE_VERSION_SET	Jackson-datatype-jsr310:2.8.x	CDO Blocked Software Campaign - Aug 2020	2021-03-31	2021	None	MoM	13	adji
ShiptrackShortStatusLoader/FE:Gamma	Gamma	7219	ShiptrackShortStatusLoader-release	Arti Kansal	artiag	0	1	8	None	0	163	40	APOLLO_CAN_UPDATE_VERSION_SET	Jackson-datatype-guava:2.8.x	CDO Blocked Software Campaign - Aug 2020	2021-03-31	2021	None	MoM	13	adji
CondoService/Integ/NA/Devo/maulmodi:Beta	Beta	None	None	maulik modi	maulmodi	0	0	5	Internal	1	220	13	APOLLO_CAN_SYNC	AmazonCommons-jmx:1.0	CDO Blocked Software Campaign - Aug 2020	2021-03-31	2021	None	MoM	13	adji
OctaneBrazilTools/kuppr:Prod	Prod	None	None	Karthik Senthil	karsenth	0	0	7	Internal	1	163	163	APOLLO_CAN_DELETE	Jackson-module-jaxb-annotations:2.8.x	CDO Blocked Software Campaign - Aug 2020	2021-03-31	2021	None	MoM	13	adji
HedwigOrderOnHoldNotifications/EU:Beta	Beta	246990	HedwigOrderOnHoldNotifications	Vidhi Bhagdev	vidhi	0	0	5	None	0	136	1	APOLLO_CAN_DEPLOY_VERSION_SET_REVISION	Jackson-databind:2.8.x	CDO Blocked Software Campaign - Aug 2020	2021-03-31	2021	None	MoM	13	adji
SonorusDisplayService/EU/OneBox:Prod	Prod	637599	SonorusDisplayService	Vidhi Bhagdev	vidhi	0	0	7	Internal	1	163	48	APOLLO_CAN_UPDATE_VERSION_SET	Jackson-datatype-guava:2.9.x	CDO Blocked Software Campaign - Aug 2020	2021-03-31	2021	None	MoM	13	adji
OrderHistoryAggregator/US2/Batch:Prod	Prod	5641	OrderHistoryAggregator	Indraneel Palyam	ipalyam	0	1	11	Internal	1	163	39	APOLLO_CAN_UPDATE_VERSION_SET	Jackson-datatype-guava:2.8.x	CDO Blocked Software Campaign - Aug 2020	2021-03-31	2021	None	MoM	13	adji
OnlineProxyFleet/US/TPATemplate/cder-env3:Gamma	Gamma	None	None	Greg Chan	gregorc	0	0	5	Internal	1	426	73	APOLLO_CAN_DEPLOY_VERSION_SET_REVISION	Sleepycat:5.2	Blocked Software Campaign - Feb 2020	2021-03-31	2021	None	MoM	13	adjiSELECT * FROM rawdata_schema.temp_sas_apollo_metrics LIMIT 10;
apollo_environment_name	apollo_environment_stage	pipeline_id	pipeline_name	owner	owner_login	consumers	children	software_count	visibility	host_count	days_non_compliant	days_non_compliant_recommendation	apollo_action	blocked_resources	associated_campaigns	metric_run_date	metric_year	classification	metric_snapshot	snapshot_number
PlanningWorldService/NA:Beta	Beta	182054	PlanningWorldService	John O'Connor	jconnn	0	4	2	Internal	1	567	-1	APOLLO_NO_ACTION	Netty:3.6	CDO Blocked Software Campaign Jul 2021 PMV	2025-02-17	2025	None	WoW	8
PlanningSessionExecutor/EU:Gamma	Gamma	186213	PlanningSessionExecutor	John O'Connor	jconnn	0	2	1	Internal	2	1130	-1	APOLLO_NO_ACTION	Jetty:8.1.12	CDO Blocked Software Campaign - Jan 2022	2025-02-17	2025	None	WoW	8
PlanningSessionExecutor/NA/Testing:Prod	Prod	186213	PlanningSessionExecutor	John O'Connor	jconnn	0	0	1	Internal	2	1042	-1	APOLLO_NO_ACTION	Jetty:8.1.12	CDO Blocked Software Campaign - Jan 2022	2025-02-17	2025	None	WoW	8
goodreads-www/na/dev/mwadikar:Alpha	Alpha	None	None	Mugdha Ravindra Wadikar	mwadikar	0	0	1	Internal	1	1172	22	APOLLO_CAN_UPDATE	Redis:2.8.x	CDO Blocked Software Campaign Jul 2021 PMV	2025-02-17	2025	None	WoW	8
goodreads-www/na/dev/arushiar-rev2:Alpha	Alpha	None	None	Arushi Arora	arushiar	0	0	1	None	0	242	20	APOLLO_CAN_UPDATE	Redis:2.8.x	CDO Blocked Software Campaign Jul 2021 PMV	2025-02-17	2025	None	WoW	8
DataMuncher:Alpha	Alpha	74079	DataMuncher	Manik Kumar	manikk	0	0	1	Internal	1	2048	1042	APOLLO_CAN_DELETE	Jackson-databind:2.5.x	Blocked Software Campaign - Jul 2019	2025-02-17	2025	None	WoW	8
goodreads-www/na/dev/bhavenvi:Alpha	Alpha	None	None	Khom Kaowthumrong	khomkrit	0	0	1	Internal	1	625	21	APOLLO_CAN_UPDATE	Redis:2.8.x	CDO Blocked Software Campaign Jul 2021 PMV	2025-02-17	2025	None	WoW	8
goodreads-www/na/dev/bhavenvi:Alpha	Alpha	None	None	Khom Kaowthumrong	khomkrit	0	0	1	Internal	1	625	21	APOLLO_CAN_UPDATE	Redis:2.8.x	CDO Blocked Software Campaign Jul 2021 PMV	2025-02-17	2025	None	WoW	8
MLPython/manindra:Beta	Beta	None	None	Josh Tang	joshtang	0	0	2	Internal	1	1311	1311	APOLLO_CAN_DELETE	SQLite:8417e5a6efbfeba959acebae069166788b91ab55	CDO Blocked Software List Jul 2021 Commit	2025-02-17	2025	None	WoW	8
MLPython/manindra:Beta	Beta	None	None	Josh Tang	joshtang	0	0	2	Internal	1	1840	1645	APOLLO_CAN_DELETE	Sleepycat:4.6	Blocked Software Campaign - Feb 2020	2025-02-17	2025	None	WoW	8SELECT * FROM rawdata_schema.sas_apollo_metrics LIMIT 10;
apollo_environment_name	apollo_environment_stage	pipeline_id	pipeline_name	owner	owner_login	consumers	children	software_count	visibility	host_count	days_non_compliant	days_non_compliant_recommendation	apollo_action	blocked_resources	associated_campaigns	metric_run_date	metric_year	classification	metric_snapshot	metric_snapshot_number	aggregate_roll_up_to
DevDesktopAL2/jallam-1-click-al2:Prod	Prod	3183499	DevDesktop-jallam-1-click-al2-release	Jalal Allam	jallam	0	0	1	Internal	1	11	9	APOLLO_CAN_UPDATE	Libssh2:1.x	[Security][Internal-Important]- CDO Blocked Software Campaign - March 2021	2021-03-31	2021	None	MoM	13	aaaburas
CSNotificationControllerService/NA/halmaani:Beta	Beta	None	None	Hussein Al-Ma'ani	halmaani	0	0	5	Internal	1	6	-1	APOLLO_NO_ACTION	XStream:1.3	[Security][Internal-Important]- CDO Blocked Software Campaign - March 2021	2021-03-31	2021	None	MoM	13	aalfar
FlareService/NA/baraarah:Beta	Beta	None	None	Baraa' Rahhal	baraarah	0	0	8	Internal	1	25	-1	APOLLO_NO_ACTION	Jackson-databind:2.8.x	CDO Blocked Software Campaign - Aug 2020	2021-03-31	2021	None	MoM	13	aalfar
CODPaymentEligibilityService:Gamma	Gamma	712484	CODPaymentEligibilityService	Meteb Alnueimat	mahmadn	0	0	5	Internal	2	220	-1	APOLLO_NO_ACTION	AmazonCommons-jmx:1.0	CDO Blocked Software Campaign - Aug 2020	2021-03-31	2021	None	MoM	13	aalfar
FlareService/NA/ankhalid:Beta	Beta	None	None	Khaled Al-Ansari	ankhalid	0	0	8	None	1	163	-1	APOLLO_NO_ACTION	Jackson-dataformat-yaml:2.8.x	CDO Blocked Software Campaign - Aug 2020	2021-03-31	2021	None	MoM	13	aalfar
CROWSILService/EU/talkahwa:Beta	Beta	None	None	Tawfik Alkahwaji	talkahwa	0	0	5	Internal	1	25	-1	APOLLO_NO_ACTION	Jetty:9.4.x	CDO Blocked Software Campaign - Aug 2020	2021-03-31	2021	None	MoM	13	aalfar
FlareService/NA/ankhalid:Beta	Beta	None	None	Khaled Al-Ansari	ankhalid	0	0	8	None	1	216	-1	APOLLO_NO_ACTION	AmazonCommons-jmx:1.0	CDO Blocked Software Campaign - Aug 2020	2021-03-31	2021	None	MoM	13	aalfar
OTPforCODService/HEX3/Prod/Default/IN/PreProd:Gamma	Gamma	81204	OTPforCODService-mainline	Meteb Alnueimat	mahmadn	0	0	1	Internal	1	220	-1	APOLLO_NO_ACTION	AmazonCommons-jmx:1.0	CDO Blocked Software Campaign - Aug 2020	2021-03-31	2021	None	MoM	13	aalfar
RegionalTodWorker/DUB/GCAccountingSilService:Prod	Prod	None	None	Momen Abunafiseh	abunafis	0	0	3	Internal	1	162	-1	APOLLO_NO_ACTION	Jackson-dataformat-xml:2.8.x	CDO Blocked Software Campaign - Aug 2020	2021-03-31	2021	None	MoM	13	aalfar
CODPaymentEligibilityService:Gamma	Gamma	712484	CODPaymentEligibilityService	Meteb Alnueimat	mahmadn	0	0	5	Internal	2	164	-1	APOLLO_NO_ACTION	Jackson-module-jaxb-annotations:2.8.x	CDO Blocked Software Campaign - Aug 2020	2021-03-31	2021	None	MoM	13	aalfar

STRUCTURE FOR PIPELINE METRICS BRAZIL WOULD BE SAME  
SELECT * FROM aggregate_schema.sas_risk_summary LIMIT 10;

risk_count	metric_rolled_up_to	metric_run_date	metric_snapshot	metric_snapshot_number	aggregate_roll_up_to	metric_year	metric_type
8	dmmitta	2020-06-21	WoW	25	abbaamer	2020	recommendations
4	smanjeer	2020-06-21	WoW	25	abbaamer	2020	recommendations
4	mkkhleif	2020-06-21	WoW	25	abbaamer	2020	recommendations
11	abhsriva	2020-06-21	WoW	25	abbaamer	2020	recommendations
3	qazzeh	2020-06-21	WoW	25	abbaamer	2020	recommendations
4	rajyda	2020-06-21	WoW	25	abbaamer	2020	recommendations
135	qrababah	2020-06-21	WoW	25	abbaamer	2020	total
37	adurrani	2020-06-21	WoW	25	abbaamer	2020	total
313	abbaamer	2020-06-21	WoW	25	abbaamer	2020	total
29	lbbotero	2020-06-21	WoW	25	colbywar	2020	total

------

Cedric: Based on the table structures shown, here's an analysis and approach for adding severity classification:

1. **Current Structure Analysis**

**Apollo Metrics Structure:**
```sql
-- Key fields in raw_sas_apollo_metrics
- apollo_environment_name
- apollo_environment_stage
- days_non_compliant
- days_non_compliant_recommendation
- apollo_action
- blocked_resources
- associated_campaigns
- metric_snapshot (WoW/MoM)
```

**Aggregated Risk Summary Structure:**
```sql
-- Key fields in sas_risk_summary
- risk_count
- metric_type (recommendations/total)
- metric_snapshot (WoW/MoM)
- aggregate_roll_up_to
```

2. **Proposed Schema Changes**

```sql
-- Add severity classification to raw_sas_apollo_metrics
ALTER TABLE rawdata_schema.raw_sas_apollo_metrics
ADD COLUMN severity VARCHAR(10) DEFAULT NULL,
ADD COLUMN risk_priority INT DEFAULT 0;

-- Add severity classification to raw_sas_brazil_metrics
ALTER TABLE rawdata_schema.raw_sas_brazil_metrics
ADD COLUMN severity VARCHAR(10) DEFAULT NULL,
ADD COLUMN risk_priority INT DEFAULT 0;

-- Add severity classification to raw_sas_pipeline_target_metrics
ALTER TABLE rawdata_schema.raw_sas_pipeline_target_metrics
ADD COLUMN severity VARCHAR(10) DEFAULT NULL,
ADD COLUMN risk_priority INT DEFAULT 0;

-- Add severity metrics to aggregate_schema.sas_risk_summary
ALTER TABLE aggregate_schema.sas_risk_summary
ADD COLUMN severity VARCHAR(10) DEFAULT NULL;
```

3. **Severity Classification Logic**
```sql
-- Example classification view
CREATE OR REPLACE VIEW v_apollo_severity_classification AS
SELECT 
    *,
    CASE 
        WHEN days_non_compliant > 30 OR days_non_compliant_recommendation > 20 THEN 'EMERGENCY'
        WHEN days_non_compliant > 20 OR apollo_action = 'APOLLO_CAN_UPDATE' THEN 'CRITICAL'
        ELSE 'HIGH'
    END as severity,
    CASE 
        WHEN days_non_compliant > 30 THEN 3
        WHEN days_non_compliant > 20 THEN 2
        ELSE 1
    END as risk_priority
FROM rawdata_schema.raw_sas_apollo_metrics;
```

4. **Test Implementation**
```sql
-- Create test table with new structure
CREATE TABLE test_rawdata_schema.test_sas_apollo_metrics (
    LIKE rawdata_schema.raw_sas_apollo_metrics INCLUDING ALL,
    severity VARCHAR(10),
    risk_priority INT
);

-- Test data population
INSERT INTO test_rawdata_schema.test_sas_apollo_metrics
SELECT 
    *,
    CASE 
        WHEN days_non_compliant > 30 THEN 'EMERGENCY'
        WHEN days_non_compliant > 20 THEN 'CRITICAL'
        ELSE 'HIGH'
    END,
    CASE 
        WHEN days_non_compliant > 30 THEN 3
        WHEN days_non_compliant > 20 THEN 2
        ELSE 1
    END
FROM rawdata_schema.raw_sas_apollo_metrics;
```

5. **Validation Queries**
```sql
-- Check severity distribution
SELECT 
    severity,
    COUNT(*) as risk_count,
    AVG(days_non_compliant) as avg_days,
    MAX(days_non_compliant) as max_days
FROM test_rawdata_schema.test_sas_apollo_metrics
GROUP BY severity;

-- Compare with original data
SELECT 
    a.apollo_environment_name,
    a.days_non_compliant,
    a.apollo_action,
    b.severity,
    b.risk_priority
FROM rawdata_schema.raw_sas_apollo_metrics a
JOIN test_rawdata_schema.test_sas_apollo_metrics b
    ON a.apollo_environment_name = b.apollo_environment_name
LIMIT 10;
```

6. **Aggregate Metrics**
```sql
-- New aggregation query
SELECT 
    aggregate_roll_up_to,
    severity,
    COUNT(*) as risk_count,
    metric_snapshot,
    metric_year
FROM test_rawdata_schema.test_sas_apollo_metrics
GROUP BY 
    aggregate_roll_up_to,
    severity,
    metric_snapshot,
    metric_year;
```

This approach:
1. Maintains existing functionality
2. Adds severity classification
3. Allows for testing without affecting production
4. Preserves historical data
5. Enables gradual rollout

The test table approach allows you to verify the changes without affecting the production tables, making it safer to implement and validate the new severity classification system.

------

venry: Approaches for Risk Categorization

Approach 1: Direct RAGS API Integration

Description:

This approach leverages the existing RAGS API to directly obtain risk severity information. The API provides built-in severity classification, which simplifies the implementation and maintains consistency with Amazon's existing risk classification.

Changes Required:

* Initialize the SAS client using the RAGS API.
* Use the GetResourceHighLevelRiskDetailsByCampaignType API to fetch high-level risk details.
* Process the data to extract severity information.
* Store the processed data in AWS Redshift.
* Update the Page-0 Dashboard to reflect the new severity-based risk categorization.
* Add a new column for severity classification in the Redshift tables.

Approach 2: Custom Severity Classification

Description:

This approach involves implementing custom logic to classify risks based on various factors such as risk age, recommendation age, and risk count. This provides flexibility in defining severity levels.

Changes Required:

* Implement custom logic to classify risks based on predefined criteria.
* Process the data to extract custom severity information.
* Store the processed data in AWS Redshift.
* Update the Page-0 Dashboard to reflect the custom severity-based risk categorization.
* Add a new column for custom severity classification in the Redshift tables.


I wanna use approach one, 

give me detailed steps to implemetnt the approach 1 changes in sas plugin and config step by step,

I've created test tables with 
CREATE TABLE aeetest.raw_sas_apollo_metrics AS SELECT * FROM rawdata_schema.raw_sas_apollo_metrics;
CREATE TABLE aeetest.raw_sas_brazil_metrics AS SELECT * FROM rawdata_schema.raw_sas_brazil_metrics;
CREATE TABLE aeetest.raw_sas_pipeline_target_metrics AS SELECT * FROM rawdata_schema.raw_sas_pipeline_target_metrics;
CREATE TABLE aeetest.sas_risk_summary AS SELECT * FROM aggregate_schema.sas_risk_summary;
and added severity to all of them and risk_priority to 3 of them


------

Cedric: # Implementation Guide: Direct RAGS API Integration Approach

## 1. Update SAS_Config Class
```python
class SAS_Config:
    # Add new configurations for severity
    SEVERITY_EMERGENCY = 'EMERGENCY'
    SEVERITY_CRITICAL = 'CRITICAL'
    SEVERITY_HIGH = 'HIGH'
    
    # Update table names for testing
    test_redshift_apollo_table = "aeetest.raw_sas_apollo_metrics"
    test_redshift_brazil_table = "aeetest.raw_sas_brazil_metrics"
    test_redshift_pipeline_target_table = "aeetest.raw_sas_pipeline_target_metrics"
    test_sas_risk_summary = "aeetest.sas_risk_summary"
    
    # Add new query templates for severity
    severity_update_query = """
        UPDATE {}
        SET severity = %s,
            risk_priority = %s
        WHERE apollo_environment_name = %s
        AND metric_run_date = %s
    """
    
    # Add aggregation query for severity
    severity_aggregation_query = """
        INSERT INTO {} (
            risk_count, 
            metric_rolled_up_to, 
            metric_run_date, 
            metric_snapshot, 
            metric_snapshot_number, 
            aggregate_roll_up_to, 
            metric_year, 
            metric_type,
            severity
        )
        SELECT 
            COUNT(*) as risk_count,
            %s as metric_rolled_up_to,
            %s as metric_run_date,
            %s as metric_snapshot,
            %s as metric_snapshot_number,
            %s as aggregate_roll_up_to,
            %s as metric_year,
            %s as metric_type,
            severity
        FROM {}
        WHERE metric_run_date = %s
        GROUP BY severity
    """
```

## 2. Modify SAS_Plugin Class
```python
class SAS_Plugin:
    def __init__(self):
        Logger.printLogs("INFO", "Triggering SAS Plugin with Severity Classification")
        self.rs_upsert = redshift_operation_wrapper()
        
    def process_severity_classification(self, high_level_data):
        """Process severity from RAGS API response"""
        severity_mapping = {
            'Critical': SAS_Config.SEVERITY_EMERGENCY,
            'High': SAS_Config.SEVERITY_CRITICAL,
            'Medium': SAS_Config.SEVERITY_HIGH
        }
        
        risk_priority_mapping = {
            SAS_Config.SEVERITY_EMERGENCY: 3,
            SAS_Config.SEVERITY_CRITICAL: 2,
            SAS_Config.SEVERITY_HIGH: 1
        }
        
        return {
            'severity': severity_mapping.get(high_level_data.get('riskSeverity'), SAS_Config.SEVERITY_HIGH),
            'risk_priority': risk_priority_mapping.get(severity_mapping.get(high_level_data.get('riskSeverity')), 1)
        }

    def call_get_high_level_resource_details(self, manager, _client):
        """Modified to include severity information"""
        try:
            data = _client.get_resource_high_level_risk_details_by_campaign_type(
                aggregate_id = "Person:" + manager,
                campaign_type='BLOCKED'
            )
            
            # Process severity information
            severity_data = self.process_severity_classification(data)
            data.update({'severity_info': severity_data})
            
            return data
            
        except ThrottlingException as e:
            Logger.printLogs("EXCEPTION", f"ThrottlingException occurred: {e}")
            time.sleep(5)
        except Exception as err:
            Logger.printLogs("ERROR", f"Error occurred: {err}")
            return 1

    def get_apollo_data_and_write_in_file(self, data, file, metric_snapshot):
        """Modified to include severity information"""
        list_data = self.coral_to_python(data)
        for filtered_depth_data in list_data['Details']:
            # Extract existing data
            env_name = filtered_depth_data['Name']
            # ... [existing extractions]
            
            # Extract severity information
            severity_info = data.get('severity_info', {
                'severity': SAS_Config.SEVERITY_HIGH,
                'risk_priority': 1
            })
            
            # Write to file with severity
            file.write("{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(
                env_name,
                stage_name,
                pipeline_id,
                # ... [existing fields],
                severity_info['severity'],
                severity_info['risk_priority']
            ))

    def load_SAS_data_to_redshift(self, program_name):
        """Modified to use test tables"""
        if (program_name == SAS_Config.program_name_apollo):
            try:
                # Use test tables
                os.system(COMMANDS.Redshift_Operations_Command.format(
                    ODIN.Redshift_ODIN,
                    f"truncate table {SAS_Config.test_redshift_apollo_table}"
                ))
                
                # ... [rest of the function using test tables]
                
            except Exception as error:
                Logger.printLogs("ERROR", f"Data loading failed: {error}")
                sys.exit(1)

    def aggregate_metrics(self, distinct_managers):
        """Modified to include severity in aggregation"""
        for manager in distinct_managers:
            try:
                # Aggregate by severity
                self.rs_upsert.execute_redshift_sql_query(
                    SAS_Config.severity_aggregation_query.format(
                        SAS_Config.test_sas_risk_summary,
                        manager,
                        current_date,
                        'WoW',
                        current_week,
                        manager,
                        current_year,
                        'severity_based',
                        SAS_Config.test_redshift_apollo_table
                    )
                )
            except Exception as error:
                Logger.printLogs("ERROR", f"Aggregation failed: {error}")
                sys.exit(1)
```

## 3. Implementation Steps

1. **Create Test Environment**
```sql
-- Add severity columns if not already added
ALTER TABLE aeetest.raw_sas_apollo_metrics 
ADD COLUMN IF NOT EXISTS severity VARCHAR(10),
ADD COLUMN IF NOT EXISTS risk_priority INT;

-- Repeat for brazil and pipeline tables
```

2. **Update Configuration**
```python
# Update SAS_Config.py with new configurations
# Add the code from step 1
```

3. **Modify Plugin Code**
```python
# Update SAS_Plugin.py with new methods
# Add the code from step 2
```

4. **Test Implementation**
```python
def test_severity_classification():
    plugin = SAS_Plugin()
    client = plugin.SAS_Init()
    
    # Test with sample data
    test_data = {
        'manager': 'test_manager',
        'metric_snapshot': 'WoW'
    }
    
    plugin.create_metrics(client, test_data)
```

5. **Validation Queries**
```sql
-- Check severity distribution
SELECT severity, COUNT(*) 
FROM aeetest.raw_sas_apollo_metrics
GROUP BY severity;

-- Check risk priorities
SELECT severity, risk_priority, COUNT(*) 
FROM aeetest.raw_sas_apollo_metrics
GROUP BY severity, risk_priority;

-- Check aggregation
SELECT 
    severity,
    metric_type,
    SUM(risk_count) as total_risks
FROM aeetest.sas_risk_summary
GROUP BY severity, metric_type;
```

## 4. Deployment Steps

1. **Backup Current Data**
```sql
CREATE TABLE aeetest.raw_sas_apollo_metrics_backup AS 
SELECT * FROM aeetest.raw_sas_apollo_metrics;
```

2. **Deploy New Code**
```bash
# Deploy updated configuration
cp SAS_Config.py /path/to/deployment/

# Deploy updated plugin
cp SAS_Plugin.py /path/to/deployment/
```

3. **Monitor Initial Run**
```python
# Add monitoring logs
Logger.printLogs("INFO", "Severity classification statistics: {}".format(
    self.get_severity_stats()
))

def get_severity_stats(self):
    query = """
    SELECT severity, COUNT(*) 
    FROM aeetest.raw_sas_apollo_metrics 
    GROUP BY severity
    """
    return self.rs_upsert.run_redshift_select_query(query)
```

4. **Rollback Plan**
```sql
-- If needed, restore from backup
TRUNCATE TABLE aeetest.raw_sas_apollo_metrics;
INSERT INTO aeetest.raw_sas_apollo_metrics 
SELECT * FROM aeetest.raw_sas_apollo_metrics_backup;
```

This implementation:
- Uses RAGS API for severity classification
- Maintains existing functionality
- Adds new severity-based metrics
- Includes testing and validation
- Provides rollback capability

The changes can be tested in the test environment before moving to production.

------

venry: brazil commands to create a new workspace and pull packages like use

------

Cedric: Here's how to set up a Brazil workspace for the SAS Plugin development:

```bash
# 1. Create a new Brazil workspace
brazil workspace create sas-plugin-dev

# 2. Change to workspace directory
cd sas-plugin-dev

# 3. Create a new package configuration
brazil ws use --package AEE-OE-Data-Collector-Aggregator

# 4. Add required dependencies in the Config file
brazil ws use --package SASRiskAggregationServicePythonClient
brazil ws use --package CoralPythonClient
brazil ws use --package PyOdinHttp
brazil ws use --package RetryingPythonClient

# 5. Additional commonly used packages
brazil ws use --package Logger
brazil ws use --package AWSCommonRuntimePython
brazil ws use --package BrazilConfig
brazil ws use --package AmazonDateUtilPython

# 6. Sync the workspace
brazil ws sync

# 7. Build the workspace
brazil-build

# 8. Check workspace setup
brazil workspace diagnostics

# 9. List all packages in use
brazil workspace show

# 10. Create a new feature branch
brazil workspace use -p AEE-OE-Data-Collector-Aggregator --branch feature-sas-severity

# Common Additional Commands:
# Update specific package
brazil ws use --package SASRiskAggregationServicePythonClient --version release-1.0

# Clean workspace
brazil-build clean

# Rebuild specific package
brazil-build --package AEE-OE-Data-Collector-Aggregator

# Check package versions
brazil ws show-versions

# Update all packages to latest version
brazil ws sync --md

# Get package information
brazil ws info AEE-OE-Data-Collector-Aggregator
```

Dependencies typically needed in `Config` file:
```python
dependencies = {
    1.0 = {
        SASRiskAggregationServicePythonClient = 1.0;
        CoralPythonClient = 1.0;
        PyOdinHttp = 1.0;
        RetryingPythonClient = 1.0;
        Logger = 1.0;
        AWSCommonRuntimePython = 1.0;
        BrazilConfig = 1.0;
        AmazonDateUtilPython = 1.0;
    };
};
```

To get development environment setup:
```bash
# Set up Apollo environment
apollo-env

# Install Python development tools
brazil-build-tool-installer python

# Set up virtual environment
brazil-python-3.9 -m venv venv
source venv/bin/activate

# Install development requirements
pip install -r requirements-dev.txt

# Set up pre-commit hooks
pre-commit install
```

Common development workflow:
```bash
# 1. Create new feature branch
git checkout -b feature-sas-severity

# 2. Make changes to code

# 3. Build and test
brazil-build
brazil-build test

# 4. Run style checks
brazil-build style

# 5. Run unit tests
brazil-build unit-test

# 6. Commit changes
git add .
git commit -m "Added severity classification to SAS Plugin"

# 7. Push changes
git push origin feature-sas-severity

# 8. Create code review
brazil cr create
```

Debugging setup:
```bash
# Enable debug logging
export BRAZIL_DEBUG=1

# Run with debug output
brazil-build --debug

# Check build logs
cat /tmp/brazil-build.log

# Run specific tests with debug
brazil-build test -t test_severity_classification --debug
```

Useful environment variables:
```bash
# Set environment
export BRAZIL_WORKSPACE_DEFAULT=/workplace/sas-plugin-dev

# Set Python path
export PYTHONPATH=/workplace/sas-plugin-dev/build/lib

# Set test environment
export TEST_ENV=development
```

These commands should help you set up and manage your Brazil workspace for developing the SAS Plugin enhancements. Let me know if you need any clarification or additional commands!

------

venry: create a new workspace for this SAS work in (25-02-19 9:44:54) <0> [/workplace/venry/PipelineMailer/src/AEE-OE-Data-Collector-Aggregator-Config]  

in venry

------

Cedric: Here's how to create a new workspace for SAS work in your venry directory:

```bash
# 1. Navigate to venry directory
cd /workplace/venry

# 2. Create a new workspace for SAS work
brazil workspace create sas-severity-enhancement

# 3. Change to new workspace directory
cd sas-severity-enhancement

# 4. Add main package and dependencies
brazil ws use --package AEE-OE-Data-Collector-Aggregator
brazil ws use --package AEE-OE-Data-Collector-Aggregator-Config
brazil ws use --package SASRiskAggregationServicePythonClient
brazil ws use --package CoralPythonClient
brazil ws use --package PyOdinHttp
brazil ws use --package RetryingPythonClient

# 5. Sync workspace
brazil ws sync

# 6. Create a new feature branch
brazil workspace use -p AEE-OE-Data-Collector-Aggregator --branch feature-sas-severity-classification

# 7. Build the workspace
brazil-build

# 8. Verify setup
brazil workspace diagnostics

# 9. Check current packages
brazil workspace show
```

Directory structure should look like:
```
/workplace/venry/
 sas-severity-enhancement/
     src/
        AEE-OE-Data-Collector-Aggregator/
        AEE-OE-Data-Collector-Aggregator-Config/
     build/
     env/
```

To copy existing configuration files:
```bash
# Create directories if they don't exist
mkdir -p src/AEE-OE-Data-Collector-Aggregator-Config/config

# Copy existing config files from your PipelineMailer workspace
cp /workplace/venry/PipelineMailer/src/AEE-OE-Data-Collector-Aggregator-Config/config/sas_config.py src/AEE-OE-Data-Collector-Aggregator-Config/config/

# Copy other necessary files
cp /workplace/venry/PipelineMailer/src/AEE-OE-Data-Collector-Aggregator-Config/config/basic_config.py src/AEE-OE-Data-Collector-Aggregator-Config/config/
```

Set up development environment:
```bash
# Set up Apollo environment
apollo-env

# Create and activate virtual environment
brazil-python-3.9 -m venv venv
source venv/bin/activate

# Install development requirements
pip install -r requirements-dev.txt
```

Verify workspace setup:
```bash
# Check package versions
brazil ws show-versions

# Verify build
brazil-build clean
brazil-build
```

Start development:
```bash
# Navigate to source directory
cd src/AEE-OE-Data-Collector-Aggregator-Config/config

# Create backup of original config
cp sas_config.py sas_config.py.backup

# Edit sas_config.py to add new severity configurations
vim sas_config.py
```

For testing:
```bash
# Set up test environment variables
export TEST_ENV=development
export BRAZIL_DEBUG=1

# Run tests
brazil-build test
```

To commit changes:
```bash
# Check status
git status

# Add changes
git add src/AEE-OE-Data-Collector-Aggregator-Config/config/sas_config.py

# Commit
git commit -m "Added severity classification configurations for SAS Plugin"

# Push changes
git push origin feature-sas-severity-classification
```

Additional useful commands:
```bash
# To check logs
tail -f /apollo/env/AEE-OE-Data-Collector-Aggregator/var/output/logs/application.log

# To run specific tests
brazil-build test -t test_sas_severity

# To check code style
brazil-build style

# To create code review
brazil cr create
```

Remember to:
1. Keep original backups of files you modify
2. Test changes in development environment first
3. Update documentation as you make changes
4. Create meaningful commit messages
5. Run tests before pushing changes

Let me know if you need help with any specific part of the setup!

------

venry: I've imported the file and config, what changes should be made, step by step 

* Initialize the SAS client using the RAGS API.
* Use the GetResourceHighLevelRiskDetailsByCampaignType API to fetch high-level risk details.
* Process the data to extract severity information.
* Store the processed data in AWS Redshift.
* Update the Page-0 Dashboard to reflect the new severity-based risk categorization.
* Add a new column for severity classification in the Redshift tables.

can this be used?

def init(self, client):
self.client = client

def get_severity_based_risks(self, aggregate_id):
# Use GetResourceHighLevelRiskDetailsByCampaignType API
high_level_data = self.client.get_resource_high_level_risk_details_by_campaign_type(
aggregate_id=aggregate_id,
campaign_type="BLOCKED"
)
# Process built-in severity mapping
severity_counts = {
"EMERGENCY": high_level_data.riskSeverityMapWithRecommendations.get("Critical", 0),
"CRITICAL": high_level_data.riskSeverityMapWithRecommendations.get("High", 0),
"HIGH": high_level_data.riskSeverityMapWithRecommendations.get("Medium", 0)
}
return severity_counts

------

Cedric: Yes, we can adapt that code. Here's the step-by-step implementation:

1. First, update `SAS_Config.py`:
```python
class SAS_Config:
    # Add new severity constants
    SEVERITY_LEVELS = {
        "EMERGENCY": "Critical",
        "CRITICAL": "High",
        "HIGH": "Medium"
    }
    
    RISK_PRIORITY = {
        "EMERGENCY": 3,
        "CRITICAL": 2,
        "HIGH": 1
    }

    # Add test table names
    test_apollo_table = "aeetest.raw_sas_apollo_metrics"
    test_brazil_table = "aeetest.raw_sas_brazil_metrics"
    test_pipeline_table = "aeetest.raw_sas_pipeline_target_metrics"
    test_risk_summary = "aeetest.sas_risk_summary"
```

2. Modify `SAS_Plugin.py`:
```python
class SAS_Plugin:
    def __init__(self):
        Logger.printLogs("INFO", "Triggering SAS Plugin")
        self.rs_upsert = redshift_operation_wrapper()
        self.severity_mapping = SAS_Config.SEVERITY_LEVELS
        self.risk_priority = SAS_Config.RISK_PRIORITY

    def get_severity_based_risks(self, high_level_data):
        """
        Process severity information from RAGS API response
        """
        try:
            severity_info = {
                "EMERGENCY": high_level_data.riskSeverityMapWithRecommendations.get("Critical", 0),
                "CRITICAL": high_level_data.riskSeverityMapWithRecommendations.get("High", 0),
                "HIGH": high_level_data.riskSeverityMapWithRecommendations.get("Medium", 0)
            }
            Logger.printLogs("DEBUG", f"Processed severity info: {severity_info}")
            return severity_info
        except Exception as e:
            Logger.printLogs("ERROR", f"Error processing severity: {e}")
            return {"HIGH": 0}  # Default fallback

    def call_get_high_level_resource_details(self, manager, _client):
        """Modified to include severity information"""
        try:
            data = _client.get_resource_high_level_risk_details_by_campaign_type(
                aggregate_id = "Person:" + manager,
                campaign_type='BLOCKED'
            )
            
            # Add severity information
            severity_data = self.get_severity_based_risks(data)
            data.update({'severity_info': severity_data})
            
            return data

        except ThrottlingException as e:
            Logger.printLogs("EXCEPTION", f"ThrottlingException: {e}")
            time.sleep(5)
        except Exception as err:
            Logger.printLogs("ERROR", f"Error occurred: {err}")
            return 1

    def get_apollo_data_and_write_in_file(self, data, file, metric_snapshot):
        """Modified to include severity information"""
        list_data = self.coral_to_python(data)
        for filtered_depth_data in list_data['Details']:
            # Existing data extraction
            env_name = filtered_depth_data['Name']
            stage_name = env_name.split(':')[1]
            # ... [other existing extractions]

            # Get severity information
            severity = self.determine_severity(filtered_depth_data)
            risk_priority = self.risk_priority.get(severity, 1)

            # Write to file including severity
            file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(
                env_name,
                stage_name,
                pipeline_id,
                pipeline_name,
                owner,
                owner_login,
                consumer_count,
                children_count,
                risk_count,
                visibility,
                host_count,
                oldest_risk_age,
                oldest_recommendation_age,
                action_required,
                blocked_resources,
                associated_campaigns,
                current_date,
                current_year,
                classification,
                metric_snapshot,
                severity,           # New field
                risk_priority      # New field
            ))

    def determine_severity(self, risk_data):
        """Determine severity based on risk data"""
        try:
            if int(risk_data.get('days_non_compliant', 0)) > 30:
                return "EMERGENCY"
            elif int(risk_data.get('days_non_compliant', 0)) > 20:
                return "CRITICAL"
            return "HIGH"
        except:
            return "HIGH"

    def load_SAS_data_to_redshift(self, program_name):
        """Modified to use test tables"""
        if (program_name == SAS_Config.program_name_apollo):
            try:
                # Use test table
                os.system(COMMANDS.Redshift_Operations_Command.format(
                    ODIN.Redshift_ODIN,
                    f"truncate table {SAS_Config.test_apollo_table}"
                ))
                
                os.system(COMMANDS.Redshift_Update_Comamnd.format(
                    ODIN.Redshift_ODIN,
                    "csv",
                    SAS_Config.test_apollo_table,
                    Common_Constants.S3_Bucket+'/'+SAS_Config.sas_apollo_temp_s3_location
                ))
                
                Logger.printLogs("INFO", f"Data updated in {SAS_Config.test_apollo_table}")

            except Exception as error:
                Logger.printLogs("ERROR", f"Data loading failed: {error}")
                sys.exit(1)

    def aggregate_metrics(self, distinct_managers):
        """Add severity-based aggregation"""
        try:
            for manager in distinct_managers:
                # Aggregate by severity
                query = """
                INSERT INTO {} (
                    risk_count, 
                    metric_rolled_up_to,
                    metric_run_date,
                    metric_snapshot,
                    metric_snapshot_number,
                    aggregate_roll_up_to,
                    metric_year,
                    metric_type,
                    severity
                )
                SELECT 
                    COUNT(*) as risk_count,
                    %s as metric_rolled_up_to,
                    CURRENT_DATE as metric_run_date,
                    %s as metric_snapshot,
                    %s as metric_snapshot_number,
                    %s as aggregate_roll_up_to,
                    EXTRACT(year from CURRENT_DATE) as metric_year,
                    severity as metric_type,
                    severity
                FROM {}
                WHERE metric_run_date = CURRENT_DATE
                GROUP BY severity
                """
                
                self.rs_upsert.execute_redshift_sql_query(
                    query.format(
                        SAS_Config.test_risk_summary,
                        SAS_Config.test_apollo_table
                    ),
                    (manager, 'WoW', self.get_week_number(), manager)
                )
                
        except Exception as error:
            Logger.printLogs("ERROR", f"Aggregation failed: {error}")
            sys.exit(1)
```

3. Test the changes:
```python
def test_severity_classification():
    """Test function for severity classification"""
    plugin = SAS_Plugin()
    client = plugin.SAS_Init()
    
    # Test with sample manager
    test_manager = "test_manager"
    data = plugin.call_get_high_level_resource_details(test_manager, client)
    
    # Verify severity information
    assert 'severity_info' in data
    assert any(severity in data['severity_info'] for severity in ["EMERGENCY", "CRITICAL", "HIGH"])
```

4. Verify in Redshift:
```sql
-- Check severity distribution
SELECT severity, COUNT(*) 
FROM aeetest.raw_sas_apollo_metrics
GROUP BY severity;

-- Check risk priorities
SELECT severity, risk_priority, COUNT(*) 
FROM aeetest.raw_sas_apollo_metrics
GROUP BY severity, risk_priority;

-- Check aggregated metrics
SELECT 
    severity,
    SUM(risk_count) as total_risks,
    metric_snapshot
FROM aeetest.sas_risk_summary
GROUP BY severity, metric_snapshot;
```

5. Run the plugin:
```python
if __name__ == "__main__":
    plugin = SAS_Plugin()
    _client = plugin.SAS_Init()
    plugin.create_metrics(_client, 'WoW')
```

This implementation:
1. Uses RAGS API for severity data
2. Processes and maps severity levels
3. Stores severity in test tables
4. Aggregates by severity
5. Includes logging and error handling

Make sure to:
- Test in development first
- Verify data accuracy
- Monitor performance
- Have rollback plan ready

Let me know if you need clarification on any part!

------

venry: explain the workng of it

------

Cedric: Let's break down the working of the SAS Plugin with severity classification:

## 1. Flow Diagram
```mermaid
graph TD
    A[Initialize SAS Plugin] --> B[Get High Level Risk Details]
    B --> C[Process Severity Information]
    C --> D[Write to CSV Files]
    D --> E[Load to RedShift]
    E --> F[Aggregate Metrics]
    F --> G[Update Dashboard]
```

## 2. Detailed Working

### A. Initialization and Config
```python
class SAS_Config:
    # Severity mapping definitions
    SEVERITY_LEVELS = {
        "EMERGENCY": "Critical",  # Highest severity
        "CRITICAL": "High",      # Medium severity
        "HIGH": "Medium"         # Lower severity
    }
    
    RISK_PRIORITY = {
        "EMERGENCY": 3,  # Highest priority
        "CRITICAL": 2,   # Medium priority
        "HIGH": 1        # Lower priority
    }
```

### B. Getting Risk Details
```python
def call_get_high_level_resource_details(self, manager, _client):
    """
    1. Makes API call to RAGS for each manager
    2. Gets risk details including severity information
    3. Adds processed severity data to response
    """
    data = _client.get_resource_high_level_risk_details_by_campaign_type(
        aggregate_id = "Person:" + manager,
        campaign_type='BLOCKED'
    )
    
    # Process severity information
    severity_data = self.get_severity_based_risks(data)
    data.update({'severity_info': severity_data})
    
    return data
```

### C. Severity Processing
```python
def get_severity_based_risks(self, high_level_data):
    """
    1. Extracts severity information from RAGS response
    2. Maps to our severity levels
    3. Returns count of risks by severity
    """
    try:
        return {
            "EMERGENCY": high_level_data.riskSeverityMapWithRecommendations.get("Critical", 0),
            "CRITICAL": high_level_data.riskSeverityMapWithRecommendations.get("High", 0),
            "HIGH": high_level_data.riskSeverityMapWithRecommendations.get("Medium", 0)
        }
    except Exception as e:
        Logger.printLogs("ERROR", f"Error processing severity: {e}")
        return {"HIGH": 0}  # Default fallback
```

### D. Data Writing
```python
def get_apollo_data_and_write_in_file(self, data, file, metric_snapshot):
    """
    1. Processes each risk detail
    2. Determines severity for each risk
    3. Writes to CSV with severity information
    """
    list_data = self.coral_to_python(data)
    for risk in list_data['Details']:
        # Get severity and priority
        severity = self.determine_severity(risk)
        risk_priority = self.risk_priority.get(severity, 1)
        
        # Write to CSV
        file.write("{},{},{},{},...,{},{}\n".format(
            # Existing fields...
            severity,           # New severity field
            risk_priority      # New priority field
        ))
```

### E. RedShift Loading
```python
def load_SAS_data_to_redshift(self, program_name):
    """
    1. Truncates test tables
    2. Loads new data from CSV
    3. Updates with severity information
    """
    if (program_name == SAS_Config.program_name_apollo):
        # Truncate test table
        os.system(COMMANDS.Redshift_Operations_Command.format(
            ODIN.Redshift_ODIN,
            f"truncate table {SAS_Config.test_apollo_table}"
        ))
        
        # Load new data
        os.system(COMMANDS.Redshift_Update_Comamnd.format(
            ODIN.Redshift_ODIN,
            "csv",
            SAS_Config.test_apollo_table,
            Common_Constants.S3_Bucket+'/'+SAS_Config.sas_apollo_temp_s3_location
        ))
```

### F. Metric Aggregation
```python
def aggregate_metrics(self, distinct_managers):
    """
    1. Aggregates risks by severity
    2. Calculates counts per severity level
    3. Updates summary table
    """
    for manager in distinct_managers:
        query = """
        INSERT INTO {} (
            risk_count, 
            metric_rolled_up_to,
            severity,
            -- other fields...
        )
        SELECT 
            COUNT(*) as risk_count,
            %s as metric_rolled_up_to,
            severity,
            -- other fields...
        FROM {}
        WHERE metric_run_date = CURRENT_DATE
        GROUP BY severity
        """
        
        self.rs_upsert.execute_redshift_sql_query(query.format(
            SAS_Config.test_risk_summary,
            SAS_Config.test_apollo_table
        ))
```

## 3. Data Flow Example

```mermaid
sequenceDiagram
    participant Manager
    participant RAGS API
    participant SAS Plugin
    participant CSV Files
    participant RedShift
    participant Dashboard

    Manager->>SAS Plugin: Request Risk Data
    SAS Plugin->>RAGS API: Get High Level Details
    RAGS API->>SAS Plugin: Return Risk Data
    SAS Plugin->>SAS Plugin: Process Severity
    SAS Plugin->>CSV Files: Write Data
    CSV Files->>RedShift: Load Data
    RedShift->>Dashboard: Update Metrics
```

## 4. Sample Data Flow

```python
# 1. Input Data (from RAGS)
{
    "riskSeverityMapWithRecommendations": {
        "Critical": 5,
        "High": 10,
        "Medium": 15
    }
}

# 2. Processed Severity Data
{
    "EMERGENCY": 5,  # Mapped from Critical
    "CRITICAL": 10,  # Mapped from High
    "HIGH": 15       # Mapped from Medium
}

# 3. RedShift Data
# Table: aeetest.raw_sas_apollo_metrics
# apollo_environment_name | severity  | risk_priority | risk_count
# ---------------------- | --------- | ------------- | ----------
# Environment1           | EMERGENCY | 3             | 5
# Environment2           | CRITICAL  | 2             | 10
# Environment3           | HIGH      | 1             | 15
```

## 5. Error Handling and Logging

```python
try:
    # Process severity
    severity_data = self.get_severity_based_risks(data)
    Logger.printLogs("DEBUG", f"Processed severity: {severity_data}")
except Exception as e:
    Logger.printLogs("ERROR", f"Failed to process severity: {e}")
    # Use default severity
    severity_data = {"HIGH": 0}
```

This implementation ensures:
1. Proper severity classification from RAGS
2. Consistent data flow
3. Error handling and logging
4. Data validation
5. Metric aggregation
6. Dashboard updates

The data flows from RAGS API through the plugin, gets processed with severity information, stored in RedShift, and finally appears in the dashboard with the new severity classifications.

------

venry: show the difference between current workflow and after this change

------

Cedric: # Current vs New Workflow Comparison

## Visual Comparison
```mermaid
graph TB
    subgraph "Current Workflow"
    A1[Get Manager Data] --> B1[Get Blocked Risks]
    B1 --> C1[Process Risk Data]
    C1 --> D1[Store in RedShift]
    D1 --> E1[Page-0 Dashboard<br>Shows Blocked/Non-Blocked]
    end

    subgraph "New Workflow with Severity"
    A2[Get Manager Data] --> B2[Get Blocked Risks + Severity]
    B2 --> C2[Process Risk & Severity Data]
    C2 --> D2[Store in RedShift with Severity]
    D2 --> E2[Page-0 Dashboard<br>Shows Emergency/Critical/High]
    end
```

## Code Comparison

### 1. Risk Data Collection

**Current:**
```python
def call_get_high_level_resource_details(self, manager, _client):
    try:
        # Only gets blocked campaign data
        data = _client.get_resource_high_level_risk_details_by_campaign_type(
            aggregate_id = "Person:" + manager,
            campaign_type='BLOCKED'
        )
        return data
    except Exception as err:
        Logger.printLogs("ERROR", f"Error occurred: {err}")
        return 1
```

**New:**
```python
def call_get_high_level_resource_details(self, manager, _client):
    try:
        # Gets blocked campaign data with severity
        data = _client.get_resource_high_level_risk_details_by_campaign_type(
            aggregate_id = "Person:" + manager,
            campaign_type='BLOCKED'
        )
        
        # Add severity information
        severity_data = self.get_severity_based_risks(data)
        data.update({'severity_info': severity_data})
        
        return data
    except Exception as err:
        Logger.printLogs("ERROR", f"Error occurred: {err}")
        return 1
```

### 2. Data Processing

**Current:**
```python
def get_apollo_data_and_write_in_file(self, data, file, metric_snapshot):
    list_data = self.coral_to_python(data)
    for filtered_depth_data in list_data['Details']:
        env_name = filtered_depth_data['Name']
        # ... other fields
        
        file.write("{},{},{},{},{},{},{},{},{},{},{}\n".format(
            env_name,
            stage_name,
            pipeline_id,
            pipeline_name,
            owner,
            owner_login,
            risk_count,
            days_non_compliant,
            metric_snapshot,
            'BLOCKED',  # Only blocked status
            snapshot_number
        ))
```

**New:**
```python
def get_apollo_data_and_write_in_file(self, data, file, metric_snapshot):
    list_data = self.coral_to_python(data)
    for filtered_depth_data in list_data['Details']:
        env_name = filtered_depth_data['Name']
        # ... other fields
        
        # New severity processing
        severity = self.determine_severity(filtered_depth_data)
        risk_priority = self.risk_priority.get(severity, 1)
        
        file.write("{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(
            env_name,
            stage_name,
            pipeline_id,
            pipeline_name,
            owner,
            owner_login,
            risk_count,
            days_non_compliant,
            metric_snapshot,
            severity,      # New field
            risk_priority, # New field
            snapshot_number
        ))
```

### 3. Database Schema

**Current:**
```sql
-- Current Apollo metrics table
CREATE TABLE rawdata_schema.raw_sas_apollo_metrics (
    apollo_environment_name VARCHAR,
    stage_name VARCHAR,
    pipeline_id VARCHAR,
    owner_login VARCHAR,
    risk_count INT,
    days_non_compliant INT,
    metric_snapshot VARCHAR,
    classification VARCHAR,  -- Just Blocked/Non-Blocked
    metric_run_date DATE
);
```

**New:**
```sql
-- New Apollo metrics table
CREATE TABLE aeetest.raw_sas_apollo_metrics (
    apollo_environment_name VARCHAR,
    stage_name VARCHAR,
    pipeline_id VARCHAR,
    owner_login VARCHAR,
    risk_count INT,
    days_non_compliant INT,
    metric_snapshot VARCHAR,
    classification VARCHAR,
    severity VARCHAR(10),    -- New: EMERGENCY/CRITICAL/HIGH
    risk_priority INT,       -- New: 3/2/1
    metric_run_date DATE
);
```

### 4. Metric Aggregation

**Current:**
```python
def aggregate_metrics(self, distinct_managers):
    for manager in distinct_managers:
        query = """
        INSERT INTO aggregate_schema.sas_risk_summary
        SELECT 
            COUNT(*) as risk_count,
            metric_rolled_up_to,
            metric_run_date,
            'total' as metric_type
        FROM rawdata_schema.raw_sas_apollo_metrics
        WHERE classification = 'BLOCKED'
        GROUP BY metric_rolled_up_to, metric_run_date
        """
        self.rs_upsert.execute_redshift_sql_query(query)
```

**New:**
```python
def aggregate_metrics(self, distinct_managers):
    for manager in distinct_managers:
        query = """
        INSERT INTO aeetest.sas_risk_summary
        SELECT 
            COUNT(*) as risk_count,
            metric_rolled_up_to,
            metric_run_date,
            severity as metric_type,
            severity,
            risk_priority
        FROM aeetest.raw_sas_apollo_metrics
        GROUP BY 
            metric_rolled_up_to, 
            metric_run_date,
            severity,
            risk_priority
        """
        self.rs_upsert.execute_redshift_sql_query(query)
```

### 5. Data Flow Changes

**Current Flow:**
```
RAGS API (Blocked)  Process  Store  Aggregate (Blocked/Non-Blocked)  Dashboard
```

**New Flow:**
```
RAGS API (Severity)  Process Severity  Store with Severity  Aggregate by Severity  Enhanced Dashboard
```

### 6. Configuration Changes

**Current:**
```python
class SAS_Config:
    campaign_type = 'BLOCKED'
    redshift_apollo_table = "rawdata_schema.raw_sas_apollo_metrics"
```

**New:**
```python
class SAS_Config:
    campaign_type = 'BLOCKED'
    test_apollo_table = "aeetest.raw_sas_apollo_metrics"
    
    # New severity configurations
    SEVERITY_LEVELS = {
        "EMERGENCY": "Critical",
        "CRITICAL": "High",
        "HIGH": "Medium"
    }
    
    RISK_PRIORITY = {
        "EMERGENCY": 3,
        "CRITICAL": 2,
        "HIGH": 1
    }
```

### 7. Output Changes

**Current Dashboard:**
```
Manager: John Doe
- Blocked Risks: 25
- Non-Blocked Risks: 10
```

**New Dashboard:**
```
Manager: John Doe
- Emergency Risks: 8  (Priority 3)
- Critical Risks: 12  (Priority 2)
- High Risks: 15      (Priority 1)
```

Key Differences:
1. **Data Collection:** Now includes severity information
2. **Processing:** Added severity classification logic
3. **Storage:** New columns for severity and priority
4. **Aggregation:** Groups by severity levels
5. **Display:** More detailed risk categorization
6. **Configuration:** Added severity mappings
7. **Monitoring:** Enhanced risk tracking

This enhancement provides:
- More granular risk classification
- Better prioritization
- Enhanced reporting capabilities
- More actionable insights
- Improved risk management

------

venry: @retry(wait_fixed=10000, stop_max_attempt_number=3)
	def call_get_high_level_resource_details(self, manager, _client):
		'''
		Makes API call to get the high level SAS risk details for any particular manager. It includes all Blocked SAS risks.
		'''
		try:
			data = _client.get_resource_high_level_risk_details_by_campaign_type(aggregate_id = "Person:"+manager, campaign_type='BLOCKED')
			severity_data = self.get_severity_based_risks(data)
			data.update({'severity_info': severity_data})
			
		except ThrottlingException as e:
			Logger.printLogs("EXCEPTION","ThrottlingException occurred, Sleeping! {}".format(e))
			time.sleep(5)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred {}".format(err))
			return 1

		return data

is this okay?

------

Cedric: Yes, that's a good minimal change! Let's complete it with the companion method:

```python
@retry(wait_fixed=10000, stop_max_attempt_number=3)
def call_get_high_level_resource_details(self, manager, _client):
    '''
    Makes API call to get the high level SAS risk details for any particular manager. It includes all Blocked SAS risks.
    '''
    try:
        data = _client.get_resource_high_level_risk_details_by_campaign_type(
            aggregate_id = "Person:"+manager, 
            campaign_type='BLOCKED'
        )
        severity_data = self.get_severity_based_risks(data)
        data.update({'severity_info': severity_data})
        
    except ThrottlingException as e:
        Logger.printLogs("EXCEPTION","ThrottlingException occurred, Sleeping! {}".format(e))
        time.sleep(5)
    except Exception as err:
        Logger.printLogs("[ERROR]","Error occurred {}".format(err))
        return 1

    return data

def get_severity_based_risks(self, high_level_data):
    """
    Process severity information from RAGS API response
    """
    try:
        severity_counts = {
            "EMERGENCY": high_level_data.riskSeverityMapWithRecommendations.get("Critical", 0),
            "CRITICAL": high_level_data.riskSeverityMapWithRecommendations.get("High", 0),
            "HIGH": high_level_data.riskSeverityMapWithRecommendations.get("Medium", 0)
        }
        Logger.printLogs("DEBUG", f"Processed severity info: {severity_counts}")
        return severity_counts
    except Exception as e:
        Logger.printLogs("ERROR", f"Error processing severity: {e}")
        return {"HIGH": 0}  # Default fallback
```

This approach:
1. Makes minimal changes to existing code
2. Adds severity classification
3. Maintains error handling
4. Provides default fallback
5. Includes logging

Now you just need to update the file writing part to include the severity information when processing the data.

Would you like to see how to modify the file writing part as well?

------

venry: yes

------

Cedric: Here's how to modify the file writing part to include severity information:

```python
def get_apollo_data_and_write_in_file(self, data, file, metric_snapshot):
    '''
    Fetches all the details from processed Apollo data and writes into a csv file.
    '''
    list_data = self.coral_to_python(data)
    for filtered_depth_data in list_data['Details']:
        env_name = filtered_depth_data['Name']
        stage_name = env_name.split(':')[1]
        owner_login = filtered_depth_data['OwnerLogin']
        children_count = filtered_depth_data['ChildrenCount']
        consumer_count = filtered_depth_data['ConsumerCount']
        host_count = filtered_depth_data['HostCount']
        owner = filtered_depth_data['Owner']
        pipeline_id = filtered_depth_data['PipelineId']
        pipeline_name = filtered_depth_data['PipelineName']
        oldest_risk_age = filtered_depth_data['OldestRiskAge']
        risk_count = filtered_depth_data['RiskCount']
        visibility = filtered_depth_data['Visibility']

        # Get severity info from data
        severity_info = data.get('severity_info', {})
        severity = "HIGH"  # default severity
        risk_priority = 1  # default priority

        # Determine severity based on risk age
        if oldest_risk_age > 30:
            severity = "EMERGENCY"
            risk_priority = 3
        elif oldest_risk_age > 20:
            severity = "CRITICAL"
            risk_priority = 2

        current_date = date.today()
        snapshot_number = current_date.isocalendar()[1]
        classification = 'None'
        curent_year = current_date.strftime('%Y')

        file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(
            env_name,
            stage_name,
            pipeline_id,
            pipeline_name,
            owner,
            owner_login,
            consumer_count,
            children_count,
            risk_count,
            visibility,
            host_count,
            oldest_risk_age,
            oldest_recommendation_age,
            action_required,
            blocked_resources,
            associated_campaigns,
            current_date,
            curent_year,
            classification,
            metric_snapshot,
            severity,        # New field
            risk_priority    # New field
        ))
```

And similarly for Brazil data:

```python
def get_brazil_data_and_write_in_file(self, data, file, metric_snapshot):
    '''
    Fetches all the details from processed Brazil data and writes into a csv file.
    '''
    list_data = self.coral_to_python(data)
    for version_set_json_info in list_data['Details']:
        version_set = version_set_json_info['Name']
        owner_login = version_set_json_info['OwnerLogin']
        owner = version_set_json_info['Owner']
        pipeline_id = version_set_json_info['PipelineId']
        pipeline_name = version_set_json_info['PipelineName']
        oldest_recommendation_age = version_set_json_info['OldestRecommendationAge']
        oldest_risk_age = version_set_json_info['OldestRiskAge']
        risk_count = version_set_json_info['RiskCount']

        # Get severity info from data
        severity_info = data.get('severity_info', {})
        severity = "HIGH"  # default severity
        risk_priority = 1  # default priority

        # Determine severity based on risk age
        if oldest_risk_age > 30:
            severity = "EMERGENCY"
            risk_priority = 3
        elif oldest_risk_age > 20:
            severity = "CRITICAL"
            risk_priority = 2

        current_date = date.today()
        snapshot_number = current_date.isocalendar()[1]
        classification = 'None'
        curent_year = current_date.strftime('%Y')

        file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(
            version_set,
            pipeline_id,
            pipeline_name,
            owner,
            owner_login,
            risk_count,
            oldest_risk_age,
            oldest_recommendation_age,
            blocked_resources,
            action_required,
            associated_campaigns,
            current_date,
            curent_year,
            classification,
            metric_snapshot,
            severity,        # New field
            risk_priority    # New field
        ))
```

To make this work, you'll need to ensure your test tables have the new columns:

```sql
-- For Apollo metrics
ALTER TABLE aeetest.raw_sas_apollo_metrics 
ADD COLUMN severity VARCHAR(10),
ADD COLUMN risk_priority INT;

-- For Brazil metrics
ALTER TABLE aeetest.raw_sas_brazil_metrics 
ADD COLUMN severity VARCHAR(10),
ADD COLUMN risk_priority INT;

-- For Pipeline metrics
ALTER TABLE aeetest.raw_sas_pipeline_target_metrics 
ADD COLUMN severity VARCHAR(10),
ADD COLUMN risk_priority INT;
```

And update the configuration in `SAS_Config.py`:

```python
class SAS_Config:
    # Add severity related configurations
    SEVERITY_LEVELS = {
        "EMERGENCY": {"threshold": 30, "priority": 3},
        "CRITICAL": {"threshold": 20, "priority": 2},
        "HIGH": {"threshold": 0, "priority": 1}
    }

    # Test table names
    test_redshift_apollo_table = "aeetest.raw_sas_apollo_metrics"
    test_redshift_brazil_table = "aeetest.raw_sas_brazil_metrics"
    test_redshift_pipeline_target_table = "aeetest.raw_sas_pipeline_target_metrics"
```

The changes:
1. Add severity and risk_priority to CSV output
2. Use risk age to determine severity
3. Keep existing fields unchanged
4. Provide default values for new fields
5. Maintain existing functionality

This implementation minimally impacts the existing code while adding the required severity classification.

------

venry: what changes should be done for this 

def get_apollo_data_and_write_in_file(self, data, file, metric_snapshot):
		'''
		Fetches all the details from processed Apollo data and writes into a csv file.
		'''
		list_data = self.coral_to_python(data)
		for filtered_depth_data in list_data['Details']:
			env_name = filtered_depth_data['Name']
			stage_name = env_name.split(':')[1]
			owner_login = filtered_depth_data['OwnerLogin']
			children_count = filtered_depth_data['ChildrenCount']
			consumer_count = filtered_depth_data['ConsumerCount']
			host_count = filtered_depth_data['HostCount']
			owner = filtered_depth_data['Owner']
			pipeline_id = filtered_depth_data['PipelineId']
			pipeline_name = filtered_depth_data['PipelineName']
			oldest_risk_age = filtered_depth_data['OldestRiskAge']
			risk_count = filtered_depth_data['RiskCount']
			visibility = filtered_depth_data['Visibility']
			recommendation_with_action = {}
			for recommendation in filtered_depth_data['Recommendations']:
				for target_resource in recommendation['AssociatedTargetResources']:
					recommendation_with_action[target_resource] = recommendation['Template']

			for software_risk in filtered_depth_data['SoftwareRisks']:
				for risk_details in software_risk['SoftwareRiskDetails']:
					blocked_resources = risk_details['TargetResource']
					action_required = recommendation_with_action[blocked_resources]
					associated_campaigns = software_risk['AssociatedCampaign']['Name'].replace(",","")

					oldest_risk_age = risk_details['RiskAge']
					oldest_recommendation_age = str(risk_details['RecommendationAge'])

					if (not oldest_recommendation_age.isnumeric()):
						oldest_recommendation_age = '-1'

					current_date = date.today()
					snapshot_number = current_date.isocalendar()[1]
					classification = 'None'
					curent_year = current_date.strftime('%Y')
					file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(env_name,
																										 stage_name,
																										 pipeline_id,
																										 pipeline_name,
																										 owner,
																										 owner_login,
																										 consumer_count,
																										 children_count,
																										 risk_count,
																										 visibility,
																										 host_count,
																										 oldest_risk_age,
																										 oldest_recommendation_age,
																										 action_required,
																										 blocked_resources,
																										 associated_campaigns,
																										 current_date,
																										 curent_year,
																										 classification,
																										 metric_snapshot,
																										 snapshot_number))

------

Cedric: Here's the minimally modified version of the function to include severity information:

```python
def get_apollo_data_and_write_in_file(self, data, file, metric_snapshot):
    '''
    Fetches all the details from processed Apollo data and writes into a csv file.
    '''
    list_data = self.coral_to_python(data)
    for filtered_depth_data in list_data['Details']:
        env_name = filtered_depth_data['Name']
        stage_name = env_name.split(':')[1]
        owner_login = filtered_depth_data['OwnerLogin']
        children_count = filtered_depth_data['ChildrenCount']
        consumer_count = filtered_depth_data['ConsumerCount']
        host_count = filtered_depth_data['HostCount']
        owner = filtered_depth_data['Owner']
        pipeline_id = filtered_depth_data['PipelineId']
        pipeline_name = filtered_depth_data['PipelineName']
        oldest_risk_age = filtered_depth_data['OldestRiskAge']
        risk_count = filtered_depth_data['RiskCount']
        visibility = filtered_depth_data['Visibility']
        recommendation_with_action = {}
        
        for recommendation in filtered_depth_data['Recommendations']:
            for target_resource in recommendation['AssociatedTargetResources']:
                recommendation_with_action[target_resource] = recommendation['Template']

        for software_risk in filtered_depth_data['SoftwareRisks']:
            for risk_details in software_risk['SoftwareRiskDetails']:
                blocked_resources = risk_details['TargetResource']
                action_required = recommendation_with_action[blocked_resources]
                associated_campaigns = software_risk['AssociatedCampaign']['Name'].replace(",","")

                oldest_risk_age = risk_details['RiskAge']
                oldest_recommendation_age = str(risk_details['RecommendationAge'])

                if (not oldest_recommendation_age.isnumeric()):
                    oldest_recommendation_age = '-1'

                # Add severity classification
                severity = "HIGH"  # default
                risk_priority = 1  # default
                
                # Determine severity based on risk age
                if int(oldest_risk_age) > 30:
                    severity = "EMERGENCY"
                    risk_priority = 3
                elif int(oldest_risk_age) > 20:
                    severity = "CRITICAL"
                    risk_priority = 2

                current_date = date.today()
                snapshot_number = current_date.isocalendar()[1]
                classification = 'None'
                curent_year = current_date.strftime('%Y')
                
                # Added severity and risk_priority to the output
                file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(
                    env_name,
                    stage_name,
                    pipeline_id,
                    pipeline_name,
                    owner,
                    owner_login,
                    consumer_count,
                    children_count,
                    risk_count,
                    visibility,
                    host_count,
                    oldest_risk_age,
                    oldest_recommendation_age,
                    action_required,
                    blocked_resources,
                    associated_campaigns,
                    current_date,
                    curent_year,
                    classification,
                    metric_snapshot,
                    snapshot_number,
                    severity,        # New field
                    risk_priority    # New field
                ))
```

Key changes made:
1. Added severity determination logic based on risk age
2. Added two new fields to the CSV output:
   - severity (EMERGENCY/CRITICAL/HIGH)
   - risk_priority (3/2/1)
3. Kept all existing functionality intact
4. Used simple logic for severity classification

Make sure to:
1. Update the table schema to include these new columns
2. Update any code that reads this CSV file to handle the new columns
3. Update the redshift table load queries to include these new columns

Would you like to see the corresponding table schema changes as well?

------

venry: yes please

------

Cedric: Here are the necessary table schema changes:

1. **For Test Tables**
```sql
-- Add columns to apollo test table
ALTER TABLE aeetest.raw_sas_apollo_metrics 
ADD COLUMN severity VARCHAR(10) DEFAULT 'HIGH',
ADD COLUMN risk_priority INT DEFAULT 1;

-- Add columns to brazil test table
ALTER TABLE aeetest.raw_sas_brazil_metrics 
ADD COLUMN severity VARCHAR(10) DEFAULT 'HIGH',
ADD COLUMN risk_priority INT DEFAULT 1;

-- Add columns to pipeline test table
ALTER TABLE aeetest.raw_sas_pipeline_target_metrics 
ADD COLUMN severity VARCHAR(10) DEFAULT 'HIGH',
ADD COLUMN risk_priority INT DEFAULT 1;

-- Add columns to risk summary test table
ALTER TABLE aeetest.sas_risk_summary
ADD COLUMN severity VARCHAR(10) DEFAULT 'HIGH';
```

2. **Verify Schema**
```sql
-- Check apollo table structure
SELECT column_name, data_type, column_default 
FROM information_schema.columns 
WHERE table_schema = 'aeetest' 
AND table_name = 'raw_sas_apollo_metrics'
ORDER BY ordinal_position;

-- Check data
SELECT apollo_environment_name, 
       stage_name, 
       oldest_risk_age, 
       severity, 
       risk_priority 
FROM aeetest.raw_sas_apollo_metrics 
LIMIT 5;
```

3. **Update Load SAS Data Function**
```python
def load_SAS_data_to_redshift(self, program_name):
    '''
    Truncate the rawdata tables and update the new rawdata in tables. 
    '''
    if (program_name == SAS_Config.program_name_apollo):
        try:
            # Truncate temp table
            os.system(COMMANDS.Redshift_Operations_Command.format(
                ODIN.Redshift_ODIN,
                "truncate table {}".format(SAS_Config.test_apollo_table)
            ))
            
            # Load new data including severity columns
            os.system(COMMANDS.Redshift_Update_Comamnd.format(
                ODIN.Redshift_ODIN,
                "csv",
                SAS_Config.test_apollo_table,
                Common_Constants.S3_Bucket+'/'+SAS_Config.sas_apollo_temp_s3_location
            ))
            
            Logger.printLogs("INFO", f"Data updated in {SAS_Config.test_apollo_table}")

        except Exception as error:
            Logger.printLogs("ERROR", f"Apollo data loading failed: {error}")
            sys.exit(1)
```

4. **Test Queries**
```sql
-- Check severity distribution
SELECT severity, COUNT(*) as count
FROM aeetest.raw_sas_apollo_metrics
GROUP BY severity;

-- Check risk priority distribution
SELECT severity, risk_priority, COUNT(*) as count
FROM aeetest.raw_sas_apollo_metrics
GROUP BY severity, risk_priority
ORDER BY risk_priority DESC;

-- Check high-risk items
SELECT apollo_environment_name,
       oldest_risk_age,
       severity,
       risk_priority
FROM aeetest.raw_sas_apollo_metrics
WHERE severity = 'EMERGENCY'
LIMIT 5;
```

5. **Backup Queries (Optional but Recommended)**
```sql
-- Create backup before modifying
CREATE TABLE aeetest.raw_sas_apollo_metrics_backup AS 
SELECT * FROM aeetest.raw_sas_apollo_metrics;

-- Restore if needed
INSERT INTO aeetest.raw_sas_apollo_metrics 
SELECT * FROM aeetest.raw_sas_apollo_metrics_backup;
```

6. **Validation Queries**
```sql
-- Check for NULL values in new columns
SELECT COUNT(*) as null_severity_count
FROM aeetest.raw_sas_apollo_metrics
WHERE severity IS NULL;

-- Verify severity assignment
SELECT 
    CASE 
        WHEN oldest_risk_age > 30 THEN 'Should be EMERGENCY'
        WHEN oldest_risk_age > 20 THEN 'Should be CRITICAL'
        ELSE 'Should be HIGH'
    END as expected_severity,
    severity as actual_severity,
    COUNT(*) as count
FROM aeetest.raw_sas_apollo_metrics
GROUP BY 
    CASE 
        WHEN oldest_risk_age > 30 THEN 'Should be EMERGENCY'
        WHEN oldest_risk_age > 20 THEN 'Should be CRITICAL'
        ELSE 'Should be HIGH'
    END,
    severity;
```

7. **Monitor Query Performance**
```sql
-- Add these columns to existing indices or create new ones if needed
CREATE INDEX idx_severity 
ON aeetest.raw_sas_apollo_metrics(severity);

CREATE INDEX idx_risk_priority 
ON aeetest.raw_sas_apollo_metrics(risk_priority);
```

Execute these changes in this order:
1. Create backups
2. Add columns
3. Verify schema changes
4. Test data loading
5. Validate data
6. Check performance
7. Monitor results

Make sure to test these changes in the test environment before applying to production tables. Let me know if you need any clarification or additional queries!

------

venry: is there any change need to be done in 

def load_SAS_data_to_redshift(self, program_name):
		'''
		Truncate the rawdata tables and update the new rawdata in tables. 
		Input: program_name i.e. type of SAS riks
		'''
		if (program_name == SAS_Config.program_name_apollo):
			try:
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table {}".format(SAS_Config.temp_redshift_apollo_table)))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.temp_redshift_apollo_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table " + SAS_Config.redshift_apollo_table))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.redshift_apollo_table))
				os.system(COMMANDS.Redshift_Update_Comamnd.format(ODIN.Redshift_ODIN,"csv",SAS_Config.temp_redshift_apollo_table,Common_Constants.S3_Bucket+'/'+SAS_Config.sas_apollo_temp_s3_location))
				Logger.printLogs("INFO","Data updation to table '{}' completed successfully.".format(SAS_Config.temp_redshift_apollo_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,SAS_Config.query_for_getting_required_data.format(SAS_Config.redshift_apollo_table,SAS_Config.temp_redshift_apollo_table)))
				Logger.printLogs("INFO","Unique Data updation to table '{}' completed successfully.".format(SAS_Config.redshift_apollo_table))

			except Exception as error:
				Logger.printLogs("[ ERROR ]","Transferring Apollo data to redshift table failed with message : {}".format(error))
				sys.exit(1)

		elif (program_name == SAS_Config.program_name_brazil):
			try:
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table {}".format(SAS_Config.temp_redshift_brazil_table)))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.temp_redshift_brazil_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table " + SAS_Config.redshift_brazil_table))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.redshift_brazil_table))
				os.system(COMMANDS.Redshift_Update_Comamnd.format(ODIN.Redshift_ODIN,"csv",SAS_Config.temp_redshift_brazil_table,Common_Constants.S3_Bucket+'/'+SAS_Config.sas_brazil_temp_s3_location))
				Logger.printLogs("INFO","Data updation to table '{}' completed successfully.".format(SAS_Config.redshift_brazil_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,SAS_Config.query_for_getting_required_data.format(SAS_Config.redshift_brazil_table,SAS_Config.temp_redshift_brazil_table)))
				Logger.printLogs("INFO","Unique Data updation to table '{}' completed successfully.".format(SAS_Config.redshift_brazil_table))

			except Exception as error:
				Logger.printLogs("[ ERROR ]","Transferring Brazil data to redshift table failed with message : {}".format(error))
				sys.exit(1)

		elif (program_name == SAS_Config.program_name_pipeline_targets):
			try:
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table {}".format(SAS_Config.temp_redshift_pipeline_target_table)))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.temp_redshift_pipeline_target_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table " + SAS_Config.redshift_pipeline_target_table))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.redshift_pipeline_target_table))
				os.system(COMMANDS.Redshift_Update_Comamnd.format(ODIN.Redshift_ODIN,"csv",SAS_Config.temp_redshift_pipeline_target_table,Common_Constants.S3_Bucket+'/'+SAS_Config.sas_pipeline_target_temp_s3_location))
				Logger.printLogs("INFO","Data updation to table '{}' completed successfully.".format(SAS_Config.temp_redshift_pipeline_target_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,SAS_Config.query_for_getting_required_data.format(SAS_Config.redshift_pipeline_target_table,SAS_Config.temp_redshift_pipeline_target_table)))
				Logger.printLogs("INFO","Unique Data updation to table '{}' completed successfully.".format(SAS_Config.redshift_pipeline_target_table))

			except Exception as error:
				Logger.printLogs("ERROR","Transferring Pipeline Target data to redshift table failed with message : {}".format(error))
				sys.exit(1)
		else:
			Logger.printLogs("ERROR","Wrong Program name passed")
			return


I've made this changes in config 

severity_emergency = 'EMERGENCY'

	severity_critical = 'CRITICAL'
	
	severity_high = 'HIGH'

	test_redshift_apollo_table = "aeetest.raw_sas_apollo_metrics"
	test_redshift_brazil_table = "aeetest.raw_sas_brazil_metrics"
	test_redshift_pipeline_target_table = "aeetest.raw_sas_pipeline_target_metrics"
	test_sas_risk_summary = "aeetest.sas_risk_summary"

	severity_update_query = """update {} set severity = %s, risk_priority = %s where apollo_environment_name = %s and metric_run_date = %s"""
	severity_aggregate_update_query = """
	insert into {} (
		risk_count,
		metric_rolled_up_to,
		metric_run_date,
		metric_snapshot,
		metric_snapshot_number,
		aggregate_roll_up_to,
		metric_year,
		metric_type,
		severity
	)
	select
		count(*) as risk_count,
		%s as metric_roll_up_to,
		%s as metric_run_date,
		%s as metric_snapshot,
		%s as metric_snapshot_number,
		%s as aggregate_roll_up_to,
		%s as metric_year,
		%s as metric_type,
		severity
	from {} where metric run date = %s
		group by severity	
	"""

------

Cedric: Yes, for testing with the new severity fields, you should modify the `load_SAS_data_to_redshift` method to use the test tables. Here's how:

```python
def load_SAS_data_to_redshift(self, program_name):
    '''
    Truncate the rawdata tables and update the new rawdata in tables. 
    Input: program_name i.e. type of SAS risks
    '''
    if (program_name == SAS_Config.program_name_apollo):
        try:
            # Use test table instead of production table
            os.system(COMMANDS.Redshift_Operations_Command.format(
                ODIN.Redshift_ODIN,
                "truncate table {}".format(SAS_Config.test_redshift_apollo_table)
            ))
            Logger.printLogs("INFO", "Truncate test table '{}' completed successfully.".format(
                SAS_Config.test_redshift_apollo_table
            ))

            # Load data to test table
            os.system(COMMANDS.Redshift_Update_Comamnd.format(
                ODIN.Redshift_ODIN,
                "csv",
                SAS_Config.test_redshift_apollo_table,
                Common_Constants.S3_Bucket+'/'+SAS_Config.sas_apollo_temp_s3_location
            ))
            Logger.printLogs("INFO", "Data updation to test table '{}' completed successfully.".format(
                SAS_Config.test_redshift_apollo_table
            ))

        except Exception as error:
            Logger.printLogs("[ ERROR ]", "Transferring Apollo data to test table failed with message : {}".format(error))
            sys.exit(1)

    elif (program_name == SAS_Config.program_name_brazil):
        try:
            # Use test table for Brazil
            os.system(COMMANDS.Redshift_Operations_Command.format(
                ODIN.Redshift_ODIN,
                "truncate table {}".format(SAS_Config.test_redshift_brazil_table)
            ))
            Logger.printLogs("INFO", "Truncate test table '{}' completed successfully.".format(
                SAS_Config.test_redshift_brazil_table
            ))

            # Load data to test table
            os.system(COMMANDS.Redshift_Update_Comamnd.format(
                ODIN.Redshift_ODIN,
                "csv",
                SAS_Config.test_redshift_brazil_table,
                Common_Constants.S3_Bucket+'/'+SAS_Config.sas_brazil_temp_s3_location
            ))
            Logger.printLogs("INFO", "Data updation to test table '{}' completed successfully.".format(
                SAS_Config.test_redshift_brazil_table
            ))

        except Exception as error:
            Logger.printLogs("[ ERROR ]", "Transferring Brazil data to test table failed with message : {}".format(error))
            sys.exit(1)

    elif (program_name == SAS_Config.program_name_pipeline_targets):
        try:
            # Use test table for Pipeline targets
            os.system(COMMANDS.Redshift_Operations_Command.format(
                ODIN.Redshift_ODIN,
                "truncate table {}".format(SAS_Config.test_redshift_pipeline_target_table)
            ))
            Logger.printLogs("INFO", "Truncate test table '{}' completed successfully.".format(
                SAS_Config.test_redshift_pipeline_target_table
            ))

            # Load data to test table
            os.system(COMMANDS.Redshift_Update_Comamnd.format(
                ODIN.Redshift_ODIN,
                "csv",
                SAS_Config.test_redshift_pipeline_target_table,
                Common_Constants.S3_Bucket+'/'+SAS_Config.sas_pipeline_target_temp_s3_location
            ))
            Logger.printLogs("INFO", "Data updation to test table '{}' completed successfully.".format(
                SAS_Config.test_redshift_pipeline_target_table
            ))

        except Exception as error:
            Logger.printLogs("ERROR", "Transferring Pipeline Target data to test table failed with message : {}".format(error))
            sys.exit(1)
    else:
        Logger.printLogs("ERROR", "Wrong Program name passed")
        return

    # After loading data, update severity if needed
    try:
        self.update_severity_information(program_name)
    except Exception as error:
        Logger.printLogs("ERROR", "Failed to update severity information: {}".format(error))
```

And add a helper method for updating severity if needed:

```python
def update_severity_information(self, program_name):
    """
    Updates severity information in test tables if needed
    """
    try:
        if program_name == SAS_Config.program_name_apollo:
            table_name = SAS_Config.test_redshift_apollo_table
        elif program_name == SAS_Config.program_name_brazil:
            table_name = SAS_Config.test_redshift_brazil_table
        elif program_name == SAS_Config.program_name_pipeline_targets:
            table_name = SAS_Config.test_redshift_pipeline_target_table
        else:
            return

        # Update severity if not already set
        update_query = """
        UPDATE {}
        SET severity = CASE 
            WHEN days_non_compliant > 30 THEN %s
            WHEN days_non_compliant > 20 THEN %s
            ELSE %s
        END,
        risk_priority = CASE 
            WHEN days_non_compliant > 30 THEN 3
            WHEN days_non_compliant > 20 THEN 2
            ELSE 1
        END
        WHERE severity IS NULL
        """.format(table_name)

        self.rs_upsert.execute_redshift_sql_query(
            update_query, 
            (SAS_Config.severity_emergency, 
             SAS_Config.severity_critical, 
             SAS_Config.severity_high)
        )
        
        Logger.printLogs("INFO", "Updated severity information in {}".format(table_name))

    except Exception as error:
        Logger.printLogs("ERROR", "Failed to update severity: {}".format(error))
        raise
```

Key changes:
1. Uses test tables instead of production tables
2. Removes redundant truncate operations
3. Adds severity update functionality
4. Maintains existing logging
5. Uses configuration constants for severity levels

This modification allows you to test the severity changes in the test tables without affecting production data.

------

venry: do we need this update_severity_information fucntion?

------

Cedric: No, we don't need the `update_severity_information` function because we're already handling the severity classification earlier in the process:

1. First in `call_get_high_level_resource_details`:
```python
data = _client.get_resource_high_level_risk_details_by_campaign_type(
    aggregate_id = "Person:"+manager, 
    campaign_type='BLOCKED'
)
severity_data = self.get_severity_based_risks(data)
data.update({'severity_info': severity_data})
```

2. Then in the file writing functions where we write the severity data to CSV:
```python
# In get_apollo_data_and_write_in_file and other similar functions
file.write("{},{},{},... {},{}".format(
    # existing fields...,
    severity,
    risk_priority
))
```

So when the data is loaded to RedShift using `load_SAS_data_to_redshift`, the severity information is already included in the CSV files. We just need to modify `load_SAS_data_to_redshift` to use the test tables:

```python
def load_SAS_data_to_redshift(self, program_name):
    '''
    Truncate the rawdata tables and update the new rawdata in tables. 
    Input: program_name i.e. type of SAS riks
    '''
    if (program_name == SAS_Config.program_name_apollo):
        try:
            os.system(COMMANDS.Redshift_Operations_Command.format(
                ODIN.Redshift_ODIN,
                "truncate table {}".format(SAS_Config.test_redshift_apollo_table)
            ))
            Logger.printLogs("INFO", "Truncate table '{}' completed successfully.".format(
                SAS_Config.test_redshift_apollo_table
            ))
            
            os.system(COMMANDS.Redshift_Update_Comamnd.format(
                ODIN.Redshift_ODIN,
                "csv",
                SAS_Config.test_redshift_apollo_table,
                Common_Constants.S3_Bucket+'/'+SAS_Config.sas_apollo_temp_s3_location
            ))
            Logger.printLogs("INFO", "Data updation to table '{}' completed successfully.".format(
                SAS_Config.test_redshift_apollo_table
            ))

        except Exception as error:
            Logger.printLogs("[ ERROR ]", "Transferring Apollo data to redshift table failed with message : {}".format(error))
            sys.exit(1)

    # Similar changes for brazil and pipeline_targets sections...
```

The flow is:
1. Get data with severity from RAGS API
2. Write to CSV with severity included
3. Load CSV to test tables
4. No additional severity update needed

This keeps the changes minimal and maintains the data flow integrity.

------

venry: how would the final sas aggragte table data be?

------

Cedric: The SAS aggregate table (aeetest.sas_risk_summary) would look like this:

```sql
-- Current sas_risk_summary data example
risk_count | metric_rolled_up_to | metric_run_date | metric_snapshot | metric_snapshot_number | aggregate_roll_up_to | metric_year | metric_type
10         | dmmitta            | 2025-02-19      | WoW            | 8                      | abbaamer            | 2025        | total
8          | smanjeer           | 2025-02-19      | WoW            | 8                      | abbaamer            | 2025        | recommendations

-- New sas_risk_summary data with severity
risk_count | metric_rolled_up_to | metric_run_date | metric_snapshot | metric_snapshot_number | aggregate_roll_up_to | metric_year | metric_type | severity
5          | dmmitta            | 2025-02-19      | WoW            | 8                      | abbaamer            | 2025        | total       | EMERGENCY
3          | dmmitta            | 2025-02-19      | WoW            | 8                      | abbaamer            | 2025        | total       | CRITICAL
2          | dmmitta            | 2025-02-19      | WoW            | 8                      | abbaamer            | 2025        | total       | HIGH
```

To achieve this, we need to modify the aggregation query in `aggregate_metrics`:

```python
def aggregate_metrics(self, distinct_managers):
    """
    Add severity-based aggregation
    """
    for manager in distinct_managers:
        try:
            query = """
            INSERT INTO {} (
                risk_count, 
                metric_rolled_up_to,
                metric_run_date,
                metric_snapshot,
                metric_snapshot_number,
                aggregate_roll_up_to,
                metric_year,
                metric_type,
                severity
            )
            SELECT 
                COUNT(*) as risk_count,
                %s as metric_rolled_up_to,
                CURRENT_DATE as metric_run_date,
                %s as metric_snapshot,
                %s as metric_snapshot_number,
                %s as aggregate_roll_up_to,
                EXTRACT(year from CURRENT_DATE) as metric_year,
                'total' as metric_type,
                severity
            FROM {}
            WHERE metric_run_date = CURRENT_DATE
            GROUP BY severity
            """.format(
                SAS_Config.test_sas_risk_summary,
                SAS_Config.test_redshift_apollo_table
            )
            
            self.rs_upsert.execute_redshift_sql_query(
                query, 
                (manager, 'WoW', self.get_week_number(), manager)
            )

        except Exception as error:
            Logger.printLogs("ERROR", f"Aggregation failed: {error}")
            sys.exit(1)
```

Sample queries to verify the data:

```sql
-- Check severity distribution
SELECT 
    severity,
    SUM(risk_count) as total_risks,
    metric_snapshot,
    metric_rolled_up_to
FROM aeetest.sas_risk_summary
WHERE metric_run_date = CURRENT_DATE
GROUP BY severity, metric_snapshot, metric_rolled_up_to
ORDER BY 
    metric_rolled_up_to,
    CASE 
        WHEN severity = 'EMERGENCY' THEN 1
        WHEN severity = 'CRITICAL' THEN 2
        WHEN severity = 'HIGH' THEN 3
    END;

-- Check aggregation by manager
SELECT 
    aggregate_roll_up_to,
    severity,
    SUM(risk_count) as total_risks
FROM aeetest.sas_risk_summary
WHERE metric_run_date = CURRENT_DATE
GROUP BY aggregate_roll_up_to, severity
ORDER BY 
    aggregate_roll_up_to,
    severity;
```

Expected output would look like:
```
aggregate_roll_up_to | severity  | total_risks | metric_snapshot
-------------------- | --------- | ----------- | ---------------
manager1            | EMERGENCY  | 5           | WoW
manager1            | CRITICAL   | 3           | WoW
manager1            | HIGH       | 2           | WoW
manager2            | EMERGENCY  | 4           | WoW
manager2            | CRITICAL   | 6           | WoW
manager2            | HIGH       | 3           | WoW
```

This gives you:
1. Risk count by severity level
2. Manager-wise breakdown
3. Trend analysis capability (WoW/MoM)
4. Priority-based sorting
5. Clear visibility of critical issues

Would you like to see the queries to analyze this data or any other specific aggregation pattern?

------

venry: risk_count | metric_rolled_up_to | metric_run_date | metric_snapshot | metric_snapshot_number | aggregate_roll_up_to | metric_year | metric_type | severity
5          | dmmitta            | 2025-02-19      | WoW            | 8                      | abbaamer            | 2025        | total       | EMERGENCY
3          | dmmitta            | 2025-02-19      | WoW            | 8                      | abbaamer            | 2025        | total       | CRITICAL
2          | dmmitta            | 2025-02-19      | WoW            | 8                      | abbaamer            | 2025        | total       | HIGH


explain this so for each manager now it displays three rows?

------

Cedric: Yes, exactly! Let me explain the change:

**Before (Current):**
```sql
risk_count | metric_rolled_up_to | metric_run_date | metric_snapshot | aggregate_roll_up_to | metric_type
10         | dmmitta            | 2025-02-19      | WoW            | abbaamer            | total       # Single row showing total risks
```

**After (With Severity):**
```sql
risk_count | metric_rolled_up_to | metric_run_date | metric_snapshot | aggregate_roll_up_to | metric_type | severity
5          | dmmitta            | 2025-02-19      | WoW            | abbaamer            | total       | EMERGENCY  # Row 1: Emergency risks
3          | dmmitta            | 2025-02-19      | WoW            | abbaamer            | total       | CRITICAL  # Row 2: Critical risks
2          | dmmitta            | 2025-02-19      | WoW            | abbaamer            | total       | HIGH      # Row 3: High risks
```

This happens because of the `GROUP BY severity` in our aggregation query:

```python
"""
SELECT 
    COUNT(*) as risk_count,           # Counts risks for each severity
    %s as metric_rolled_up_to,        # manager (dmmitta)
    CURRENT_DATE as metric_run_date,  # 2025-02-19
    %s as metric_snapshot,            # WoW
    %s as metric_snapshot_number,     # 8
    %s as aggregate_roll_up_to,       # abbaamer
    EXTRACT(year from CURRENT_DATE) as metric_year, # 2025
    'total' as metric_type,
    severity                          # EMERGENCY/CRITICAL/HIGH
FROM {}
WHERE metric_run_date = CURRENT_DATE
GROUP BY severity                     # This creates separate rows for each severity
"""
```

So for each manager:
1. EMERGENCY row: Shows count of risks with days_non_compliant > 30
2. CRITICAL row: Shows count of risks with days_non_compliant > 20
3. HIGH row: Shows count of remaining risks

Total risks for manager = 5 + 3 + 2 = 10 (same as before, but now broken down by severity)

This gives better visibility into:
- How many high-priority (EMERGENCY) issues need immediate attention
- Distribution of risks across severity levels
- Tracking progress in each severity category

------

venry: Detailed Implementation for Approach 1: Direct RAGS API IntegrationThis document provides step-by-step implementation details, covering every change in the code, API calls, table structure, CSV format, and validation process. Changes in API CallsSince we are now leveraging the RAGS API for severity classification, we need to update how the SAS Plugin fetches risk data. 1.1 Modify API Calls to Fetch Severity-Based Risks Before (Fetching Only Blocked Risks Without Severity Information)def fetch_sas_risks(self, aggregate_id): return self.client.get_resource_high_level_risk_details_by_campaign_type( aggregate_id=aggregate_id, campaign_type="BLOCKED" ) After (Fetching Risks with Severity Categorization)def get_severity_based_risks(self, aggregate_id): high_level_data = self.client.get_resource_high_level_risk_details_by_campaign_type( aggregate_id=aggregate_id, campaign_type="BLOCKED" ) severity_counts = { "EMERGENCY": high_level_data.riskSeverityMapWithRecommendations.get("Critical", 0), "CRITICAL": high_level_data.riskSeverityMapWithRecommendations.get("High", 0), "HIGH": high_level_data.riskSeverityMapWithRecommendations.get("Medium", 0) } return severity_counts 1.2 API Response Example API Request:{ "aggregate_id": "Person:SDM123", "campaign_type": "BLOCKED"} API Response:{ "riskSeverityMapWithRecommendations": { "Critical": 4, "High": 7, "Medium": 2 }}This means:4 "Critical" Risks (Mapped to "EMERGENCY")7 "High" Risks (Mapped to "CRITICAL")2 "Medium" Risks (Mapped to "HIGH") Changes in Data ProcessingWe need to include severity while processing the data before inserting it into Redshift. 2.1 Modify Data Writing for Apollo, Brazil, and Pipeline RisksWe update existing functions like get_apollo_data_and_write_in_file() to include severity. Before (Writing Data Without Severity)def get_apollo_data_and_write_in_file(self, data, file, metric_snapshot): list_data = self.coral_to_python(data) for filtered_depth_data in list_data['Details']: env_name = filtered_depth_data['Name'] stage_name = env_name.split(':')[1] owner_login = filtered_depth_data['OwnerLogin'] file.write("{},{},{},{},{},{}\n".format( env_name, stage_name, owner_login, metric_snapshot, date.today(), date.today().year )) After (Including Severity)def get_apollo_data_and_write_in_file(self, data, file, metric_snapshot, aggregate_id): list_data = self.coral_to_python(data) # Fetch severity mapping from RAGS API severity_counts = self.get_severity_based_risks(aggregate_id) for filtered_depth_data in list_data['Details']: env_name = filtered_depth_data['Name'] stage_name = env_name.split(':')[1] owner_login = filtered_depth_data['OwnerLogin'] # Assign severity severity = "LOW" if severity_counts["EMERGENCY"] > 0: severity = "EMERGENCY" elif severity_counts["CRITICAL"] > 0: severity = "CRITICAL" elif severity_counts["HIGH"] > 0: severity = "HIGH" file.write("{},{},{},{},{},{},{}\n".format( env_name, stage_name, owner_login, metric_snapshot, severity, date.today(), date.today().year )) Apply This Change to:get_brazil_data_and_write_in_file()get_pipeline_target_data_and_write_in_file() Changes in CSV StructureSince severity is now included, the CSV file structure will change. Before (Existing CSV Format)env_name, stage_name, owner_login, metric_snapshot, date, yeardev_apollo, beta, user123, 20240205, 2024-02-05, 2024prod_apollo, gamma, user456, 20240205, 2024-02-05, 2024 After (Updated CSV Format with Severity)env_name, stage_name, owner_login, metric_snapshot, severity, date, yeardev_apollo, beta, user123, 20240205, HIGH, 2024-02-05, 2024prod_apollo, gamma, user456, 20240205, CRITICAL, 2024-02-05, 2024 Changes in Redshift TableWe need to ensure that the severity column is added to Redshift. Before (Existing Table Schema)CREATE TABLE sas_aggregate_table ( manager_id VARCHAR(50), risk_count INT, risk_age INT); After (New Table Schema with Severity Column)ALTER TABLE sas_aggregate_table ADD COLUMN severity VARCHAR(20);ORCREATE TABLE sas_aggregate_table ( manager_id VARCHAR(50), risk_count INT, risk_age INT, severity VARCHAR(20) CHECK (severity IN ('EMERGENCY', 'CRITICAL', 'HIGH', 'LOW'))); Ensure Redshift Ingestion WorksSince the CSV structure now includes severity, we need to verify that the COPY command loads all columns correctly. Redshift COPY Command to Load CSV with SeverityCOPY sas_aggregate_table FROM 's3://your-bucket/path/to/file.csv'WITH (FORMAT csv, IGNOREHEADER 1); Validation Checklist API Validation Ensure API returns severity-based risks using get_resource_high_level_risk_details_by_campaign_type.
 Verify API response includes riskSeverityMapWithRecommendations with valid counts. Data Processing Validation Ensure get_severity_based_risks correctly maps severity counts.
 Verify severity is correctly assigned in CSV output files. Redshift Validation Ensure the severity column exists in the table.
 Verify that the COPY command successfully ingests the updated CSV.
 Run a sample query to check stored severity values:SELECT manager_id, risk_count, risk_age, severity FROM sas_aggregate_table LIMIT 10; Final Summary of ChangesComponentChanges RequiredAPI CallsFetch severity-based risks using get_resource_high_level_risk_details_by_campaign_type.Data ProcessingModify write_in_file functions to include severity.CSV FormatUpdate to include severity in the last column.Redshift SchemaAdd a severity column to sas_aggregate_table.Redshift IngestionEnsure COPY command processes the updated CSV format. Next Steps Implement API changes in SAS Plugin. Update data processing functions for severity mapping. Modify Redshift schema manually to include severity. Test CSV generation and ingestion in staging. Validate severity classification on Page-0 Dashboard. Why This Approach? Minimal Code Changes  Uses RAGS API without modifying multiple API calls.
 Keeps Existing Flow Intact  Simply extends the existing logic.
 Better Maintainability  Aligns with Amazons risk classification model.Would you like a test plan with sample inputs and expected outputs? 

------

Cedric: Yes, this is a good summary. Based on our previous changes, let me break down what we've already implemented and what it actually means:

1. **API Call Change** (Implemented)
```python
def call_get_high_level_resource_details(self, manager, _client):
    try:
        data = _client.get_resource_high_level_risk_details_by_campaign_type(
            aggregate_id = "Person:"+manager, 
            campaign_type='BLOCKED'
        )
        severity_data = self.get_severity_based_risks(data)
        data.update({'severity_info': severity_data})
```

2. **Test Tables Created** (Done)
```sql
-- Already created test tables with severity columns
aeetest.raw_sas_apollo_metrics
aeetest.raw_sas_brazil_metrics
aeetest.raw_sas_pipeline_target_metrics
aeetest.sas_risk_summary
```

3. **Aggregate Result** (What we get)
```sql
-- For each manager, instead of one row:
risk_count | metric_rolled_up_to | severity
10         | manager1           | null     -- Old format

-- We now get three rows:
risk_count | metric_rolled_up_to | severity
5          | manager1           | EMERGENCY  -- High priority risks
3          | manager1           | CRITICAL   -- Medium priority risks
2          | manager1           | HIGH       -- Lower priority risks
```

4. **Config Changes** (Done)
```python
class SAS_Config:
    severity_emergency = 'EMERGENCY'
    severity_critical = 'CRITICAL'
    severity_high = 'HIGH'
    
    test_redshift_apollo_table = "aeetest.raw_sas_apollo_metrics"
    # ... other test tables
```

5. **Data Flow**:
```mermaid
graph TD
    A[RAGS API] --> B[Get Risk Data with Severity]
    B --> C[Write to CSV with Severity Column]
    C --> D[Load to Test Tables]
    D --> E[Aggregate by Severity]
    E --> F[Three Rows per Manager in Summary]
```

The main benefit is:
- Instead of just seeing "10 risks for manager1"
- We now see "5 EMERGENCY, 3 CRITICAL, 2 HIGH risks for manager1"
- Helps prioritize which risks need immediate attention

Would you like to see some test queries to verify the data in the new format?

------

venry: and what about the config changes?

------

Cedric: Here are the complete config changes needed in `SAS_Config.py`:

```python
class SAS_Config:
    # Existing configurations remain same
    region = "us-west-2"
    security_token = None
    service = "sas-rags-frontend-prod"
    endpoint = "https://sas-rags-frontend-prod-global-pdx.pdx.proxy.amazon.com"

    # Add new severity constants
    severity_emergency = 'EMERGENCY'
    severity_critical = 'CRITICAL'
    severity_high = 'HIGH'

    # Test table names for severity testing
    test_redshift_apollo_table = "aeetest.raw_sas_apollo_metrics"
    test_redshift_brazil_table = "aeetest.raw_sas_brazil_metrics"
    test_redshift_pipeline_target_table = "aeetest.raw_sas_pipeline_target_metrics"
    test_sas_risk_summary = "aeetest.sas_risk_summary"

    # Existing table names remain same
    redshift_apollo_table = "rawdata_schema.raw_sas_apollo_metrics"
    redshift_brazil_table = "rawdata_schema.raw_sas_brazil_metrics"
    redshift_pipeline_target_table = "rawdata_schema.raw_sas_pipeline_target_metrics"
    redshift_apollo_table_v2 = "rawdata_schema.sas_apollo_metrics_v2"
    redshift_brazil_table_v2 = "rawdata_schema.sas_brazil_metrics_v2"
    redshift_pipeline_target_table_v2 = "rawdata_schema.sas_pipeline_target_metrics_v2"
    
    # Existing temp tables remain same
    temp_redshift_apollo_table = "rawdata_schema.temp_sas_apollo_metrics"
    temp_redshift_brazil_table = "rawdata_schema.temp_sas_brazil_metrics"
    temp_redshift_pipeline_target_table = "rawdata_schema.temp_sas_pipeline_target_metrics"

    # Existing rolled tables remain same
    rolled_redshift_apollo_table = "rawdata_schema.sas_apollo_metrics"
    rolled_redshift_brazil_table = "rawdata_schema.sas_brazil_metrics"
    rolled_redshift_pipeline_target_table = "rawdata_schema.sas_pipeline_target_metrics"

    # Severity update query
    severity_update_query = """
        UPDATE {} 
        SET severity = %s,
            risk_priority = %s 
        WHERE apollo_environment_name = %s 
        AND metric_run_date = %s
    """

    # Severity aggregation query
    severity_aggregate_query = """
        INSERT INTO {} (
            risk_count,
            metric_rolled_up_to,
            metric_run_date,
            metric_snapshot,
            metric_snapshot_number,
            aggregate_roll_up_to,
            metric_year,
            metric_type,
            severity
        )
        SELECT 
            COUNT(*) as risk_count,
            %s as metric_rolled_up_to,
            %s as metric_run_date,
            %s as metric_snapshot,
            %s as metric_snapshot_number,
            %s as aggregate_roll_up_to,
            %s as metric_year,
            %s as metric_type,
            severity
        FROM {} 
        WHERE metric_run_date = %s
        GROUP BY severity
    """

    # Rest of existing configurations remain same
    program_name_apollo = 'APOLLO'
    program_name_brazil = 'BRAZIL'
    program_name_pipeline_targets = 'PIPELINE_TARGET'
    BATCH_SIZE = 100
```

The changes are:
1. Added severity constants
2. Added test table names
3. Added severity update query
4. Added severity aggregation query

All other existing configurations remain unchanged. This approach:
- Keeps existing functionality intact
- Adds new severity-related configurations
- Uses test tables for development
- Maintains backward compatibility

Do you want to see how these configurations are used in the main plugin code?

------

venry: yes

------

Cedric: I'll show how these configurations are used in the main plugin code:

1. **First, in call_get_high_level_resource_details** (Already modified):
```python
@retry(wait_fixed=10000, stop_max_attempt_number=3)
def call_get_high_level_resource_details(self, manager, _client):
    try:
        data = _client.get_resource_high_level_risk_details_by_campaign_type(
            aggregate_id = "Person:"+manager, 
            campaign_type='BLOCKED'
        )
        severity_data = self.get_severity_based_risks(data)
        data.update({'severity_info': severity_data})
        
    except ThrottlingException as e:
        Logger.printLogs("EXCEPTION", "ThrottlingException occurred, Sleeping! {}".format(e))
        time.sleep(5)
    except Exception as err:
        Logger.printLogs("[ERROR]", "Error occurred {}".format(err))
        return 1

    return data
```

2. **In get_apollo_data_and_write_in_file**:
```python
def get_apollo_data_and_write_in_file(self, data, file, metric_snapshot):
    list_data = self.coral_to_python(data)
    for filtered_depth_data in list_data['Details']:
        env_name = filtered_depth_data['Name']
        stage_name = env_name.split(':')[1]
        # ... (existing field extractions)

        # Determine severity based on risk age
        days_non_compliant = filtered_depth_data.get('OldestRiskAge', 0)
        if days_non_compliant > 30:
            severity = SAS_Config.severity_emergency
            risk_priority = 3
        elif days_non_compliant > 20:
            severity = SAS_Config.severity_critical
            risk_priority = 2
        else:
            severity = SAS_Config.severity_high
            risk_priority = 1

        # Write to file with severity
        file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(
            env_name,
            stage_name,
            pipeline_id,
            pipeline_name,
            owner,
            owner_login,
            consumer_count,
            children_count,
            risk_count,
            visibility,
            host_count,
            days_non_compliant,
            days_non_compliant_recommendation,
            action_required,
            blocked_resources,
            associated_campaigns,
            current_date,
            current_year,
            classification,
            metric_snapshot,
            severity,           # New field
            risk_priority      # New field
        ))
```

3. **In load_SAS_data_to_redshift**:
```python
def load_SAS_data_to_redshift(self, program_name):
    if (program_name == SAS_Config.program_name_apollo):
        try:
            # Using test table instead of production
            os.system(COMMANDS.Redshift_Operations_Command.format(
                ODIN.Redshift_ODIN,
                "truncate table {}".format(SAS_Config.test_redshift_apollo_table)
            ))
            Logger.printLogs("INFO", "Truncate table '{}' completed successfully.".format(
                SAS_Config.test_redshift_apollo_table
            ))

            os.system(COMMANDS.Redshift_Update_Comamnd.format(
                ODIN.Redshift_ODIN,
                "csv",
                SAS_Config.test_redshift_apollo_table,
                Common_Constants.S3_Bucket+'/'+SAS_Config.sas_apollo_temp_s3_location
            ))
            Logger.printLogs("INFO", "Data updation to table '{}' completed successfully.".format(
                SAS_Config.test_redshift_apollo_table
            ))

        except Exception as error:
            Logger.printLogs("ERROR", "Transferring Apollo data to redshift table failed: {}".format(error))
            sys.exit(1)
    # ... similar changes for brazil and pipeline targets
```

4. **In aggregate_metrics**:
```python
def aggregate_metrics(self, distinct_managers):
    for manager in distinct_managers:
        try:
            # Using the new severity aggregation query
            self.rs_upsert.execute_redshift_sql_query(
                SAS_Config.severity_aggregate_query.format(
                    SAS_Config.test_sas_risk_summary,  # Target table
                    SAS_Config.test_redshift_apollo_table  # Source table
                ),
                (
                    manager,                    # metric_rolled_up_to
                    date.today(),              # metric_run_date
                    'WoW',                     # metric_snapshot
                    self.get_week_number(),    # metric_snapshot_number
                    manager,                   # aggregate_roll_up_to
                    date.today().year,         # metric_year
                    'total'                    # metric_type
                )
            )
            Logger.printLogs("INFO", f"Aggregated metrics for manager {manager}")

        except Exception as error:
            Logger.printLogs("ERROR", f"Aggregation failed: {error}")
            sys.exit(1)
```

5. **To verify the data**:
```python
def verify_severity_data(self):
    """Helper method to verify severity data"""
    try:
        query = """
        SELECT 
            severity,
            COUNT(*) as risk_count
        FROM {}
        GROUP BY severity
        """.format(SAS_Config.test_redshift_apollo_table)
        
        result = self.rs_upsert.run_redshift_select_query(query)
        Logger.printLogs("INFO", f"Severity distribution: {result}")
        
        return result
    except Exception as error:
        Logger.printLogs("ERROR", f"Verification failed: {error}")
        return None
```

The flow is:
1. Get data with severity from RAGS API
2. Write to CSV with severity fields
3. Load to test tables
4. Aggregate by severity
5. Each manager gets three rows in summary (EMERGENCY, CRITICAL, HIGH)

All configuration values are used from SAS_Config, making it easy to:
- Modify severity thresholds
- Switch between test and production tables
- Update queries if needed
- Maintain consistent severity values

Do you want to see any specific part in more detail?

------

venry: def aggregate_metrics(self, distinct_managers):
		'''
		This function creates Aggregate metrics from the Rolled up data
		'''
		Logger.printLogs("[INFO]","Starting Aggregate Rolled up of SAS metrics")
		manager_processed = 0
		for manager in distinct_managers:

			manager_processed = manager_processed + 1

			Logger.printLogs("[DEBUG]","Manager processed [{}]/[{}]".format(manager_processed,len(distinct_managers)))


			insert_total_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_total_query.format(SAS_Config.sas_aggregate_table, manager))
			Logger.printLogs("[DEBUG]","Total Metrics rolled upto {} manager query result {}".format(manager, insert_total_result))


			insert_recommended_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_recommendation_query.format(SAS_Config.sas_aggregate_table, manager))
			Logger.printLogs("[DEBUG]","Recommendations Metrics rolled upto {} manager query result {}".format(manager, insert_recommended_result))


			insert_older_recommendation_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_long_recommendation_query.format(SAS_Config.sas_aggregate_table, manager))
			Logger.printLogs("[DEBUG]","Recommendations Metrics rolled upto {} manager query result {}".format(manager, insert_older_recommendation_result))


			insert_recommended_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_near_sla_query.format(SAS_Config.sas_aggregate_table, manager))
			Logger.printLogs("[DEBUG]","SLA near missed Metrics rolled upto {} manager query result {}".format(manager, insert_recommended_result))

			if (insert_total_result !=0 or insert_recommended_result != 0 or insert_older_recommendation_result != 0):
				Logger.printLogs("ERROR","Aggregation Query Execution failed for manager {}".format(manager))
				sys.exit(1)

how should the changes be made for this fucntion

------

Cedric: Here's how to modify the `aggregate_metrics` function to include severity-based aggregation while maintaining existing functionality:

```python
def aggregate_metrics(self, distinct_managers):
    '''
    This function creates Aggregate metrics from the Rolled up data including severity classification
    '''
    Logger.printLogs("[INFO]", "Starting Aggregate Rolled up of SAS metrics with severity")
    manager_processed = 0
    
    for manager in distinct_managers:
        manager_processed = manager_processed + 1
        Logger.printLogs("[DEBUG]", "Manager processed [{}/{}]".format(
            manager_processed, 
            len(distinct_managers)
        ))

        try:
            # 1. Original total metrics (maintain existing functionality)
            insert_total_result = self.rs_upsert.execute_redshift_sql_query(
                SAS_Config.aggregate_roll_upto_total_query.format(
                    SAS_Config.test_sas_risk_summary, 
                    manager
                )
            )
            Logger.printLogs("[DEBUG]", "Total Metrics rolled up to {} manager query result {}".format(
                manager, 
                insert_total_result
            ))

            # 2. New severity-based metrics
            severity_result = self.rs_upsert.execute_redshift_sql_query(
                SAS_Config.severity_aggregate_query.format(
                    SAS_Config.test_sas_risk_summary,
                    SAS_Config.test_redshift_apollo_table
                ),
                (
                    manager,                    # metric_rolled_up_to
                    date.today(),              # metric_run_date
                    'WoW',                     # metric_snapshot
                    self.get_week_number(),    # metric_snapshot_number
                    manager,                   # aggregate_roll_up_to
                    date.today().year,         # metric_year
                    'severity_based'           # metric_type
                )
            )
            Logger.printLogs("[DEBUG]", "Severity Metrics rolled up to {} manager query result {}".format(
                manager, 
                severity_result
            ))

            # 3. Original recommendation metrics
            insert_recommended_result = self.rs_upsert.execute_redshift_sql_query(
                SAS_Config.aggregate_roll_upto_recommendation_query.format(
                    SAS_Config.test_sas_risk_summary, 
                    manager
                )
            )
            Logger.printLogs("[DEBUG]", "Recommendations Metrics rolled up to {} manager query result {}".format(
                manager, 
                insert_recommended_result
            ))

            # 4. Original older recommendation metrics
            insert_older_recommendation_result = self.rs_upsert.execute_redshift_sql_query(
                SAS_Config.aggregate_roll_upto_long_recommendation_query.format(
                    SAS_Config.test_sas_risk_summary, 
                    manager
                )
            )
            Logger.printLogs("[DEBUG]", "Long Recommendations Metrics rolled up to {} manager query result {}".format(
                manager, 
                insert_older_recommendation_result
            ))

            # 5. Original SLA metrics
            insert_sla_result = self.rs_upsert.execute_redshift_sql_query(
                SAS_Config.aggregate_roll_upto_near_sla_query.format(
                    SAS_Config.test_sas_risk_summary, 
                    manager
                )
            )
            Logger.printLogs("[DEBUG]", "SLA near missed Metrics rolled up to {} manager query result {}".format(
                manager, 
                insert_sla_result
            ))

            # Check for any failures
            if (insert_total_result != 0 or 
                insert_recommended_result != 0 or 
                insert_older_recommendation_result != 0 or 
                insert_sla_result != 0 or 
                severity_result != 0):
                Logger.printLogs("ERROR", "Aggregation Query Execution failed for manager {}".format(manager))
                sys.exit(1)

        except Exception as error:
            Logger.printLogs("ERROR", "Aggregation failed for manager {}: {}".format(manager, error))
            sys.exit(1)

    Logger.printLogs("INFO", "Completed Aggregate Rolled up of SAS metrics with severity")
```

And add this new query to `SAS_Config`:

```python
class SAS_Config:
    # Existing queries remain the same
    aggregate_roll_upto_total_query = """..."""
    aggregate_roll_upto_recommendation_query = """..."""
    aggregate_roll_upto_long_recommendation_query = """..."""
    aggregate_roll_upto_near_sla_query = """..."""

    # New severity aggregation query
    severity_aggregate_query = """
        INSERT INTO {} (
            risk_count,
            metric_rolled_up_to,
            metric_run_date,
            metric_snapshot,
            metric_snapshot_number,
            aggregate_roll_up_to,
            metric_year,
            metric_type,
            severity
        )
        SELECT 
            COUNT(*) as risk_count,
            %s as metric_rolled_up_to,
            %s as metric_run_date,
            %s as metric_snapshot,
            %s as metric_snapshot_number,
            %s as aggregate_roll_up_to,
            %s as metric_year,
            %s as metric_type,
            severity
        FROM {} 
        WHERE metric_run_date = %s
        GROUP BY severity
    """
```

Key changes:
1. Added severity-based aggregation while maintaining existing metrics
2. Uses test tables for development
3. Added proper error handling and logging
4. Maintains backward compatibility
5. Groups risks by severity for each manager

The result will look like:
```sql
-- Original metrics remain unchanged
risk_count | metric_type           | manager
10        | total                 | manager1
5         | recommendations       | manager1
3         | long_recommendations  | manager1
2         | near_sla             | manager1

-- New severity-based metrics added
risk_count | metric_type     | severity  | manager
5         | severity_based  | EMERGENCY | manager1
3         | severity_based  | CRITICAL  | manager1
2         | severity_based  | HIGH      | manager1
```

This approach:
- Preserves existing functionality
- Adds severity classification
- Maintains data consistency
- Enables better risk tracking
- Supports both old and new metrics

------

venry:  # 2. New severity-based metrics
            severity_result = self.rs_upsert.execute_redshift_sql_query(
                SAS_Config.severity_aggregate_query.format(
                    SAS_Config.test_sas_risk_summary,
                    SAS_Config.test_redshift_apollo_table
                ),
                (
                    manager,                    # metric_rolled_up_to
                    date.today(),              # metric_run_date
                    'WoW',                     # metric_snapshot
                    self.get_week_number(),    # metric_snapshot_number
                    manager,                   # aggregate_roll_up_to
                    date.today().year,         # metric_year
                    'severity_based'           # metric_type
                )
            )

is it only based on apollo table?

------

Cedric: Ah good catch! No, we should aggregate from all three tables (Apollo, Brazil, and Pipeline). Here's the corrected version:

```python
# 2. New severity-based metrics
severity_result = self.rs_upsert.execute_redshift_sql_query(
    SAS_Config.severity_aggregate_query.format(
        SAS_Config.test_sas_risk_summary,  # target table
        SAS_Config.test_redshift_apollo_table,  # apollo source
        SAS_Config.test_redshift_brazil_table,  # brazil source
        SAS_Config.test_redshift_pipeline_target_table  # pipeline source
    ),
    (
        manager,                    # metric_rolled_up_to
        date.today(),              # metric_run_date
        'WoW',                     # metric_snapshot
        self.get_week_number(),    # metric_snapshot_number
        manager,                   # aggregate_roll_up_to
        date.today().year,         # metric_year
        'severity_based'           # metric_type
    )
)
```

And update the query in `SAS_Config`:

```python
# Updated query to combine all three tables
severity_aggregate_query = """
    INSERT INTO {} (
        risk_count,
        metric_rolled_up_to,
        metric_run_date,
        metric_snapshot,
        metric_snapshot_number,
        aggregate_roll_up_to,
        metric_year,
        metric_type,
        severity
    )
    SELECT 
        COUNT(*) as risk_count,
        %s as metric_rolled_up_to,
        %s as metric_run_date,
        %s as metric_snapshot,
        %s as metric_snapshot_number,
        %s as aggregate_roll_up_to,
        %s as metric_year,
        %s as metric_type,
        severity
    FROM (
        SELECT severity, metric_run_date FROM {}  -- Apollo
        UNION ALL
        SELECT severity, metric_run_date FROM {}  -- Brazil
        UNION ALL
        SELECT severity, metric_run_date FROM {}  -- Pipeline
    ) combined_risks
    WHERE metric_run_date = %s
    GROUP BY severity
"""
```

This will:
1. Combine risks from all three sources
2. Group by severity across all sources
3. Provide a complete view of all risks

The result will show total risks by severity across all three sources:
```sql
risk_count | metric_type     | severity  | manager
8         | severity_based  | EMERGENCY | manager1  -- Sum from all 3 sources
5         | severity_based  | CRITICAL  | manager1  -- Sum from all 3 sources
3         | severity_based  | HIGH      | manager1  -- Sum from all 3 sources
```

------

venry: I've made the changes, lemme know what I have missed or did incorrect

#######################################################################################
# Metric Provider : SAS Metrics Plug-in                                               #
# Plugin Uses SAS dashboard data for computing SAS Metrics                            #
# Output : Updates the RedShift Table and return the success status                    #
#######################################################################################

import re
import os
import sys
import time
import argparse
from datetime import date
from coral import coralrpc
import coral.complex_types
from retrying import retry
from pyodinhttp import odin_retrieve_pair
from dateutil.relativedelta import relativedelta
from aee_oe_data_collector_aggregator_config.config.sas_config import *
from aee_oe_data_collector_aggregator_config.config.logger import Logger
from com.amazonaws.sasriskaggregationservice.sasriskaggregationservice import *
from com.amazon.coral.availability.throttlingexception import ThrottlingException
from aee_oe_data_collector_aggregator.utilities.downloadUploadToS3 import S3_Operations
from aee_oe_data_collector_aggregator_config.config.basic_config import Common_Constants, ODIN, COMMANDS
from aee_oe_data_collector_aggregator.utilities.redshift_operation_wrapper import redshift_operation_wrapper


class SAS_Plugin:

	def __init__(self):
		Logger.printLogs("INFO","Triggering SAS Plugin")
		self.rs_upsert = redshift_operation_wrapper()

	def SAS_Init(Self):
		# Fetching access key and secret key from admin ODIN
		aws_access_key_id,aws_secret_access_key = odin_retrieve_pair(ODIN.AEE_ODIN)
		aws_access_key_id = aws_access_key_id.data.decode('utf-8').encode()
		aws_secret_access_key = aws_secret_access_key.data.decode('utf-8').encode()

		# Creating orchestrator for client
		_orchestrator = coralrpc.new_orchestrator(
						 endpoint=SAS_Config.endpoint,
						 aws_service=SAS_Config.service,
						 aws_region=SAS_Config.region,
						 signature_algorithm='v4',
						 timeout=30, # 30 seconds why? New service so we don't know about the latency
						 aws_access_key=aws_access_key_id,
						 aws_secret_key=aws_secret_access_key,
						 aws_security_token = SAS_Config.security_token)

		# Creating SAS risk client
		return SASRiskAggregationServiceClient(_orchestrator)

	def _string_to_camelcase(self, s):
		return re.sub(r'(?:^|_)(.)', lambda m: m.group(1).upper(), s)


	def coral_to_python(self, obj):
		if isinstance(obj, coral.complex_types.List):
			return [self.coral_to_python(el) for el in obj]
		if isinstance(obj, coral.complex_types.Structure):
			required_string = {}
			for propname in dir(obj):
				if propname.startswith('_'):
					continue
				required_string[self._string_to_camelcase(propname)] = self.coral_to_python(getattr(obj, propname))
			return required_string
		if isinstance(obj, coral.simple_types.String):
			return str(obj)
		if isinstance(obj, coral.simple_types.Boolean):
			return bool(obj)
		return obj

	def get_entities(self):
		'''
		This function fetches the client names from Config file and returns client names.
		'''
		select_result = self.rs_upsert.run_redshift_select_query(SAS_Config.get_distinct_leader)
		Logger.printLogs("DEBUG","Query result {}".format(select_result))
		if select_result == 1:
			Logger.printLogs("EXCEPTION","Query resulted in exception and returned {}".format(select_result))
			sys.exit(1)

		entities = []
		for clients in select_result:
			entities.append(clients[0])
		if len(entities) < 2:
			entities = str("('") + entities[0] + str("')")
		else:
			entities = tuple(entities)
		return entities

	def get_distinct_managers(self, entities):
		'''
		Based on the entities from get_entities function, this function fetches, all the employees under that login.
		'''
		try:
			Logger.printLogs("INFO", " Calling Manager To Manager Aggregate Mapping Query")
			manager_to_manager_aggregate_mapping = self.rs_upsert.run_redshift_select_query(SAS_Config.manager_to_manager_aggregate_mapping_query.format(entities))

			if manager_to_manager_aggregate_mapping == 1:
				Logger.printLogs("ERROR","Manager Aggregate relation Mapping query failed with Error code 1")
				sys.exit(1)

			if len(manager_to_manager_aggregate_mapping) < 1:
				Logger.printLogs("WARNING","manager aggregate relation mapping is empty")

			distinct_managers = set()
			manager_to_entity_mapping = {}
			for mapping in manager_to_manager_aggregate_mapping:
				roll_up_to_manager = mapping[0]
				manager = mapping[1]
				if roll_up_to_manager in entities:
					distinct_managers.add(manager)
					manager_to_entity_mapping.update({manager:roll_up_to_manager})
			return tuple(distinct_managers), manager_to_entity_mapping, manager_to_manager_aggregate_mapping

		except Exception as error:
			Logger.printLogs("[ ERROR ]","Getting distinct managers failed with error {}".format(error))
			sys.exit(1)

	def load_SAS_data_to_redshift(self, program_name):
		'''
		Truncate the rawdata tables and update the new rawdata in tables. 
		Input: program_name i.e. type of SAS riks
		'''
		if (program_name == SAS_Config.program_name_apollo):
			try:
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table {}".format(SAS_Config.temp_redshift_apollo_table)))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.temp_redshift_apollo_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table " + SAS_Config.redshift_apollo_table))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.redshift_apollo_table))
				os.system(COMMANDS.Redshift_Update_Comamnd.format(ODIN.Redshift_ODIN,"csv",SAS_Config.temp_redshift_apollo_table,Common_Constants.S3_Bucket+'/'+SAS_Config.sas_apollo_temp_s3_location))
				Logger.printLogs("INFO","Data updation to table '{}' completed successfully.".format(SAS_Config.temp_redshift_apollo_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,SAS_Config.query_for_getting_required_data.format(SAS_Config.redshift_apollo_table,SAS_Config.temp_redshift_apollo_table)))
				Logger.printLogs("INFO","Unique Data updation to table '{}' completed successfully.".format(SAS_Config.redshift_apollo_table))

			except Exception as error:
				Logger.printLogs("[ ERROR ]","Transferring Apollo data to redshift table failed with message : {}".format(error))
				sys.exit(1)

		elif (program_name == SAS_Config.program_name_brazil):
			try:
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table {}".format(SAS_Config.temp_redshift_brazil_table)))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.temp_redshift_brazil_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table " + SAS_Config.redshift_brazil_table))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.redshift_brazil_table))
				os.system(COMMANDS.Redshift_Update_Comamnd.format(ODIN.Redshift_ODIN,"csv",SAS_Config.temp_redshift_brazil_table,Common_Constants.S3_Bucket+'/'+SAS_Config.sas_brazil_temp_s3_location))
				Logger.printLogs("INFO","Data updation to table '{}' completed successfully.".format(SAS_Config.redshift_brazil_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,SAS_Config.query_for_getting_required_data.format(SAS_Config.redshift_brazil_table,SAS_Config.temp_redshift_brazil_table)))
				Logger.printLogs("INFO","Unique Data updation to table '{}' completed successfully.".format(SAS_Config.redshift_brazil_table))

			except Exception as error:
				Logger.printLogs("[ ERROR ]","Transferring Brazil data to redshift table failed with message : {}".format(error))
				sys.exit(1)

		elif (program_name == SAS_Config.program_name_pipeline_targets):
			try:
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table {}".format(SAS_Config.temp_redshift_pipeline_target_table)))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.temp_redshift_pipeline_target_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table " + SAS_Config.redshift_pipeline_target_table))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.redshift_pipeline_target_table))
				os.system(COMMANDS.Redshift_Update_Comamnd.format(ODIN.Redshift_ODIN,"csv",SAS_Config.temp_redshift_pipeline_target_table,Common_Constants.S3_Bucket+'/'+SAS_Config.sas_pipeline_target_temp_s3_location))
				Logger.printLogs("INFO","Data updation to table '{}' completed successfully.".format(SAS_Config.temp_redshift_pipeline_target_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,SAS_Config.query_for_getting_required_data.format(SAS_Config.redshift_pipeline_target_table,SAS_Config.temp_redshift_pipeline_target_table)))
				Logger.printLogs("INFO","Unique Data updation to table '{}' completed successfully.".format(SAS_Config.redshift_pipeline_target_table))

			except Exception as error:
				Logger.printLogs("ERROR","Transferring Pipeline Target data to redshift table failed with message : {}".format(error))
				sys.exit(1)
		else:
			Logger.printLogs("ERROR","Wrong Program name passed")
			return

	def create_metrics(self, _client, metric_snapshot):
		'''
		Main function, responsible to start generating metrics.
		Input: 
		_client: Client of SAS API
		metric_snapshot: Type of metrics refresh i.e. WoW or MoM
		'''
		entities = self.get_entities()
		distinct_managers, manager_to_entity_mapping, manager_to_manager_aggregate_mapping =self.get_distinct_managers(entities)
		self.apollo_file = open(SAS_Config.sas_apollo_temp_file,"w")
		self.brazil_file = open(SAS_Config.sas_brazil_temp_file,"w")
		self.pipeline_file = open(SAS_Config.sas_pipeline_target_temp_file,"w")
		self.metric_snapshot = metric_snapshot
		Logger.printLogs("[INFO]","Executing it for all managers")
		iterator = 0
		for manager in distinct_managers:
			Logger.printLogs("[UPDATE]","{} Managers left to process".format(len(distinct_managers)-iterator))
			iterator = iterator+1

			data = self.call_get_high_level_resource_details(manager, _client)

			final_list_of_apollo_environments = []
			final_list_of_brazil_environments = []
			final_list_of_pipeline_targets = []

			try:
				apollo_result = self.coral_to_python(data)['HighLevelResourceDetailsByResourceTypeMap']['ApolloEnvironmentNameAndStage']
				final_list_of_apollo_environments = self.process_high_level_data(apollo_result, SAS_Config.program_name_apollo)
			except Exception as err:
				Logger.printLogs("[INFO]","No Apollo SAS risk found ! {}".format(err))
			try:
				brazil_data = self.coral_to_python(data)['HighLevelResourceDetailsByResourceTypeMap']['BrazilVersionSet']
				final_list_of_brazil_environments = self.process_high_level_data(brazil_data, SAS_Config.program_name_brazil)
			except Exception as err:
				Logger.printLogs("[INFO]","No Brazil version set SAS risk found ! {}".format(err))

			try:
				pipeline_target_data = self.coral_to_python(data)['HighLevelResourceDetailsByResourceTypeMap']['AmazonPipelineTarget']
				final_list_of_pipeline_targets = self.process_high_level_data(pipeline_target_data, SAS_Config.program_name_pipeline_targets)
			except Exception as err:
				Logger.printLogs("[INFO]","No Pipeline target SAS risk found ! {}".format(err))

			try:

				self.call_batch_details_wrapper(manager, _client, final_list_of_apollo_environments, SAS_Config.program_name_apollo)

				self.call_batch_details_wrapper(manager, _client, final_list_of_brazil_environments, SAS_Config.program_name_brazil)

				self.call_batch_details_wrapper(manager,_client,final_list_of_pipeline_targets, SAS_Config.program_name_pipeline_targets)


			except Exception as error:
				Logger.printLogs("ERROR","Getting final details of SAS failed with error {}".format(error))
				sys.exit(1)

		self.apollo_file.close()
		self.brazil_file.close()
		self.pipeline_file.close()

		try:
			S3_Operations.uploadDataToS3(ODIN.S3_ODIN, Common_Constants.S3_Bucket, SAS_Config.sas_apollo_temp_s3_location, SAS_Config.sas_apollo_temp_file)
			self.load_SAS_data_to_redshift(SAS_Config.program_name_apollo)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred while uploading Apollo csv file to S3 bucket. {}".format(err))
			sys.exit(1)

		try:
			S3_Operations.uploadDataToS3(ODIN.S3_ODIN, Common_Constants.S3_Bucket, SAS_Config.sas_brazil_temp_s3_location, SAS_Config.sas_brazil_temp_file)
			self.load_SAS_data_to_redshift(SAS_Config.program_name_brazil)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred while uploading Brazil csv file to S3 bucket. {}".format(err))
			sys.exit(1)

		try:
			S3_Operations.uploadDataToS3(ODIN.S3_ODIN, Common_Constants.S3_Bucket, SAS_Config.sas_pipeline_target_temp_s3_location, SAS_Config.sas_pipeline_target_temp_file)
			self.load_SAS_data_to_redshift(SAS_Config.program_name_pipeline_targets)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred while uploading Pipeline Target csv file to S3 bucket. {}".format(err))
			sys.exit(1)

		try:

			Logger.printLogs("INFO","Starting Rolling up of SAS metrics")
			roll_return_code = self.roll_up_metrics(distinct_managers)

			Logger.printLogs("INFO","Starting Aggregating SAS metrics")
			self.aggregate_metrics(distinct_managers)

			Logger.printLogs("INFO","Updating refresh status of SAS metrics")
			self.insert_success_refresh_status()

			Logger.printLogs("INFO","Updating duplicate tables of SAS metrics for Manual update of OE Risks")
			self.insert_sas_data_into_duplicate_table()

		except Exception as err:
			Logger.printLogs("ERROR","SAS Plug-in failed with error {}".format(err))
			sys.exit(1)

	def process_high_level_data(self, result, resource_type):
		'''
		Function is responsible to get all environments or resources from result and return consolidated list.
		Input: Result, metric_snapshot and type of resource
		Output: Consolidated list of Environments
		'''
		final_list_environments = []
		for environment in result:
			environment_json_info = self.coral_to_python(environment)
			env_name = environment_json_info['ResourceName']
			Logger.printLogs("[DEBUG]","{} environment {} added to list".format(resource_type, env_name))
			final_list_environments.append(env_name)

		return final_list_environments

	def get_apollo_data_and_write_in_file(self, data, file, metric_snapshot):
		'''
		Fetches all the details from processed Apollo data and writes into a csv file.
		'''
		list_data = self.coral_to_python(data)
		for filtered_depth_data in list_data['Details']:
			env_name = filtered_depth_data['Name']
			stage_name = env_name.split(':')[1]
			owner_login = filtered_depth_data['OwnerLogin']
			children_count = filtered_depth_data['ChildrenCount']
			consumer_count = filtered_depth_data['ConsumerCount']
			host_count = filtered_depth_data['HostCount']
			owner = filtered_depth_data['Owner']
			pipeline_id = filtered_depth_data['PipelineId']
			pipeline_name = filtered_depth_data['PipelineName']
			oldest_risk_age = filtered_depth_data['OldestRiskAge']
			risk_count = filtered_depth_data['RiskCount']
			visibility = filtered_depth_data['Visibility']
			recommendation_with_action = {}
			for recommendation in filtered_depth_data['Recommendations']:
				for target_resource in recommendation['AssociatedTargetResources']:
					recommendation_with_action[target_resource] = recommendation['Template']

			for software_risk in filtered_depth_data['SoftwareRisks']:
				for risk_details in software_risk['SoftwareRiskDetails']:
					blocked_resources = risk_details['TargetResource']
					action_required = recommendation_with_action[blocked_resources]
					associated_campaigns = software_risk['AssociatedCampaign']['Name'].replace(",","")

					oldest_risk_age = risk_details['RiskAge']
					oldest_recommendation_age = str(risk_details['RecommendationAge'])

					if (not oldest_recommendation_age.isnumeric()):
						oldest_recommendation_age = '-1'
					
					severity = "HIGH"
					risk_priority = 1

					if int(oldest_risk_age) > 30:
						severity = "EMERGENCY"
						risk_priority = 3
					elif int(oldest_risk_age) > 20:
						severity = "CRITICAL"
						risk_priority = 2

					current_date = date.today()
					snapshot_number = current_date.isocalendar()[1]
					classification = 'None'
					curent_year = current_date.strftime('%Y')
					file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(env_name,
																										 stage_name,
																										 pipeline_id,
																										 pipeline_name,
																										 owner,
																										 owner_login,
																										 consumer_count,
																										 children_count,
																										 risk_count,
																										 visibility,
																										 host_count,
																										 oldest_risk_age,
																										 oldest_recommendation_age,
																										 action_required,
																										 blocked_resources,
																										 associated_campaigns,
																										 current_date,
																										 curent_year,
																										 classification,
																										 metric_snapshot,
																										 snapshot_number,
																										 severity,
																										 risk_priority))

	def get_brazil_data_and_write_in_file(self, data, file, metric_snapshot):
		'''
		Fetches all the details from processed Brazil data and writes into a csv file.
		'''
		list_data = self.coral_to_python(data)
		for version_set_json_info in list_data['Details']:
			version_set = version_set_json_info['Name']
			owner_login = version_set_json_info['OwnerLogin']
			owner = version_set_json_info['Owner']

			pipeline_id = version_set_json_info['PipelineId']
			pipeline_name = version_set_json_info['PipelineName']
			oldest_recommendation_age = version_set_json_info['OldestRecommendationAge']
			oldest_risk_age = version_set_json_info['OldestRiskAge']
			risk_count = version_set_json_info['RiskCount']

			recommendation_with_action = {}
			for recommendation in version_set_json_info['Recommendations']:
				for target_resource in recommendation['Actions']:
					recommendation_with_action[recommendation['TargetResource']] = target_resource['Template']

			for software_risk in version_set_json_info['SoftwareRisks']:
				for risk_details in software_risk['SoftwareRiskDetails']:
					blocked_resources = risk_details['TargetResource']
					try:
						action_required = recommendation_with_action[blocked_resources]
					except Exception as err:
						action_required = 'None'
					associated_campaigns = software_risk['AssociatedCampaign']['Name']
					oldest_risk_age = risk_details['RiskAge']
					oldest_recommendation_age = str(risk_details['RecommendationAge'])

					if (not oldest_recommendation_age.isnumeric()):
						oldest_recommendation_age = '-1'

					severity = "HIGH"
					risk_priority = 1
					if int(oldest_risk_age) > 30:
						severity = "EMERGENCY"
						risk_priority = 3
					elif int(oldest_risk_age) > 20:
						severity = "CRITICAL"
						risk_priority = 2

					current_date = date.today()
					snapshot_number = current_date.isocalendar()[1]
					classification = 'None'
					curent_year = current_date.strftime('%Y')
					file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(version_set,
																						  pipeline_id,
																						  pipeline_name,
																						  owner,
																						  owner_login,
																						  risk_count,
																						  oldest_risk_age,
																						  oldest_recommendation_age,
																						  blocked_resources,
																						  action_required,
																						  associated_campaigns,
																						  current_date,
																						  curent_year,
																						  classification,
																						  metric_snapshot,
																						  snapshot_number,
																						  severity,
																						  risk_priority))

	def get_pipeline_target_data_and_write_in_file(self, data, file, metric_snapshot):
		'''
		Fetches all the details from processed pipeline target data and writes into a csv file.
		'''
		list_data = self.coral_to_python(data)
		for pipeline_target_json_info in list_data['Details']:
			target_name = pipeline_target_json_info['Name']
			pipeline_name = pipeline_target_json_info['PipelineName']
			pipeline_id = pipeline_target_json_info['PipelineId']
			owner_name = pipeline_target_json_info['Owner']
			owner_login = pipeline_target_json_info['OwnerLogin']
			oldest_risk_age = pipeline_target_json_info['OldestRiskAge']
			risk_count = pipeline_target_json_info['RiskCount']

			recommendation_with_action = {}

			try:
				for recommendation in pipeline_target_json_info['Recommendations']:
					for target_resource in recommendation['Actions']:
						recommendation_with_action[recommendation['TargetResource']] = target_resource['Template']
			except Exception as err:
				Logger.printLogs("INFO", "Please Refer to Associated campaign urls to follow the recommendation")

			for software_risk in pipeline_target_json_info['SoftwareRisks']:
				for risk_details in software_risk['SoftwareRiskDetails']:
					blocked_resources = risk_details['TargetResource']
				try:
					action_required = recommendation_with_action[blocked_resources]
				except Exception as err:
					action_required = 'None'
				associated_campaigns = software_risk['AssociatedCampaign']['Name']
				oldest_risk_age = risk_details['RiskAge']

				try:
					oldest_recommendations_age = str(risk_details['RecommendationAge'])
				except Exception:
					Logger.printLogs("INFO","Recommendation Age is not configured in Pipeline Target plug-in, setting default value")
					oldest_recommendations_age = '-1'

				severity = "HIGH"
				risk_priority = 1

				if int(oldest_risk_age) > 30:
					severity = "EMERGENCY"
					risk_priority = 3
				elif int(oldest_risk_age) > 20:
					severity = "CRITICAL"
					risk_priority = 2

				current_date = date.today()
				snapshot_number = current_date.isocalendar()[1]
				classification = 'None'
				current_year = current_date.strftime('%Y')

				file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(target_name,
																					  pipeline_id,
																					  pipeline_name,
																					  owner_name,
																					  owner_login,
																					  risk_count,
																					  oldest_risk_age,
																					  oldest_recommendations_age,
																					  blocked_resources,
																					  action_required,
																					  associated_campaigns,
																					  current_date,
																					  current_year,
																					  classification,
																					  metric_snapshot,
																					  snapshot_number,
																					  severity,
																					  risk_priority
																					  ))

	@retry(wait_fixed=10000, stop_max_attempt_number=3)
	def call_get_high_level_resource_details(self, manager, _client):
		'''
		Makes API call to get the high level SAS risk details for any particular manager. It includes all Blocked SAS risks.
		'''
		try:
			data = _client.get_resource_high_level_risk_details_by_campaign_type(aggregate_id = "Person:"+manager, campaign_type='BLOCKED')
			severity_data = self.get_severity_based_risks(data)
			data.update({'severity_info': severity_data})

		except ThrottlingException as e:
			Logger.printLogs("EXCEPTION","ThrottlingException occurred, Sleeping! {}".format(e))
			time.sleep(5)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred {}".format(err))
			return 1

		return data
	
	def get_severity_based_risks(self, high_level_data):
		'''
		Process severity information from RAGS API response
		'''
		try:
			severity_counts = {
				"EMERGENCY": high_level_data.riskSeverityMapWithRecommendations.get("Critical",0),
				"CRITICAL": high_level_data.riskSeverityMapWithRecommendations.get("High", 0),
				"HIGH": high_level_data.riskSeverityMapWithRecommendations.get("Medium", 0)
			}
			Logger.printLogs("[DEBUG]", "Severity Counts: {}".format(severity_counts))
			return severity_counts
		except Exception as err:
			Logger.printLogs("[ERROR]", "Error processing severity {}".format(err))
			return 1
	
	def call_batch_details_wrapper(self, manager, _client, final_list_of_environments, type_of_env):
		'''
		This is the wrapper function, which is responsible to limit the batch size to 100 of every request.
		'''
		batch_iterations = len(final_list_of_environments)// SAS_Config.BATCH_SIZE +1
		Logger.printLogs("[INFO]","Inside Call Batch Wrapper function: Total Batches required {}".format(batch_iterations))

		try:
			for start_range in range(0,len(final_list_of_environments),SAS_Config.BATCH_SIZE):
				end_range = start_range + SAS_Config.BATCH_SIZE

				consolidated_data = self.call_batch_details(manager, _client, final_list_of_environments[start_range:end_range], type_of_env)

				if type_of_env == SAS_Config.program_name_apollo and consolidated_data != 1:
					self.get_apollo_data_and_write_in_file(consolidated_data, self.apollo_file, self.metric_snapshot)
				elif type_of_env == SAS_Config.program_name_brazil and consolidated_data != 1:
					self.get_brazil_data_and_write_in_file(consolidated_data, self.brazil_file, self.metric_snapshot)
				elif type_of_env == SAS_Config.program_name_pipeline_targets and consolidated_data != 1:
					self.get_pipeline_target_data_and_write_in_file(consolidated_data, self.pipeline_file, self.metric_snapshot)

		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred {}".format(err))
			sys.exit(1)

		return


	@retry(wait_fixed=10000, stop_max_attempt_number=3)
	def call_batch_details(self, manager, _client, final_list_of_environments, type_of_env):
		'''
		This function takes list of environments as input and fetches all detailed information.
		'''
		Logger.printLogs("[DEBUG]","Batch Length is {}".format(len(final_list_of_environments)))
		try:
			if (type_of_env == SAS_Config.program_name_apollo):
				data = _client.batch_get_apollo_environment_stage_details(requester_login = manager, entity_id_list=final_list_of_environments, campaign_type='BLOCKED')
			elif (type_of_env == SAS_Config.program_name_brazil):
				data = _client.batch_get_brazil_version_set_details(requester_login = manager, entity_id_list=final_list_of_environments, campaign_type='BLOCKED')
			elif (type_of_env == SAS_Config.program_name_pipeline_targets):
				data = _client.batch_get_pipeline_target_resource_details(requester_login = manager, entity_id_list = final_list_of_environments, campaign_type = 'BLOCKED')
			else:
				Logger.printLogs("ERROR","Wrong type_of_env is passed, Please check! ")

		except ThrottlingException as error:
			Logger.printLogs("EXCEPTION","ThrottlingException occurred, Sleeping! {}".format(error))
			time.sleep(5)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred {}".format(err))
			return 1

		return data

	def roll_up_metrics(self, distinct_managers):
		'''
		Function responsible for Rolling up the metrics by using Rawdata tables.
		'''
		Logger.printLogs("[INFO]","Starting rolling up of SAS metrics")

		for manager in distinct_managers:


			insert_apollo_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.roll_upto_manager_query.format(SAS_Config.rolled_redshift_apollo_table,
																										  manager,
																										  SAS_Config.redshift_apollo_table))
			Logger.printLogs("[DEBUG]","Apollo Data rolled upto query for {} manager result {}".format(manager, insert_apollo_result))


			insert_brazil_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.roll_upto_manager_query.format(SAS_Config.rolled_redshift_brazil_table,
																										  manager,
																										  SAS_Config.redshift_brazil_table))
			Logger.printLogs("[DEBUG]","Brazil Data rolled upto query for {} manager result {}".format(manager, insert_brazil_result))


			insert_pipeline_target_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.roll_upto_manager_query.format(SAS_Config.rolled_redshift_pipeline_target_table,
																										  manager,
																										  SAS_Config.redshift_pipeline_target_table))
			Logger.printLogs("[DEBUG]","Pipeline Target Data rolled upto query for {} manager result {}".format(manager, insert_pipeline_target_result))

			if (insert_apollo_result != 0 or insert_brazil_result != 0 or insert_pipeline_target_result != 0):
				Logger.printLogs("ERROR","Query failed to roll the data for manager {}".format(manager))
				sys.exit(1)



	def aggregate_metrics(self, distinct_managers):
		'''
		This function creates Aggregate metrics from the Rolled up data
		'''
		Logger.printLogs("[INFO]","Starting Aggregate Rolled up of SAS metrics")
		manager_processed = 0
		for manager in distinct_managers:

			manager_processed = manager_processed + 1

			Logger.printLogs("[DEBUG]","Manager processed [{}]/[{}]".format(manager_processed,len(distinct_managers)))


			insert_total_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_total_query.format(SAS_Config.test_sas_risk_summary, manager))
			Logger.printLogs("[DEBUG]","Total Metrics rolled upto {} manager query result {}".format(manager, insert_total_result))


			severity_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_severity_query.format(SAS_Config.test_sas_risk_summary, SAS_Config.test_redshift_apollo_table, SAS_Config.test_redshift_brazil_table,SAS_Config.test_redshift_pipeline_target_table), (manager, date.today(),WoW, self.get_week_number(),manager,date.today().year,'severity_based'))
			Logger.printLogs("[DEBUG]", "Severity Metrics rolled upto {} manager query result {}".format(manager, severity_result))

			insert_recommended_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_recommendation_query.format(SAS_Config.test_sas_risk_summary, manager))
			Logger.printLogs("[DEBUG]","Recommendations Metrics rolled upto {} manager query result {}".format(manager, insert_recommended_result))


			insert_older_recommendation_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_long_recommendation_query.format(SAS_Config.test_sas_risk_summary, manager))
			Logger.printLogs("[DEBUG]","Recommendations Metrics rolled upto {} manager query result {}".format(manager, insert_older_recommendation_result))


			insert_recommended_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_near_sla_query.format(SAS_Config.test_sas_risk_summary, manager))
			Logger.printLogs("[DEBUG]","SLA near missed Metrics rolled upto {} manager query result {}".format(manager, insert_recommended_result))

			insert_sla_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_sla_query.format(SAS_Config.test_sas_risk_summary, manager))
			Logger.printLogs("[DEBUG]", "SLA missed Metrics rolled upto {} manager query result {}".format(manager, insert_sla_result))
			
			if (insert_total_result !=0 or insert_recommended_result != 0 or insert_older_recommendation_result != 0 or insert_sla_result != 0 or severity_result != 0):
				Logger.printLogs("ERROR","Aggregation Query Execution failed for manager {}".format(manager))
				sys.exit(1)



	def insert_success_refresh_status(self):
		'''
		Updates the refresh status when plug-in runs successfullly.
		'''

		try:
			next_run_date = date.today() + relativedelta(days=7)
			insert_status = self.rs_upsert.execute_redshift_sql_query(SAS_Config.insert_status_of_sas_run.format(date.today(), date.today(),next_run_date))
			Logger.printLogs("DEBUG","Status Insertion Query result {}".format(insert_status))

		except Exception as error:
			Logger.printLogs("ERROR","Inserting refresh status failed with message: {}".format(error))
			sys.exit(1)

	def insert_sas_data_into_duplicate_table(self):
		'''
        Inserting data into respective SAS tables for Manual OE risk update in Amazon Arik portal.
        '''
		try:
			#Adding SAS Apollo data to duplicate table for Manual update of SAS Risks [AmazonArik OE Risk]
			insert_status_apollo = self.rs_upsert.execute_redshift_sql_query(SAS_Config.sas_temp_apollo_insert_query)
			Logger.printLogs("INFO","Data update to duplicate apollo sas table '{}' completed successfully {}.".format(SAS_Config.redshift_apollo_table_v2, insert_status_apollo))

			#Adding SAS Brazil data to duplicate table for Manual update of SAS Risks [AmazonArik OE Risk]
			insert_status_brazil = self.rs_upsert.execute_redshift_sql_query(SAS_Config.sas_temp_brazil_insert_query)
			Logger.printLogs("INFO","Data update to duplicate brazil sas table '{}' completed successfully {}.".format(SAS_Config.redshift_brazil_table_v2, insert_status_brazil))

			#Adding SAS Pipeline Target data to duplicate table for Manual update of SAS Risks [AmazonArik OE Risk]
			insert_status_pipeline_target = self.rs_upsert.execute_redshift_sql_query(SAS_Config.sas_temp_pipeline_target_insert_query)
			Logger.printLogs("INFO","Data update to duplicate Pipeline target sas table '{}' completed successfully {}.".format(SAS_Config.redshift_pipeline_target_table_v2, insert_status_pipeline_target))

		except Exception as error:
			Logger.printLogs("ERROR","Inserting into duplicate table status failed with message: {}".format(error))
			sys.exit(1)

def main():
	obj = SAS_Plugin()
	_client = obj.SAS_Init()
	parser = argparse.ArgumentParser(description='Getting the Snapshot type of SAS Plug-in')
	parser.add_argument('-s', '--snapshot',  nargs="*", help='Plugin Snapshot Type: WoW / MoM', required=True)
	args = vars(parser.parse_args())
	metric_snapshot = args['snapshot'][0]
	obj.create_metrics(_client, metric_snapshot)

if __name__ == "__main__":
	main()

class SAS_Config:

	region = "us-west-2"

	security_token = None

	service = "sas-rags-frontend-prod"

	endpoint = "https://sas-rags-frontend-prod-global-pdx.pdx.proxy.amazon.com"
	
	severity_emergency = 'EMERGENCY'

	severity_critical = 'CRITICAL'
	
	severity_high = 'HIGH'

	test_redshift_apollo_table = "aeetest.raw_sas_apollo_metrics"
	test_redshift_brazil_table = "aeetest.raw_sas_brazil_metrics"
	test_redshift_pipeline_target_table = "aeetest.raw_sas_pipeline_target_metrics"
	test_sas_risk_summary = "aeetest.sas_risk_summary"

	severity_update_query = """update {} set severity = %s, risk_priority = %s where apollo_environment_name = %s and metric_run_date = %s"""
	severity_aggregate_update_query = """
	insert into {} (
		risk_count,
		metric_rolled_up_to,
		metric_run_date,
		metric_snapshot,
		metric_snapshot_number,
		aggregate_roll_up_to,
		metric_year,
		metric_type,
		severity
	)
	select
		count(*) as risk_count,
		%s as metric_roll_up_to,
		%s as metric_run_date,
		%s as metric_snapshot,
		%s as metric_snapshot_number,
		%s as aggregate_roll_up_to,
		%s as metric_year,
		%s as metric_type,
		severity
	from (
		select severity, metric_run_date from {}
		union all
		select severity, metric_run_date from {}
		union all
		select severity, metric_run_date from {}
	) as combined_risks
	where metric_run_date = %s
	group by severity
	"""
	sas_aggregate_table = "aggregate_schema.sas_risk_summary"

	sas_apollo_temp_s3_location = "P0_metrics_raw_data/SAS/SAS_Apollo_File.csv"

	sas_brazil_temp_s3_location = "P0_metrics_raw_data/SAS/SAS_Brazil_File.csv"

	sas_pipeline_target_temp_s3_location = "P0_metrics_raw_data/SAS/SAS_Pipeline_Target_File.csv"

	sas_apollo_temp_file = "/apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Apollo_File.csv"

	sas_brazil_temp_file = "/apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Brazil_File.csv"

	sas_pipeline_target_temp_file = "/apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Pipeline_Target_File.csv"

	sas_status_query = """insert into rawdata_schema.page0_plugin_run_status values('{}','{}','{}','{}','{}','{}','{}','{}','{}','{}')"""

	redshift_apollo_table = "rawdata_schema.raw_sas_apollo_metrics"

	redshift_brazil_table = "rawdata_schema.raw_sas_brazil_metrics"

	redshift_pipeline_target_table = "rawdata_schema.raw_sas_pipeline_target_metrics"

	redshift_apollo_table_v2 = "rawdata_schema.sas_apollo_metrics_v2"

	redshift_brazil_table_v2 = "rawdata_schema.sas_brazil_metrics_v2"

	redshift_pipeline_target_table_v2 = "rawdata_schema.sas_pipeline_target_metrics_v2"

	temp_redshift_apollo_table = "rawdata_schema.temp_sas_apollo_metrics"

	temp_redshift_brazil_table = "rawdata_schema.temp_sas_brazil_metrics"

	temp_redshift_pipeline_target_table = "rawdata_schema.temp_sas_pipeline_target_metrics"

	rolled_redshift_apollo_table = "rawdata_schema.sas_apollo_metrics"

	rolled_redshift_brazil_table = "rawdata_schema.sas_brazil_metrics"

	rolled_redshift_pipeline_target_table = "rawdata_schema.sas_pipeline_target_metrics"

	program_name_apollo = 'APOLLO'

	program_name_brazil = 'BRAZIL'

	program_name_pipeline_targets = 'PIPELINE_TARGET'

	pipeline_target_function_flag = 'PIPELINE_TARGET_FUNCTION'

	BATCH_SIZE = 100

	query_for_getting_required_data = """insert into {} select distinct * from {}"""

	get_distinct_leader = "select distinct leader from relationship_schema.p0_clients_primary"

	insert_status_of_sas_run = """ 
	insert into rawdata_schema.page0_plugin_run_status 
	values('ALLORG','ALLManager','','SAS','SAS Risks',0,'{}','{}','Weekly','{}');
	"""

	manager_to_manager_aggregate_mapping_query = """
	select relation_entity,relation_manager FROM 
	relationship_schema.entity_manager_relationship WHERE relation_entity IN (SELECT DISTINCT relation_manager FROM 
	relationship_schema.entity_manager_relationship WHERE relation_entity IN {0}) and 
	relation_rundate in (select max(relation_rundate) from relationship_schema.entity_manager_relationship)"""

	roll_upto_manager_query = """
	insert into {0}
	select *,
		   '{1}' AS rolled_upto
	FROM {2}
	WHERE owner_login IN (SELECT DISTINCT login_name
					  FROM BOOKER.D_EMPLOYEE_DETAILS
					  WHERE supervisor_login_name IN (SELECT DISTINCT relation_manager
													  FROM relationship_schema.entity_manager_relationship
													  WHERE relation_entity = '{1}'))
	UNION all
	SELECT *,
		   '{1}' AS rolled_upto
	FROM {2}
	WHERE owner_login = '{1}'
	"""

	aggregate_roll_upto_total_query ="""
	insert into {0} (
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'total' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from rawdata_schema.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from rawdata_schema.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_brazil_metrics) and aggregate_roll_up_to in
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))
	union all
	select pipeline_target_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,pipeline_target_action_type as action_type
	from rawdata_schema.sas_pipeline_target_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_pipeline_target_metrics) and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}')))
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_rolled_upto,metric_year,metric_snapshot_number
	union all
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'total' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from rawdata_schema.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from rawdata_schema.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_brazil_metrics) and aggregate_roll_up_to in ('{1}')
	union all
	select pipeline_target_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,pipeline_target_action_type as action_type
	from rawdata_schema.sas_pipeline_target_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_pipeline_target_metrics) and aggregate_roll_up_to in ('{1}'))
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_roll_up_to,metric_year,metric_snapshot_number)
	"""

	aggregate_roll_upto_near_sla_query = """
	insert into {0} (
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'75DaySASRisk' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from rawdata_schema.sas_apollo_metrics where metric_run_date = (select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))  and 
		cast(days_non_compliant_recommendation as int) >= 20 and 
		cast(days_non_compliant_recommendation as int) < 30
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from rawdata_schema.sas_brazil_metrics where metric_run_date = (select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}')) and 
		cast(days_non_compliant_recommendation as int) >= 20 and 
		cast(days_non_compliant_recommendation as int) < 30)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_rolled_upto,metric_year,metric_snapshot_number
	union all
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'75DaySASRisk' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from rawdata_schema.sas_apollo_metrics where metric_run_date = (select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) >= 20 and 
		cast(days_non_compliant_recommendation as int) < 30
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from rawdata_schema.sas_brazil_metrics where metric_run_date = (select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) >= 20 and 
		cast(days_non_compliant_recommendation as int) < 30)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_roll_up_to,metric_year,metric_snapshot_number)
	"""

	aggregate_roll_upto_recommendation_query = """
	insert into {0} (
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'recommendations' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from rawdata_schema.sas_apollo_metrics where metric_run_date = (select max(metric_run_date) from rawdata_schema.sas_apollo_metrics)
	and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))  and 
		cast(days_non_compliant_recommendation as int) > 0
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from rawdata_schema.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}')) and 
		cast(days_non_compliant_recommendation as int) > 0)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_rolled_upto,metric_year,metric_snapshot_number
	union all
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'recommendations' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from rawdata_schema.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) > 0
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from rawdata_schema.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) > 0)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_roll_up_to,metric_year,metric_snapshot_number)
	"""

	aggregate_roll_upto_long_recommendation_query = """
	insert into {0} (
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'90DaySASRisk' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from rawdata_schema.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))  and 
		cast(days_non_compliant_recommendation as int) > 30
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from rawdata_schema.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_brazil_metrics) and aggregate_roll_up_to in
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}')) and 
		cast(days_non_compliant_recommendation as int) > 30)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_rolled_upto,metric_year,metric_snapshot_number
	union all
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'90DaySASRisk' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from rawdata_schema.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) > 30
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from rawdata_schema.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from rawdata_schema.sas_brazil_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) > 30)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_roll_up_to,metric_year,metric_snapshot_number)
	"""

	'''
	Insert query to copy retrieve data from parent table to new table with Max(rundate)
	for Manual update of OE Risks by SE/Dev in Arik portal project.

	Master ticket - https://issues.amazon.com/issues/AEESE-3612
	'''

	sas_temp_apollo_insert_query = """
    INSERT INTO rawdata_schema.sas_apollo_metrics_v2 (apollo_environment_name, apollo_environment_stage, pipeline_id,
    pipeline_name, owner, owner_login, consumers, children, software_count, visibility, host_count, days_non_compliant,
    days_non_compliant_recommendation, apollo_action, blocked_resources, associated_campaigns, metric_run_date, metric_year,
    classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to) 
    SELECT apollo_environment_name, apollo_environment_stage, pipeline_id, pipeline_name, owner, owner_login, consumers,
    children, software_count, visibility, host_count, days_non_compliant, days_non_compliant_recommendation, apollo_action,
    blocked_resources, associated_campaigns, metric_run_date, metric_year, classification, metric_snapshot, metric_snapshot_number,
    aggregate_roll_up_to FROM rawdata_schema.sas_apollo_metrics
    WHERE metric_run_date = (SELECT MAX(metric_run_date) FROM rawdata_schema.sas_apollo_metrics)
    """

	sas_temp_brazil_insert_query = """
	INSERT INTO rawdata_schema.sas_brazil_metrics_v2 (brazil_versionset_name, pipeline_id, pipeline_name, owner, owner_login,
	software_count, days_non_compliant, days_non_compliant_recommendation, blocked_resource, brazil_action_type, associated_campaigns,
	metric_run_date, metric_year, classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to) 
    SELECT brazil_versionset_name, pipeline_id, pipeline_name, owner, owner_login, software_count, days_non_compliant,
    days_non_compliant_recommendation, blocked_resource, brazil_action_type, associated_campaigns, metric_run_date, metric_year,
    classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to FROM rawdata_schema.sas_brazil_metrics
    WHERE metric_run_date = (SELECT MAX(metric_run_date) FROM rawdata_schema.sas_brazil_metrics)
    """

	sas_temp_pipeline_target_insert_query = """
	INSERT INTO rawdata_schema.sas_pipeline_target_metrics_v2 (pipeline_target_name, pipeline_id, pipeline_name, owner, owner_login,
	software_count, days_non_compliant, days_non_compliant_recommendation, blocked_resource, pipeline_target_action_type, associated_campaigns,
	metric_run_date, metric_year, classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to) 
    SELECT pipeline_target_name, pipeline_id, pipeline_name, owner, owner_login, software_count, days_non_compliant,
    days_non_compliant_recommendation, blocked_resource, pipeline_target_action_type, associated_campaigns, metric_run_date, metric_year,
    classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to FROM rawdata_schema.sas_pipeline_target_metrics
    WHERE metric_run_date = (SELECT MAX(metric_run_date) FROM rawdata_schema.sas_pipeline_target_metrics)
    """


------

Cedric: Looking at your changes, there are a few things that need attention:

1. In `aggregate_metrics`, you need to fix the severity query execution:
```python
# Change this line
severity_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_severity_query.format(SAS_Config.test_sas_risk_summary, SAS_Config.test_redshift_apollo_table, SAS_Config.test_redshift_brazil_table,SAS_Config.test_redshift_pipeline_target_table), (manager, date.today(),WoW, self.get_week_number(),manager,date.today().year,'severity_based'))
```

to:
```python
severity_result = self.rs_upsert.execute_redshift_sql_query(
    SAS_Config.severity_aggregate_query.format(
        SAS_Config.test_sas_risk_summary,
        SAS_Config.test_redshift_apollo_table,
        SAS_Config.test_redshift_brazil_table,
        SAS_Config.test_redshift_pipeline_target_table
    ),
    (
        manager,                    # metric_rolled_up_to
        date.today(),              # metric_run_date
        'WoW',                     # metric_snapshot
        self.get_week_number(),    # metric_snapshot_number
        manager,                   # aggregate_roll_up_to
        date.today().year,         # metric_year
        'severity_based'           # metric_type
    )
)
```

2. You need to add the missing comma in the file writes:
```python
# In get_apollo_data_and_write_in_file
file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(
    # ... existing fields ...,
    metric_snapshot,
    snapshot_number,
    severity,           # Added comma here
    risk_priority      # New field
))
```

3. In SAS_Config, rename the query to match the usage:
```python
# Change this
severity_aggregate_update_query = """..."""

# To this
severity_aggregate_query = """..."""
```

4. Make sure v2 tables also have severity columns:
```sql
ALTER TABLE rawdata_schema.sas_apollo_metrics_v2 
ADD COLUMN severity VARCHAR(10),
ADD COLUMN risk_priority INT;

-- Update v2 insert query
sas_temp_apollo_insert_query = """
INSERT INTO rawdata_schema.sas_apollo_metrics_v2 
(
    -- existing columns...,
    severity,
    risk_priority
) 
SELECT 
    -- existing columns...,
    severity,
    risk_priority
FROM rawdata_schema.sas_apollo_metrics
WHERE metric_run_date = (SELECT MAX(metric_run_date) 
                        FROM rawdata_schema.sas_apollo_metrics)
"""
```

5. Add error checking for severity processing:
```python
def get_severity_based_risks(self, high_level_data):
    try:
        severity_counts = {
            "EMERGENCY": high_level_data.riskSeverityMapWithRecommendations.get("Critical", 0),
            "CRITICAL": high_level_data.riskSeverityMapWithRecommendations.get("High", 0),
            "HIGH": high_level_data.riskSeverityMapWithRecommendations.get("Medium", 0)
        }
        Logger.printLogs("DEBUG", f"Processed severity counts: {severity_counts}")
        return severity_counts
    except Exception as e:
        Logger.printLogs("ERROR", f"Failed to process severity: {e}")
        return {"HIGH": 0}  # Default fallback
```

These changes will ensure that:
1. Severity aggregation works correctly
2. Data is written properly to files
3. v2 tables include severity information
4. Error handling is robust
5. Configuration is consistent

Let me know if you need clarification on any of these changes!

------

venry: do i need to create test tables for these too 

	redshift_apollo_table_v2 = "rawdata_schema.sas_apollo_metrics_v2"

	redshift_brazil_table_v2 = "rawdata_schema.sas_brazil_metrics_v2"

	redshift_pipeline_target_table_v2 = "rawdata_schema.sas_pipeline_target_metrics_v2"

	temp_redshift_apollo_table = "rawdata_schema.temp_sas_apollo_metrics"

	temp_redshift_brazil_table = "rawdata_schema.temp_sas_brazil_metrics"

	temp_redshift_pipeline_target_table = "rawdata_schema.temp_sas_pipeline_target_metrics"

	rolled_redshift_apollo_table = "rawdata_schema.sas_apollo_metrics"

	rolled_redshift_brazil_table = "rawdata_schema.sas_brazil_metrics"

	rolled_redshift_pipeline_target_table = "rawdata_schema.sas_pipeline_target_metrics"



------

Cedric: For testing purposes, yes, you should create test versions of these tables to avoid affecting production data. Here's how to set it up:

1. **Add Test Table Names in SAS_Config**:
```python
class SAS_Config:
    # Existing test tables
    test_redshift_apollo_table = "aeetest.raw_sas_apollo_metrics"
    test_redshift_brazil_table = "aeetest.raw_sas_brazil_metrics"
    test_redshift_pipeline_target_table = "aeetest.raw_sas_pipeline_target_metrics"
    
    # Add test versions for v2 tables
    test_redshift_apollo_table_v2 = "aeetest.sas_apollo_metrics_v2"
    test_redshift_brazil_table_v2 = "aeetest.sas_brazil_metrics_v2"
    test_redshift_pipeline_target_table_v2 = "aeetest.sas_pipeline_target_metrics_v2"
    
    # Add test versions for temp tables
    test_temp_redshift_apollo_table = "aeetest.temp_sas_apollo_metrics"
    test_temp_redshift_brazil_table = "aeetest.temp_sas_brazil_metrics"
    test_temp_redshift_pipeline_target_table = "aeetest.temp_sas_pipeline_target_metrics"
    
    # Add test versions for rolled tables
    test_rolled_redshift_apollo_table = "aeetest.sas_apollo_metrics"
    test_rolled_redshift_brazil_table = "aeetest.sas_brazil_metrics"
    test_rolled_redshift_pipeline_target_table = "aeetest.sas_pipeline_target_metrics"
```

2. **Create Test Tables SQL**:
```sql
-- Create test v2 tables
CREATE TABLE aeetest.sas_apollo_metrics_v2 
(LIKE rawdata_schema.sas_apollo_metrics_v2 INCLUDING ALL);

CREATE TABLE aeetest.sas_brazil_metrics_v2 
(LIKE rawdata_schema.sas_brazil_metrics_v2 INCLUDING ALL);

CREATE TABLE aeetest.sas_pipeline_target_metrics_v2 
(LIKE rawdata_schema.sas_pipeline_target_metrics_v2 INCLUDING ALL);

-- Create test temp tables
CREATE TABLE aeetest.temp_sas_apollo_metrics 
(LIKE rawdata_schema.temp_sas_apollo_metrics INCLUDING ALL);

CREATE TABLE aeetest.temp_sas_brazil_metrics 
(LIKE rawdata_schema.temp_sas_brazil_metrics INCLUDING ALL);

CREATE TABLE aeetest.temp_sas_pipeline_target_metrics 
(LIKE rawdata_schema.temp_sas_pipeline_target_metrics INCLUDING ALL);

-- Create test rolled tables
CREATE TABLE aeetest.sas_apollo_metrics 
(LIKE rawdata_schema.sas_apollo_metrics INCLUDING ALL);

CREATE TABLE aeetest.sas_brazil_metrics 
(LIKE rawdata_schema.sas_brazil_metrics INCLUDING ALL);

CREATE TABLE aeetest.sas_pipeline_target_metrics 
(LIKE rawdata_schema.sas_pipeline_target_metrics INCLUDING ALL);

-- Add severity columns to all test tables if not present
ALTER TABLE aeetest.sas_apollo_metrics_v2 
ADD COLUMN IF NOT EXISTS severity VARCHAR(10),
ADD COLUMN IF NOT EXISTS risk_priority INT;

-- Repeat for other tables...
```

3. **Update Methods to Use Test Tables**:
```python
def insert_sas_data_into_duplicate_table(self):
    try:
        # Using test tables instead of production
        insert_status_apollo = self.rs_upsert.execute_redshift_sql_query(
            SAS_Config.sas_temp_apollo_insert_query.format(
                SAS_Config.test_redshift_apollo_table_v2,
                SAS_Config.test_redshift_apollo_table
            )
        )
        Logger.printLogs("INFO", "Data update to test duplicate apollo table completed")

        # Similar changes for brazil and pipeline tables...

    except Exception as error:
        Logger.printLogs("ERROR", f"Insert to duplicate tables failed: {error}")
        sys.exit(1)

def load_SAS_data_to_redshift(self, program_name):
    if (program_name == SAS_Config.program_name_apollo):
        try:
            os.system(COMMANDS.Redshift_Operations_Command.format(
                ODIN.Redshift_ODIN,
                f"truncate table {SAS_Config.test_temp_redshift_apollo_table}"
            ))
            # ... rest of the method using test tables
```

4. **Update Roll-up Logic**:
```python
def roll_up_metrics(self, distinct_managers):
    for manager in distinct_managers:
        insert_apollo_result = self.rs_upsert.execute_redshift_sql_query(
            SAS_Config.roll_upto_manager_query.format(
                SAS_Config.test_rolled_redshift_apollo_table,
                manager,
                SAS_Config.test_redshift_apollo_table
            )
        )
        # ... similar changes for other tables
```

This approach:
1. Creates separate test tables
2. Maintains data isolation
3. Allows safe testing
4. Preserves production data
5. Enables easy rollback

Remember to:
- Back up any important data
- Test the changes incrementally
- Verify data consistency
- Monitor performance
- Have rollback plans

Would you like me to provide the complete set of SQL statements for creating all the test tables?

------

venry: #######################################################################################
# Metric Provider : SAS Metrics Plug-in                                               #
# Plugin Uses SAS dashboard data for computing SAS Metrics                            #
# Output : Updates the RedShift Table and return the success status                    #
#######################################################################################

import re
import os
import sys
import time
import argparse
from datetime import date
from coral import coralrpc
import coral.complex_types
from retrying import retry
from pyodinhttp import odin_retrieve_pair
from dateutil.relativedelta import relativedelta
from aee_oe_data_collector_aggregator_config.config.sas_config import *
from aee_oe_data_collector_aggregator_config.config.logger import Logger
from com.amazonaws.sasriskaggregationservice.sasriskaggregationservice import *
from com.amazon.coral.availability.throttlingexception import ThrottlingException
from aee_oe_data_collector_aggregator.utilities.downloadUploadToS3 import S3_Operations
from aee_oe_data_collector_aggregator_config.config.basic_config import Common_Constants, ODIN, COMMANDS
from aee_oe_data_collector_aggregator.utilities.redshift_operation_wrapper import redshift_operation_wrapper


class SAS_Plugin:

	def __init__(self):
		Logger.printLogs("INFO","Triggering SAS Plugin")
		self.rs_upsert = redshift_operation_wrapper()

	def SAS_Init(Self):
		# Fetching access key and secret key from admin ODIN
		aws_access_key_id,aws_secret_access_key = odin_retrieve_pair(ODIN.AEE_ODIN)
		aws_access_key_id = aws_access_key_id.data.decode('utf-8').encode()
		aws_secret_access_key = aws_secret_access_key.data.decode('utf-8').encode()

		# Creating orchestrator for client
		_orchestrator = coralrpc.new_orchestrator(
						 endpoint=SAS_Config.endpoint,
						 aws_service=SAS_Config.service,
						 aws_region=SAS_Config.region,
						 signature_algorithm='v4',
						 timeout=30, # 30 seconds why? New service so we don't know about the latency
						 aws_access_key=aws_access_key_id,
						 aws_secret_key=aws_secret_access_key,
						 aws_security_token = SAS_Config.security_token)

		# Creating SAS risk client
		return SASRiskAggregationServiceClient(_orchestrator)

	def _string_to_camelcase(self, s):
		return re.sub(r'(?:^|_)(.)', lambda m: m.group(1).upper(), s)


	def coral_to_python(self, obj):
		if isinstance(obj, coral.complex_types.List):
			return [self.coral_to_python(el) for el in obj]
		if isinstance(obj, coral.complex_types.Structure):
			required_string = {}
			for propname in dir(obj):
				if propname.startswith('_'):
					continue
				required_string[self._string_to_camelcase(propname)] = self.coral_to_python(getattr(obj, propname))
			return required_string
		if isinstance(obj, coral.simple_types.String):
			return str(obj)
		if isinstance(obj, coral.simple_types.Boolean):
			return bool(obj)
		return obj

	def get_entities(self):
		'''
		This function fetches the client names from Config file and returns client names.
		'''
		select_result = self.rs_upsert.run_redshift_select_query(SAS_Config.get_distinct_leader)
		Logger.printLogs("DEBUG","Query result {}".format(select_result))
		if select_result == 1:
			Logger.printLogs("EXCEPTION","Query resulted in exception and returned {}".format(select_result))
			sys.exit(1)

		entities = []
		for clients in select_result:
			entities.append(clients[0])
		if len(entities) < 2:
			entities = str("('") + entities[0] + str("')")
		else:
			entities = tuple(entities)
		return entities

	def get_distinct_managers(self, entities):
		'''
		Based on the entities from get_entities function, this function fetches, all the employees under that login.
		'''
		try:
			Logger.printLogs("INFO", " Calling Manager To Manager Aggregate Mapping Query")
			manager_to_manager_aggregate_mapping = self.rs_upsert.run_redshift_select_query(SAS_Config.manager_to_manager_aggregate_mapping_query.format(entities))

			if manager_to_manager_aggregate_mapping == 1:
				Logger.printLogs("ERROR","Manager Aggregate relation Mapping query failed with Error code 1")
				sys.exit(1)

			if len(manager_to_manager_aggregate_mapping) < 1:
				Logger.printLogs("WARNING","manager aggregate relation mapping is empty")

			distinct_managers = set()
			manager_to_entity_mapping = {}
			for mapping in manager_to_manager_aggregate_mapping:
				roll_up_to_manager = mapping[0]
				manager = mapping[1]
				if roll_up_to_manager in entities:
					distinct_managers.add(manager)
					manager_to_entity_mapping.update({manager:roll_up_to_manager})
			return tuple(distinct_managers), manager_to_entity_mapping, manager_to_manager_aggregate_mapping

		except Exception as error:
			Logger.printLogs("[ ERROR ]","Getting distinct managers failed with error {}".format(error))
			sys.exit(1)

	def load_SAS_data_to_redshift(self, program_name):
		'''
		Truncate the rawdata tables and update the new rawdata in tables. 
		Input: program_name i.e. type of SAS riks
		'''
		if (program_name == SAS_Config.program_name_apollo):
			try:
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table {}".format(SAS_Config.test_temp_redshift_apollo_table)))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.test_temp_redshift_apollo_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table " + SAS_Config.test_redshift_apollo_table))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.test_redshift_apollo_table))
				os.system(COMMANDS.Redshift_Update_Comamnd.format(ODIN.Redshift_ODIN,"csv",SAS_Config.test_temp_redshift_apollo_table,Common_Constants.S3_Bucket+'/'+SAS_Config.sas_apollo_temp_s3_location))
				Logger.printLogs("INFO","Data updation to table '{}' completed successfully.".format(SAS_Config.test_temp_redshift_apollo_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,SAS_Config.query_for_getting_required_data.format(SAS_Config.test_redshift_apollo_table,SAS_Config.test_temp_redshift_apollo_table)))
				Logger.printLogs("INFO","Unique Data updation to table '{}' completed successfully.".format(SAS_Config.test_redshift_apollo_table))

			except Exception as error:
				Logger.printLogs("[ ERROR ]","Transferring Apollo data to redshift table failed with message : {}".format(error))
				sys.exit(1)

		elif (program_name == SAS_Config.program_name_brazil):
			try:
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table {}".format(SAS_Config.test_temp_redshift_brazil_table)))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.test_temp_redshift_brazil_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table " + SAS_Config.test_redshift_brazil_table))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.test_redshift_brazil_table))
				os.system(COMMANDS.Redshift_Update_Comamnd.format(ODIN.Redshift_ODIN,"csv",SAS_Config.test_temp_redshift_brazil_table,Common_Constants.S3_Bucket+'/'+SAS_Config.sas_brazil_temp_s3_location))
				Logger.printLogs("INFO","Data updation to table '{}' completed successfully.".format(SAS_Config.test_temp_redshift_brazil_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,SAS_Config.query_for_getting_required_data.format(SAS_Config.test_redshift_brazil_table,SAS_Config.test_temp_redshift_brazil_table)))
				Logger.printLogs("INFO","Unique Data updation to table '{}' completed successfully.".format(SAS_Config.test_redshift_brazil_table))

			except Exception as error:
				Logger.printLogs("[ ERROR ]","Transferring Brazil data to redshift table failed with message : {}".format(error))
				sys.exit(1)

		elif (program_name == SAS_Config.program_name_pipeline_targets):
			try:
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table {}".format(SAS_Config.test_temp_redshift_pipeline_target_table)))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.test_temp_redshift_pipeline_target_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table " + SAS_Config.test_redshift_pipeline_target_table))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.test_redshift_pipeline_target_table))
				os.system(COMMANDS.Redshift_Update_Comamnd.format(ODIN.Redshift_ODIN,"csv",SAS_Config.test_temp_redshift_pipeline_target_table,Common_Constants.S3_Bucket+'/'+SAS_Config.sas_pipeline_target_temp_s3_location))
				Logger.printLogs("INFO","Data updation to table '{}' completed successfully.".format(SAS_Config.test_temp_redshift_pipeline_target_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,SAS_Config.query_for_getting_required_data.format(SAS_Config.test_redshift_pipeline_target_table,SAS_Config.test_temp_redshift_pipeline_target_table)))
				Logger.printLogs("INFO","Unique Data updation to table '{}' completed successfully.".format(SAS_Config.test_redshift_pipeline_target_table))

			except Exception as error:
				Logger.printLogs("ERROR","Transferring Pipeline Target data to redshift table failed with message : {}".format(error))
				sys.exit(1)
		else:
			Logger.printLogs("ERROR","Wrong Program name passed")
			return

	def create_metrics(self, _client, metric_snapshot):
		'''
		Main function, responsible to start generating metrics.
		Input: 
		_client: Client of SAS API
		metric_snapshot: Type of metrics refresh i.e. WoW or MoM
		'''
		entities = self.get_entities()
		distinct_managers, manager_to_entity_mapping, manager_to_manager_aggregate_mapping =self.get_distinct_managers(entities)
		self.apollo_file = open(SAS_Config.sas_apollo_temp_file,"w")
		self.brazil_file = open(SAS_Config.sas_brazil_temp_file,"w")
		self.pipeline_file = open(SAS_Config.sas_pipeline_target_temp_file,"w")
		self.metric_snapshot = metric_snapshot
		Logger.printLogs("[INFO]","Executing it for all managers")
		iterator = 0
		for manager in distinct_managers:
			Logger.printLogs("[UPDATE]","{} Managers left to process".format(len(distinct_managers)-iterator))
			iterator = iterator+1

			data = self.call_get_high_level_resource_details(manager, _client)

			final_list_of_apollo_environments = []
			final_list_of_brazil_environments = []
			final_list_of_pipeline_targets = []

			try:
				apollo_result = self.coral_to_python(data)['HighLevelResourceDetailsByResourceTypeMap']['ApolloEnvironmentNameAndStage']
				final_list_of_apollo_environments = self.process_high_level_data(apollo_result, SAS_Config.program_name_apollo)
			except Exception as err:
				Logger.printLogs("[INFO]","No Apollo SAS risk found ! {}".format(err))
			try:
				brazil_data = self.coral_to_python(data)['HighLevelResourceDetailsByResourceTypeMap']['BrazilVersionSet']
				final_list_of_brazil_environments = self.process_high_level_data(brazil_data, SAS_Config.program_name_brazil)
			except Exception as err:
				Logger.printLogs("[INFO]","No Brazil version set SAS risk found ! {}".format(err))

			try:
				pipeline_target_data = self.coral_to_python(data)['HighLevelResourceDetailsByResourceTypeMap']['AmazonPipelineTarget']
				final_list_of_pipeline_targets = self.process_high_level_data(pipeline_target_data, SAS_Config.program_name_pipeline_targets)
			except Exception as err:
				Logger.printLogs("[INFO]","No Pipeline target SAS risk found ! {}".format(err))

			try:

				self.call_batch_details_wrapper(manager, _client, final_list_of_apollo_environments, SAS_Config.program_name_apollo)

				self.call_batch_details_wrapper(manager, _client, final_list_of_brazil_environments, SAS_Config.program_name_brazil)

				self.call_batch_details_wrapper(manager,_client,final_list_of_pipeline_targets, SAS_Config.program_name_pipeline_targets)


			except Exception as error:
				Logger.printLogs("ERROR","Getting final details of SAS failed with error {}".format(error))
				sys.exit(1)

		self.apollo_file.close()
		self.brazil_file.close()
		self.pipeline_file.close()

		try:
			S3_Operations.uploadDataToS3(ODIN.S3_ODIN, Common_Constants.S3_Bucket, SAS_Config.sas_apollo_temp_s3_location, SAS_Config.sas_apollo_temp_file)
			self.load_SAS_data_to_redshift(SAS_Config.program_name_apollo)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred while uploading Apollo csv file to S3 bucket. {}".format(err))
			sys.exit(1)

		try:
			S3_Operations.uploadDataToS3(ODIN.S3_ODIN, Common_Constants.S3_Bucket, SAS_Config.sas_brazil_temp_s3_location, SAS_Config.sas_brazil_temp_file)
			self.load_SAS_data_to_redshift(SAS_Config.program_name_brazil)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred while uploading Brazil csv file to S3 bucket. {}".format(err))
			sys.exit(1)

		try:
			S3_Operations.uploadDataToS3(ODIN.S3_ODIN, Common_Constants.S3_Bucket, SAS_Config.sas_pipeline_target_temp_s3_location, SAS_Config.sas_pipeline_target_temp_file)
			self.load_SAS_data_to_redshift(SAS_Config.program_name_pipeline_targets)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred while uploading Pipeline Target csv file to S3 bucket. {}".format(err))
			sys.exit(1)

		try:

			Logger.printLogs("INFO","Starting Rolling up of SAS metrics")
			roll_return_code = self.roll_up_metrics(distinct_managers)

			Logger.printLogs("INFO","Starting Aggregating SAS metrics")
			self.aggregate_metrics(distinct_managers)

			Logger.printLogs("INFO","Updating refresh status of SAS metrics")
			self.insert_success_refresh_status()

			Logger.printLogs("INFO","Updating duplicate tables of SAS metrics for Manual update of OE Risks")
			self.insert_sas_data_into_duplicate_table()

		except Exception as err:
			Logger.printLogs("ERROR","SAS Plug-in failed with error {}".format(err))
			sys.exit(1)

	def process_high_level_data(self, result, resource_type):
		'''
		Function is responsible to get all environments or resources from result and return consolidated list.
		Input: Result, metric_snapshot and type of resource
		Output: Consolidated list of Environments
		'''
		final_list_environments = []
		for environment in result:
			environment_json_info = self.coral_to_python(environment)
			env_name = environment_json_info['ResourceName']
			Logger.printLogs("[DEBUG]","{} environment {} added to list".format(resource_type, env_name))
			final_list_environments.append(env_name)

		return final_list_environments

	def get_apollo_data_and_write_in_file(self, data, file, metric_snapshot):
		'''
		Fetches all the details from processed Apollo data and writes into a csv file.
		'''
		list_data = self.coral_to_python(data)
		for filtered_depth_data in list_data['Details']:
			env_name = filtered_depth_data['Name']
			stage_name = env_name.split(':')[1]
			owner_login = filtered_depth_data['OwnerLogin']
			children_count = filtered_depth_data['ChildrenCount']
			consumer_count = filtered_depth_data['ConsumerCount']
			host_count = filtered_depth_data['HostCount']
			owner = filtered_depth_data['Owner']
			pipeline_id = filtered_depth_data['PipelineId']
			pipeline_name = filtered_depth_data['PipelineName']
			oldest_risk_age = filtered_depth_data['OldestRiskAge']
			risk_count = filtered_depth_data['RiskCount']
			visibility = filtered_depth_data['Visibility']
			recommendation_with_action = {}
			for recommendation in filtered_depth_data['Recommendations']:
				for target_resource in recommendation['AssociatedTargetResources']:
					recommendation_with_action[target_resource] = recommendation['Template']

			for software_risk in filtered_depth_data['SoftwareRisks']:
				for risk_details in software_risk['SoftwareRiskDetails']:
					blocked_resources = risk_details['TargetResource']
					action_required = recommendation_with_action[blocked_resources]
					associated_campaigns = software_risk['AssociatedCampaign']['Name'].replace(",","")

					oldest_risk_age = risk_details['RiskAge']
					oldest_recommendation_age = str(risk_details['RecommendationAge'])

					if (not oldest_recommendation_age.isnumeric()):
						oldest_recommendation_age = '-1'
					
					severity = "HIGH"
					risk_priority = 1

					if int(oldest_risk_age) > 30:
						severity = "EMERGENCY"
						risk_priority = 3
					elif int(oldest_risk_age) > 20:
						severity = "CRITICAL"
						risk_priority = 2

					current_date = date.today()
					snapshot_number = current_date.isocalendar()[1]
					classification = 'None'
					curent_year = current_date.strftime('%Y')
					file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(env_name,
																										 stage_name,
																										 pipeline_id,
																										 pipeline_name,
																										 owner,
																										 owner_login,
																										 consumer_count,
																										 children_count,
																										 risk_count,
																										 visibility,
																										 host_count,
																										 oldest_risk_age,
																										 oldest_recommendation_age,
																										 action_required,
																										 blocked_resources,
																										 associated_campaigns,
																										 current_date,
																										 curent_year,
																										 classification,
																										 metric_snapshot,
																										 snapshot_number,
																										 severity,
																										 risk_priority))

	def get_brazil_data_and_write_in_file(self, data, file, metric_snapshot):
		'''
		Fetches all the details from processed Brazil data and writes into a csv file.
		'''
		list_data = self.coral_to_python(data)
		for version_set_json_info in list_data['Details']:
			version_set = version_set_json_info['Name']
			owner_login = version_set_json_info['OwnerLogin']
			owner = version_set_json_info['Owner']

			pipeline_id = version_set_json_info['PipelineId']
			pipeline_name = version_set_json_info['PipelineName']
			oldest_recommendation_age = version_set_json_info['OldestRecommendationAge']
			oldest_risk_age = version_set_json_info['OldestRiskAge']
			risk_count = version_set_json_info['RiskCount']

			recommendation_with_action = {}
			for recommendation in version_set_json_info['Recommendations']:
				for target_resource in recommendation['Actions']:
					recommendation_with_action[recommendation['TargetResource']] = target_resource['Template']

			for software_risk in version_set_json_info['SoftwareRisks']:
				for risk_details in software_risk['SoftwareRiskDetails']:
					blocked_resources = risk_details['TargetResource']
					try:
						action_required = recommendation_with_action[blocked_resources]
					except Exception as err:
						action_required = 'None'
					associated_campaigns = software_risk['AssociatedCampaign']['Name']
					oldest_risk_age = risk_details['RiskAge']
					oldest_recommendation_age = str(risk_details['RecommendationAge'])

					if (not oldest_recommendation_age.isnumeric()):
						oldest_recommendation_age = '-1'

					severity = "HIGH"
					risk_priority = 1
					if int(oldest_risk_age) > 30:
						severity = "EMERGENCY"
						risk_priority = 3
					elif int(oldest_risk_age) > 20:
						severity = "CRITICAL"
						risk_priority = 2

					current_date = date.today()
					snapshot_number = current_date.isocalendar()[1]
					classification = 'None'
					curent_year = current_date.strftime('%Y')
					file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(version_set,
																						  pipeline_id,
																						  pipeline_name,
																						  owner,
																						  owner_login,
																						  risk_count,
																						  oldest_risk_age,
																						  oldest_recommendation_age,
																						  blocked_resources,
																						  action_required,
																						  associated_campaigns,
																						  current_date,
																						  curent_year,
																						  classification,
																						  metric_snapshot,
																						  snapshot_number,
																						  severity,
																						  risk_priority))

	def get_pipeline_target_data_and_write_in_file(self, data, file, metric_snapshot):
		'''
		Fetches all the details from processed pipeline target data and writes into a csv file.
		'''
		list_data = self.coral_to_python(data)
		for pipeline_target_json_info in list_data['Details']:
			target_name = pipeline_target_json_info['Name']
			pipeline_name = pipeline_target_json_info['PipelineName']
			pipeline_id = pipeline_target_json_info['PipelineId']
			owner_name = pipeline_target_json_info['Owner']
			owner_login = pipeline_target_json_info['OwnerLogin']
			oldest_risk_age = pipeline_target_json_info['OldestRiskAge']
			risk_count = pipeline_target_json_info['RiskCount']

			recommendation_with_action = {}

			try:
				for recommendation in pipeline_target_json_info['Recommendations']:
					for target_resource in recommendation['Actions']:
						recommendation_with_action[recommendation['TargetResource']] = target_resource['Template']
			except Exception as err:
				Logger.printLogs("INFO", "Please Refer to Associated campaign urls to follow the recommendation")

			for software_risk in pipeline_target_json_info['SoftwareRisks']:
				for risk_details in software_risk['SoftwareRiskDetails']:
					blocked_resources = risk_details['TargetResource']
				try:
					action_required = recommendation_with_action[blocked_resources]
				except Exception as err:
					action_required = 'None'
				associated_campaigns = software_risk['AssociatedCampaign']['Name']
				oldest_risk_age = risk_details['RiskAge']

				try:
					oldest_recommendations_age = str(risk_details['RecommendationAge'])
				except Exception:
					Logger.printLogs("INFO","Recommendation Age is not configured in Pipeline Target plug-in, setting default value")
					oldest_recommendations_age = '-1'

				severity = "HIGH"
				risk_priority = 1

				if int(oldest_risk_age) > 30:
					severity = "EMERGENCY"
					risk_priority = 3
				elif int(oldest_risk_age) > 20:
					severity = "CRITICAL"
					risk_priority = 2

				current_date = date.today()
				snapshot_number = current_date.isocalendar()[1]
				classification = 'None'
				current_year = current_date.strftime('%Y')

				file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(target_name,
																					  pipeline_id,
																					  pipeline_name,
																					  owner_name,
																					  owner_login,
																					  risk_count,
																					  oldest_risk_age,
																					  oldest_recommendations_age,
																					  blocked_resources,
																					  action_required,
																					  associated_campaigns,
																					  current_date,
																					  current_year,
																					  classification,
																					  metric_snapshot,
																					  snapshot_number,
																					  severity,
																					  risk_priority
																					  ))

	@retry(wait_fixed=10000, stop_max_attempt_number=3)
	def call_get_high_level_resource_details(self, manager, _client):
		'''
		Makes API call to get the high level SAS risk details for any particular manager. It includes all Blocked SAS risks.
		'''
		try:
			data = _client.get_resource_high_level_risk_details_by_campaign_type(aggregate_id = "Person:"+manager, campaign_type='BLOCKED')
			severity_data = self.get_severity_based_risks(data)
			data.update({'severity_info': severity_data})

		except ThrottlingException as e:
			Logger.printLogs("EXCEPTION","ThrottlingException occurred, Sleeping! {}".format(e))
			time.sleep(5)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred {}".format(err))
			return 1

		return data
	
	def get_severity_based_risks(self, high_level_data):
		'''
		Process severity information from RAGS API response
		'''
		try:
			severity_counts = {
				"EMERGENCY": high_level_data.riskSeverityMapWithRecommendations.get("Critical",0),
				"CRITICAL": high_level_data.riskSeverityMapWithRecommendations.get("High", 0),
				"HIGH": high_level_data.riskSeverityMapWithRecommendations.get("Medium", 0)
			}
			Logger.printLogs("[DEBUG]", "Severity Counts: {}".format(severity_counts))
			return severity_counts
		except Exception as err:
			Logger.printLogs("[ERROR]", "Error processing severity {}".format(err))
			return 1
	
	def call_batch_details_wrapper(self, manager, _client, final_list_of_environments, type_of_env):
		'''
		This is the wrapper function, which is responsible to limit the batch size to 100 of every request.
		'''
		batch_iterations = len(final_list_of_environments)// SAS_Config.BATCH_SIZE +1
		Logger.printLogs("[INFO]","Inside Call Batch Wrapper function: Total Batches required {}".format(batch_iterations))

		try:
			for start_range in range(0,len(final_list_of_environments),SAS_Config.BATCH_SIZE):
				end_range = start_range + SAS_Config.BATCH_SIZE

				consolidated_data = self.call_batch_details(manager, _client, final_list_of_environments[start_range:end_range], type_of_env)

				if type_of_env == SAS_Config.program_name_apollo and consolidated_data != 1:
					self.get_apollo_data_and_write_in_file(consolidated_data, self.apollo_file, self.metric_snapshot)
				elif type_of_env == SAS_Config.program_name_brazil and consolidated_data != 1:
					self.get_brazil_data_and_write_in_file(consolidated_data, self.brazil_file, self.metric_snapshot)
				elif type_of_env == SAS_Config.program_name_pipeline_targets and consolidated_data != 1:
					self.get_pipeline_target_data_and_write_in_file(consolidated_data, self.pipeline_file, self.metric_snapshot)

		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred {}".format(err))
			sys.exit(1)

		return


	@retry(wait_fixed=10000, stop_max_attempt_number=3)
	def call_batch_details(self, manager, _client, final_list_of_environments, type_of_env):
		'''
		This function takes list of environments as input and fetches all detailed information.
		'''
		Logger.printLogs("[DEBUG]","Batch Length is {}".format(len(final_list_of_environments)))
		try:
			if (type_of_env == SAS_Config.program_name_apollo):
				data = _client.batch_get_apollo_environment_stage_details(requester_login = manager, entity_id_list=final_list_of_environments, campaign_type='BLOCKED')
			elif (type_of_env == SAS_Config.program_name_brazil):
				data = _client.batch_get_brazil_version_set_details(requester_login = manager, entity_id_list=final_list_of_environments, campaign_type='BLOCKED')
			elif (type_of_env == SAS_Config.program_name_pipeline_targets):
				data = _client.batch_get_pipeline_target_resource_details(requester_login = manager, entity_id_list = final_list_of_environments, campaign_type = 'BLOCKED')
			else:
				Logger.printLogs("ERROR","Wrong type_of_env is passed, Please check! ")

		except ThrottlingException as error:
			Logger.printLogs("EXCEPTION","ThrottlingException occurred, Sleeping! {}".format(error))
			time.sleep(5)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred {}".format(err))
			return 1

		return data

	def roll_up_metrics(self, distinct_managers):
		'''
		Function responsible for Rolling up the metrics by using Rawdata tables.
		'''
		Logger.printLogs("[INFO]","Starting rolling up of SAS metrics")

		for manager in distinct_managers:


			insert_apollo_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.roll_upto_manager_query.format(SAS_Config.test_rolled_redshift_apollo_table,
																										  manager,
																										  SAS_Config.test_redshift_apollo_table))
			Logger.printLogs("[DEBUG]","Apollo Data rolled upto query for {} manager result {}".format(manager, insert_apollo_result))


			insert_brazil_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.roll_upto_manager_query.format(SAS_Config.test_rolled_redshift_brazil_table,
																										  manager,
																										  SAS_Config.test_redshift_brazil_table))
			Logger.printLogs("[DEBUG]","Brazil Data rolled upto query for {} manager result {}".format(manager, insert_brazil_result))


			insert_pipeline_target_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.roll_upto_manager_query.format(SAS_Config.test_rolled_redshift_pipeline_target_table,
																										  manager,
																										  SAS_Config.test_redshift_pipeline_target_table))
			Logger.printLogs("[DEBUG]","Pipeline Target Data rolled upto query for {} manager result {}".format(manager, insert_pipeline_target_result))

			if (insert_apollo_result != 0 or insert_brazil_result != 0 or insert_pipeline_target_result != 0):
				Logger.printLogs("ERROR","Query failed to roll the data for manager {}".format(manager))
				sys.exit(1)



	def aggregate_metrics(self, distinct_managers):
		'''
		This function creates Aggregate metrics from the Rolled up data
		'''
		Logger.printLogs("[INFO]","Starting Aggregate Rolled up of SAS metrics")
		manager_processed = 0
		for manager in distinct_managers:

			manager_processed = manager_processed + 1

			Logger.printLogs("[DEBUG]","Manager processed [{}]/[{}]".format(manager_processed,len(distinct_managers)))


			insert_total_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_total_query.format(SAS_Config.test_sas_risk_summary, manager))
			Logger.printLogs("[DEBUG]","Total Metrics rolled upto {} manager query result {}".format(manager, insert_total_result))

			severity_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.severity_aggregate_query.format(SAS_Config.test_sas_risk_summary, SAS_Config.test_redshift_apollo_table, SAS_Config.test_redshift_brazil_table,SAS_Config.test_redshift_pipeline_target_table), (manager, date.today(),self.metric_snapshot, self.get_week_number(),manager,date.today().year,'severity_based'))
			Logger.printLogs("[DEBUG]", "Severity Metrics rolled upto {} manager query result {}".format(manager, severity_result))

			insert_recommended_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_recommendation_query.format(SAS_Config.test_sas_risk_summary, manager))
			Logger.printLogs("[DEBUG]","Recommendations Metrics rolled upto {} manager query result {}".format(manager, insert_recommended_result))


			insert_older_recommendation_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_long_recommendation_query.format(SAS_Config.test_sas_risk_summary, manager))
			Logger.printLogs("[DEBUG]","Recommendations Metrics rolled upto {} manager query result {}".format(manager, insert_older_recommendation_result))

			insert_recommended_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_near_sla_query.format(SAS_Config.test_sas_risk_summary, manager))
			Logger.printLogs("[DEBUG]","SLA near missed Metrics rolled upto {} manager query result {}".format(manager, insert_recommended_result))

			insert_sla_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_sla_query.format(SAS_Config.test_sas_risk_summary, manager))
			Logger.printLogs("[DEBUG]", "SLA missed Metrics rolled upto {} manager query result {}".format(manager, insert_sla_result))
			
			if (insert_total_result !=0 or insert_recommended_result != 0 or insert_older_recommendation_result != 0 or insert_sla_result != 0 or severity_result != 0):
				Logger.printLogs("ERROR","Aggregation Query Execution failed for manager {}".format(manager))
				sys.exit(1)



	def insert_success_refresh_status(self):
		'''
		Updates the refresh status when plug-in runs successfullly.
		'''

		try:
			next_run_date = date.today() + relativedelta(days=7)
			insert_status = self.rs_upsert.execute_redshift_sql_query(SAS_Config.insert_status_of_sas_run.format(date.today(), date.today(),next_run_date))
			Logger.printLogs("DEBUG","Status Insertion Query result {}".format(insert_status))

		except Exception as error:
			Logger.printLogs("ERROR","Inserting refresh status failed with message: {}".format(error))
			sys.exit(1)

	def insert_sas_data_into_duplicate_table(self):
		'''
        Inserting data into respective SAS tables for Manual OE risk update in Amazon Arik portal.
        '''
		try:
			#Adding SAS Apollo data to duplicate table for Manual update of SAS Risks [AmazonArik OE Risk]
			insert_status_apollo = self.rs_upsert.execute_redshift_sql_query(SAS_Config.sas_temp_apollo_insert_query)
			Logger.printLogs("INFO","Data update to duplicate apollo sas table '{}' completed successfully {}.".format(SAS_Config.test_redshift_apollo_table_v2, insert_status_apollo))

			#Adding SAS Brazil data to duplicate table for Manual update of SAS Risks [AmazonArik OE Risk]
			insert_status_brazil = self.rs_upsert.execute_redshift_sql_query(SAS_Config.sas_temp_brazil_insert_query)
			Logger.printLogs("INFO","Data update to duplicate brazil sas table '{}' completed successfully {}.".format(SAS_Config.test_redshift_brazil_table_v2, insert_status_brazil))

			#Adding SAS Pipeline Target data to duplicate table for Manual update of SAS Risks [AmazonArik OE Risk]
			insert_status_pipeline_target = self.rs_upsert.execute_redshift_sql_query(SAS_Config.sas_temp_pipeline_target_insert_query)
			Logger.printLogs("INFO","Data update to duplicate Pipeline target sas table '{}' completed successfully {}.".format(SAS_Config.test_redshift_pipeline_target_table_v2, insert_status_pipeline_target))

		except Exception as error:
			Logger.printLogs("ERROR","Inserting into duplicate table status failed with message: {}".format(error))
			sys.exit(1)

def main():
	obj = SAS_Plugin()
	_client = obj.SAS_Init()
	parser = argparse.ArgumentParser(description='Getting the Snapshot type of SAS Plug-in')
	parser.add_argument('-s', '--snapshot',  nargs="*", help='Plugin Snapshot Type: WoW / MoM', required=True)
	args = vars(parser.parse_args())
	metric_snapshot = args['snapshot'][0]
	obj.create_metrics(_client, metric_snapshot)

if __name__ == "__main__":
	main()


class SAS_Config:

	region = "us-west-2"

	security_token = None

	service = "sas-rags-frontend-prod"

	endpoint = "https://sas-rags-frontend-prod-global-pdx.pdx.proxy.amazon.com"
	
	severity_emergency = 'EMERGENCY'

	severity_critical = 'CRITICAL'
	
	severity_high = 'HIGH'

	test_redshift_apollo_table = "aeetest.raw_sas_apollo_metrics"
	test_redshift_brazil_table = "aeetest.raw_sas_brazil_metrics"
	test_redshift_pipeline_target_table = "aeetest.raw_sas_pipeline_target_metrics"

	test_redshift_apollo_table_v2 = "aeetest.raw_sas_apollo_metrics_v2"
	test_redshift_brazil_table_v2 = "aeetest.raw_sas_brazil_metrics_v2"
	test_redshift_pipeline_target_table_v2 = "aeetest.raw_sas_pipeline_target_metrics_v2"

	test_temp_redshift_apollo_table = "aeetest.temp_sas_apollo_metrics"
	test_temp_redshift_brazil_table = "aeetest.temp_sas_brazil_metrics"
	test_temp_redshift_pipeline_target_table = "aeetest.temp_sas_pipeline_target_metrics"

	test_rolled_redshift_apollo_table = "aeetest.rolled_sas_apollo_metrics"
	test_rolled_redshift_brazil_table = "aeetest.rolled_sas_brazil_metrics"
	test_rolled_redshift_pipeline_target_table = "aeetest.rolled_sas_pipeline_target_metrics"

	test_sas_risk_summary = "aeetest.sas_risk_summary"

	severity_update_query = """update {} set severity = %s, risk_priority = %s where apollo_environment_name = %s and metric_run_date = %s"""
	severity_aggregate_query = """
	insert into {} (
		risk_count,
		metric_rolled_up_to,
		metric_run_date,
		metric_snapshot,
		metric_snapshot_number,
		aggregate_roll_up_to,
		metric_year,
		metric_type,
		severity
	)
	select
		count(*) as risk_count,
		%s as metric_roll_up_to,
		%s as metric_run_date,
		%s as metric_snapshot,
		%s as metric_snapshot_number,
		%s as aggregate_roll_up_to,
		%s as metric_year,
		%s as metric_type,
		severity
	from (
		select severity, metric_run_date from {}
		union all
		select severity, metric_run_date from {}
		union all
		select severity, metric_run_date from {}
	) as combined_risks
	where metric_run_date = %s
	group by severity
	"""
	sas_aggregate_table = "aggregate_schema.sas_risk_summary"

	sas_apollo_temp_s3_location = "P0_metrics_raw_data/SAS/SAS_Apollo_File.csv"

	sas_brazil_temp_s3_location = "P0_metrics_raw_data/SAS/SAS_Brazil_File.csv"

	sas_pipeline_target_temp_s3_location = "P0_metrics_raw_data/SAS/SAS_Pipeline_Target_File.csv"

	sas_apollo_temp_file = "/apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Apollo_File.csv"

	sas_brazil_temp_file = "/apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Brazil_File.csv"

	sas_pipeline_target_temp_file = "/apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Pipeline_Target_File.csv"

	sas_status_query = """insert into rawdata_schema.page0_plugin_run_status values('{}','{}','{}','{}','{}','{}','{}','{}','{}','{}')"""

	# redshift_apollo_table = "rawdata_schema.raw_sas_apollo_metrics"

	# redshift_brazil_table = "rawdata_schema.raw_sas_brazil_metrics"

	# redshift_pipeline_target_table = "rawdata_schema.raw_sas_pipeline_target_metrics"

	# redshift_apollo_table_v2 = "rawdata_schema.sas_apollo_metrics_v2"

	# redshift_brazil_table_v2 = "rawdata_schema.sas_brazil_metrics_v2"

	# redshift_pipeline_target_table_v2 = "rawdata_schema.sas_pipeline_target_metrics_v2"

	# temp_redshift_apollo_table = "rawdata_schema.temp_sas_apollo_metrics"

	# temp_redshift_brazil_table = "rawdata_schema.temp_sas_brazil_metrics"

	# temp_redshift_pipeline_target_table = "rawdata_schema.temp_sas_pipeline_target_metrics"

	# rolled_redshift_apollo_table = "rawdata_schema.sas_apollo_metrics"

	# rolled_redshift_brazil_table = "rawdata_schema.sas_brazil_metrics"

	# rolled_redshift_pipeline_target_table = "rawdata_schema.sas_pipeline_target_metrics"

	program_name_apollo = 'APOLLO'

	program_name_brazil = 'BRAZIL'

	program_name_pipeline_targets = 'PIPELINE_TARGET'

	pipeline_target_function_flag = 'PIPELINE_TARGET_FUNCTION'

	BATCH_SIZE = 100

	query_for_getting_required_data = """insert into {} select distinct * from {}"""

	get_distinct_leader = "select distinct leader from relationship_schema.p0_clients_primary"

	insert_status_of_sas_run = """ 
	insert into aeetest.page0_plugin_run_status 
	values('ALLORG','ALLManager','','SAS','SAS Risks',0,'{}','{}','Weekly','{}');
	"""

	manager_to_manager_aggregate_mapping_query = """
	select relation_entity,relation_manager FROM 
	relationship_schema.entity_manager_relationship WHERE relation_entity IN (SELECT DISTINCT relation_manager FROM 
	relationship_schema.entity_manager_relationship WHERE relation_entity IN {0}) and 
	relation_rundate in (select max(relation_rundate) from relationship_schema.entity_manager_relationship)"""

	roll_upto_manager_query = """
	insert into {0}
	select *,
		   '{1}' AS rolled_upto
	FROM {2}
	WHERE owner_login IN (SELECT DISTINCT login_name
					  FROM BOOKER.D_EMPLOYEE_DETAILS
					  WHERE supervisor_login_name IN (SELECT DISTINCT relation_manager
													  FROM relationship_schema.entity_manager_relationship
													  WHERE relation_entity = '{1}'))
	UNION all
	SELECT *,
		   '{1}' AS rolled_upto
	FROM {2}
	WHERE owner_login = '{1}'
	"""

	aggregate_roll_upto_total_query ="""
	insert into {0} (
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'total' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from aeetest.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from aeetest.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_brazil_metrics) and aggregate_roll_up_to in
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))
	union all
	select pipeline_target_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,pipeline_target_action_type as action_type
	from aeetest.sas_pipeline_target_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_pipeline_target_metrics) and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}')))
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_rolled_upto,metric_year,metric_snapshot_number
	union all
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'total' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from aeetest.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from aeetest.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_brazil_metrics) and aggregate_roll_up_to in ('{1}')
	union all
	select pipeline_target_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,pipeline_target_action_type as action_type
	from aeetest.sas_pipeline_target_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_pipeline_target_metrics) and aggregate_roll_up_to in ('{1}'))
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_roll_up_to,metric_year,metric_snapshot_number)
	"""

	aggregate_roll_upto_near_sla_query = """
	insert into {0} (
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'75DaySASRisk' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from aeetest.sas_apollo_metrics where metric_run_date = (select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))  and 
		cast(days_non_compliant_recommendation as int) >= 20 and 
		cast(days_non_compliant_recommendation as int) < 30
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from aeetest.sas_brazil_metrics where metric_run_date = (select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}')) and 
		cast(days_non_compliant_recommendation as int) >= 20 and 
		cast(days_non_compliant_recommendation as int) < 30)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_rolled_upto,metric_year,metric_snapshot_number
	union all
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'75DaySASRisk' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from aeetest.sas_apollo_metrics where metric_run_date = (select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) >= 20 and 
		cast(days_non_compliant_recommendation as int) < 30
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from aeetest.sas_brazil_metrics where metric_run_date = (select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) >= 20 and 
		cast(days_non_compliant_recommendation as int) < 30)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_roll_up_to,metric_year,metric_snapshot_number)
	"""

	aggregate_roll_upto_recommendation_query = """
	insert into {0} (
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'recommendations' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from aeetest.sas_apollo_metrics where metric_run_date = (select max(metric_run_date) from aeetest.sas_apollo_metrics)
	and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))  and 
		cast(days_non_compliant_recommendation as int) > 0
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from aeetest.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}')) and 
		cast(days_non_compliant_recommendation as int) > 0)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_rolled_upto,metric_year,metric_snapshot_number
	union all
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'recommendations' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from aeetest.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) > 0
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from aeetest.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) > 0)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_roll_up_to,metric_year,metric_snapshot_number)
	"""

	aggregate_roll_upto_long_recommendation_query = """
	insert into {0} (
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'90DaySASRisk' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from aeetest.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))  and 
		cast(days_non_compliant_recommendation as int) > 30
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from aeetest.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_brazil_metrics) and aggregate_roll_up_to in
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}')) and 
		cast(days_non_compliant_recommendation as int) > 30)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_rolled_upto,metric_year,metric_snapshot_number
	union all
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'90DaySASRisk' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from aeetest.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) > 30
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from aeetest.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_brazil_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) > 30)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_roll_up_to,metric_year,metric_snapshot_number)
	"""

	'''
	Insert query to copy retrieve data from parent table to new table with Max(rundate)
	for Manual update of OE Risks by SE/Dev in Arik portal project.

	Master ticket - https://issues.amazon.com/issues/AEESE-3612
	'''

	sas_temp_apollo_insert_query = """
    INSERT INTO aeetest.sas_apollo_metrics_v2 (apollo_environment_name, apollo_environment_stage, pipeline_id,
    pipeline_name, owner, owner_login, consumers, children, software_count, visibility, host_count, days_non_compliant,
    days_non_compliant_recommendation, apollo_action, blocked_resources, associated_campaigns, metric_run_date, metric_year,
    classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to, severity, risk_priority) 
    SELECT apollo_environment_name, apollo_environment_stage, pipeline_id, pipeline_name, owner, owner_login, consumers,
    children, software_count, visibility, host_count, days_non_compliant, days_non_compliant_recommendation, apollo_action,
    blocked_resources, associated_campaigns, metric_run_date, metric_year, classification, metric_snapshot, metric_snapshot_number,
    aggregate_roll_up_to,severity, risk_priority FROM aeetest.sas_apollo_metrics
    WHERE metric_run_date = (SELECT MAX(metric_run_date) FROM aeetest.sas_apollo_metrics)
    """

	sas_temp_brazil_insert_query = """
	INSERT INTO aeetest.sas_brazil_metrics_v2 (brazil_versionset_name, pipeline_id, pipeline_name, owner, owner_login,
	software_count, days_non_compliant, days_non_compliant_recommendation, blocked_resource, brazil_action_type, associated_campaigns,
	metric_run_date, metric_year, classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to,severity, risk_priority) 
    SELECT brazil_versionset_name, pipeline_id, pipeline_name, owner, owner_login, software_count, days_non_compliant,
    days_non_compliant_recommendation, blocked_resource, brazil_action_type, associated_campaigns, metric_run_date, metric_year,
    classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to,severity, risk_priority FROM aeetest.sas_brazil_metrics
    WHERE metric_run_date = (SELECT MAX(metric_run_date) FROM aeetest.sas_brazil_metrics)
    """

	sas_temp_pipeline_target_insert_query = """
	INSERT INTO aeetest.sas_pipeline_target_metrics_v2 (pipeline_target_name, pipeline_id, pipeline_name, owner, owner_login,
	software_count, days_non_compliant, days_non_compliant_recommendation, blocked_resource, pipeline_target_action_type, associated_campaigns,
	metric_run_date, metric_year, classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to,severity, risk_priority) 
    SELECT pipeline_target_name, pipeline_id, pipeline_name, owner, owner_login, software_count, days_non_compliant,
    days_non_compliant_recommendation, blocked_resource, pipeline_target_action_type, associated_campaigns, metric_run_date, metric_year,
    classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to,severity, risk_priority FROM aeetest.sas_pipeline_target_metrics
    WHERE metric_run_date = (SELECT MAX(metric_run_date) FROM aeetest.sas_pipeline_target_metrics)
    """

made all the changes, would it run perfectly?

------

Cedric: Looking at your changes, there are a few points to check/fix:

1. **File Writing Format** - There's a mismatch in the format string and parameters:
```python
# In get_apollo_data_and_write_in_file
file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(...))
# You're providing 21 placeholders but including 22 values (severity and risk_priority)

# Fix:
file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(...))
```

2. **Severity Query Parameter** - In aggregate_metrics:
```python
severity_result = self.rs_upsert.execute_redshift_sql_query(
    SAS_Config.severity_aggregate_query.format(
        SAS_Config.test_sas_risk_summary,
        SAS_Config.test_redshift_apollo_table,
        SAS_Config.test_redshift_brazil_table,
        SAS_Config.test_redshift_pipeline_target_table
    ),
    (
        manager,                    # metric_rolled_up_to
        date.today(),              # metric_run_date
        self.metric_snapshot,      # metric_snapshot (changed from WoW)
        self.get_week_number(),    # metric_snapshot_number
        manager,                   # aggregate_roll_up_to
        date.today().year,         # metric_year
        'severity_based'           # metric_type
    )
)
```

3. **Table Creation** - Make sure you've created all test tables with severity columns:
```sql
-- Create test tables SQL script
CREATE TABLE aeetest.raw_sas_apollo_metrics (
    -- existing columns...,
    severity VARCHAR(10),
    risk_priority INT
);

-- Repeat for all test tables:
-- aeetest.raw_sas_brazil_metrics
-- aeetest.raw_sas_pipeline_target_metrics
-- aeetest.sas_apollo_metrics_v2
-- etc...
```

4. **Missing aggregate_roll_upto_sla_query** - You reference this in aggregate_metrics but it's not defined in SAS_Config:
```python
# Add to SAS_Config
aggregate_roll_upto_sla_query = """..."""
```

5. **Test Tables Path** - In insert_sas_data_into_duplicate_table:
```python
# Update log message to use correct test table name
Logger.printLogs("INFO", "Data update to duplicate apollo sas table '{}' completed successfully {}.".format(
    SAS_Config.test_redshift_apollo_table_v2,  # Changed from redshift_apollo_table_v2
    insert_status_apollo
))
```

To ensure it runs perfectly:

1. **Create Test Tables**:
```sql
-- Run these SQL commands to create test tables
CREATE SCHEMA IF NOT EXISTS aeetest;

-- Create all required test tables with severity columns
CREATE TABLE aeetest.raw_sas_apollo_metrics (...);
CREATE TABLE aeetest.raw_sas_brazil_metrics (...);
-- ... create all other test tables
```

2. **Verify Table Access**:
```sql
-- Verify access to test tables
SELECT * FROM aeetest.raw_sas_apollo_metrics LIMIT 1;
```

3. **Test Run With Small Dataset**:
```python
# Run with a single manager first
test_manager = "test_manager"
data = obj.call_get_high_level_resource_details(test_manager, _client)
```

4. **Monitor Logs**:
```python
# Add more debug logging
Logger.printLogs("DEBUG", "Severity data: {}".format(severity_data))
```

Would you like me to provide the complete SQL script for creating all test tables or focus on any specific part?

------

venry: what's the use of aggregate_roll_upto_sla_query

------

Cedric: The `aggregate_roll_upto_sla_query` would be used to track risks that have missed their SLA (Service Level Agreement). Based on the other queries in your code, it should follow a similar pattern but focus on risks that have exceeded their SLA thresholds.

Here's how it should be defined:

```python
aggregate_roll_upto_sla_query = """
insert into {0} (
    select count(risk_name) as risk_count,
    aggregate_roll_up_to,
    metric_run_date,
    metric_snapshot,
    metric_snapshot_number,
    '{1}' as aggregate_rolled_upto,
    metric_year,
    'SLAMissed' as metric_type 
    from (
        select apollo_environment_name as risk_name,
        aggregate_roll_up_to,
        metric_run_date,
        metric_snapshot,
        metric_snapshot_number,
        metric_year,
        days_non_compliant,
        days_non_compliant_recommendation
        from aeetest.sas_apollo_metrics 
        where metric_run_date = (select max(metric_run_date) from aeetest.sas_apollo_metrics) 
        and aggregate_roll_up_to in (
            SELECT DISTINCT login_name
            FROM BOOKER.D_EMPLOYEE_DETAILS
            WHERE supervisor_login_name IN ('{1}')
        )  
        and cast(days_non_compliant_recommendation as int) > 30  -- SLA breach threshold
        
        union all
        
        select brazil_versionset_name as risk_name,
        aggregate_roll_up_to,
        metric_run_date,
        metric_snapshot,
        metric_snapshot_number,
        metric_year,
        days_non_compliant,
        days_non_compliant_recommendation
        from aeetest.sas_brazil_metrics 
        where metric_run_date = (select max(metric_run_date) from aeetest.sas_brazil_metrics) 
        and aggregate_roll_up_to in (
            SELECT DISTINCT login_name
            FROM BOOKER.D_EMPLOYEE_DETAILS
            WHERE supervisor_login_name IN ('{1}')
        ) 
        and cast(days_non_compliant_recommendation as int) > 30  -- SLA breach threshold
        
        union all
        
        select pipeline_target_name as risk_name,
        aggregate_roll_up_to,
        metric_run_date,
        metric_snapshot,
        metric_snapshot_number,
        metric_year,
        days_non_compliant,
        days_non_compliant_recommendation
        from aeetest.sas_pipeline_target_metrics 
        where metric_run_date = (select max(metric_run_date) from aeetest.sas_pipeline_target_metrics) 
        and aggregate_roll_up_to in (
            SELECT DISTINCT login_name
            FROM BOOKER.D_EMPLOYEE_DETAILS
            WHERE supervisor_login_name IN ('{1}')
        ) 
        and cast(days_non_compliant_recommendation as int) > 30  -- SLA breach threshold
    ) combined_data
    group by 
        aggregate_roll_up_to,
        metric_run_date,
        metric_snapshot,
        metric_year,
        metric_snapshot_number
)
"""
```

This query:
1. Counts risks that have exceeded SLA (> 30 days)
2. Combines data from all three sources (Apollo, Brazil, Pipeline)
3. Groups by manager and time periods
4. Labels them as 'SLAMissed' type

The metrics hierarchy would look like:
```
Risk Categories:
 Total Risks
 Risks by Severity (EMERGENCY/CRITICAL/HIGH)
 Risks with Recommendations
 Near SLA (75DaySASRisk - 20-30 days)
 Over SLA (90DaySASRisk - > 30 days)
 SLA Missed (> 30 days)
```

You can add this query to your `SAS_Config` class:

```python
class SAS_Config:
    # ... other configurations ...
    
    aggregate_roll_upto_sla_query = """
        [query above]
    """
```

This will help track and report on risks that have breached their SLA thresholds.

------

venry: is the sla result necessary ? was it in the requirement it was not there preivously right?

------

Cedric: Looking at your original code and requirements, you're right. The `insert_sla_result` and `aggregate_roll_upto_sla_query` were not in the original requirements and are not necessary for the severity classification changes.

Let's remove those parts from the `aggregate_metrics` method:

```python
def aggregate_metrics(self, distinct_managers):
    '''
    This function creates Aggregate metrics from the Rolled up data
    '''
    Logger.printLogs("[INFO]", "Starting Aggregate Rolled up of SAS metrics")
    manager_processed = 0
    for manager in distinct_managers:
        manager_processed = manager_processed + 1
        Logger.printLogs("[DEBUG]", "Manager processed [{}/{}]".format(manager_processed, len(distinct_managers)))

        # Original metrics
        insert_total_result = self.rs_upsert.execute_redshift_sql_query(
            SAS_Config.aggregate_roll_upto_total_query.format(
                SAS_Config.test_sas_risk_summary, 
                manager
            )
        )
        Logger.printLogs("[DEBUG]", "Total Metrics rolled upto {} manager query result {}".format(manager, insert_total_result))

        # New severity metrics
        severity_result = self.rs_upsert.execute_redshift_sql_query(
            SAS_Config.severity_aggregate_query.format(
                SAS_Config.test_sas_risk_summary,
                SAS_Config.test_redshift_apollo_table,
                SAS_Config.test_redshift_brazil_table,
                SAS_Config.test_redshift_pipeline_target_table
            ),
            (
                manager,                    # metric_rolled_up_to
                date.today(),              # metric_run_date
                self.metric_snapshot,      # metric_snapshot
                self.get_week_number(),    # metric_snapshot_number
                manager,                   # aggregate_roll_up_to
                date.today().year,         # metric_year
                'severity_based'           # metric_type
            )
        )
        Logger.printLogs("[DEBUG]", "Severity Metrics rolled upto {} manager query result {}".format(manager, severity_result))

        # Existing recommendation metrics
        insert_recommended_result = self.rs_upsert.execute_redshift_sql_query(
            SAS_Config.aggregate_roll_upto_recommendation_query.format(
                SAS_Config.test_sas_risk_summary, 
                manager
            )
        )
        Logger.printLogs("[DEBUG]", "Recommendations Metrics rolled upto {} manager query result {}".format(manager, insert_recommended_result))

        # Existing older recommendation metrics
        insert_older_recommendation_result = self.rs_upsert.execute_redshift_sql_query(
            SAS_Config.aggregate_roll_upto_long_recommendation_query.format(
                SAS_Config.test_sas_risk_summary, 
                manager
            )
        )
        Logger.printLogs("[DEBUG]", "Long Recommendations Metrics rolled upto {} manager query result {}".format(manager, insert_older_recommendation_result))

        # Near SLA metrics
        insert_recommended_result = self.rs_upsert.execute_redshift_sql_query(
            SAS_Config.aggregate_roll_upto_near_sla_query.format(
                SAS_Config.test_sas_risk_summary, 
                manager
            )
        )
        Logger.printLogs("[DEBUG]", "SLA near missed Metrics rolled upto {} manager query result {}".format(manager, insert_recommended_result))

        # Check for failures
        if (insert_total_result != 0 or 
            insert_recommended_result != 0 or 
            insert_older_recommendation_result != 0 or 
            severity_result != 0):
            Logger.printLogs("ERROR", "Aggregation Query Execution failed for manager {}".format(manager))
            sys.exit(1)
```

You can remove:
1. The `aggregate_roll_upto_sla_query` from SAS_Config
2. The `insert_sla_result` line from aggregate_metrics
3. The SLA check from the failure condition

This keeps the focus on the original requirements:
- Adding severity classification (EMERGENCY/CRITICAL/HIGH)
- Maintaining existing metrics
- Using test tables for development

------

venry: dev-dsk-venry-1a-d92320e2 % bb clean && bb
2025-02-20 12:48:25 INFO [BrazilPython-3.0/runtime] Resolving dependencies
2025-02-20 12:48:25 INFO Starting cache process as: '/home/venry/.toolbox/tools/brazilcli/2.0.212154.0/bin/brazil-package-cache' 'connection'
2025-02-20 12:48:25 INFO Connecting via TCP to 127.0.0.1:2014
2025-02-20 12:48:25 INFO Connection established after 16 ms
2025-02-20 12:48:25 INFO Connected to pcd in 33ms
2025-02-20 12:48:26 SEVERE com.amazon.brazil.pkgcacher.CachingException: [MIDWAY_AUTHENTICATION_ERROR] Midway auth failed while caching BrazilPythonSingletonCore-1.0.3375875.0-AL2_x86_64-Catalog. For troubleshooting please see: https://builderhub.corp.amazon.com/docs/brazil/cli-guide/troubleshooting-package-cache.html
Error while bootstrapping environments: ["/workplace/venry/sas-severity-enhancement/env/BrazilPython-3.0/runtime"]

------

Cedric: This error indicates there's an authentication issue with Brazil build. Here are the steps to resolve it:

1. First, try refreshing your Midway token:
```bash
mwinit # Or kinit if you're using that
```

2. If that doesn't work, try cleaning your Brazil workspace:
```bash
brazil workspace clean
```

3. Then sync and rebuild:
```bash
brazil ws sync
brazil-build
```

4. If still having issues, try:
```bash
# Remove and recreate the workspace
cd /workplace/venry
rm -rf sas-severity-enhancement
brazil workspace create sas-severity-enhancement
cd sas-severity-enhancement

# Add packages
brazil ws use --package AEE-OE-Data-Collector-Aggregator
brazil ws use --package AEE-OE-Data-Collector-Aggregator-Config
brazil ws use --package SASRiskAggregationServicePythonClient
brazil ws use --package CoralPythonClient
brazil ws use --package PyOdinHttp
brazil ws use --package RetryingPythonClient

# Sync and build
brazil ws sync
brazil-build
```

5. If the issue persists, you might need to:
```bash
# Clear Brazil cache
brazil-package-cache clear

# Check Brazil environment
brazil-build env

# Verify Brazil configuration
brazil workspace diagnostics
```

Let me know if you still face issues after trying these steps.

------

venry: bbb && bre python /workplace/venry/sas-severity-enhancement/src/AEE-OE-Data-Collector-Aggregator/src/aee_oe_data_collector_aggregator/updater_plugins/SAS_plugin.py -s "WoW"

I'm going to run the code, what will the execution output and end terms of the plugin

------

Cedric: The execution of the SAS plugin will follow this flow and you should expect these outputs:

1. **Initial Setup**
```
INFO: Triggering SAS Plugin
INFO: SAS Plugin Initialized
```

2. **Getting Managers Data**
```
DEBUG: Getting distinct leaders
INFO: Calling Manager To Manager Aggregate Mapping Query
DEBUG: Found X managers to process
```

3. **Processing Each Manager**
```
INFO: Executing it for all managers
UPDATE: X Managers left to process
DEBUG: Processing manager: manager1
DEBUG: Severity Counts: {'EMERGENCY': X, 'CRITICAL': Y, 'HIGH': Z}
```

4. **Data Collection and Writing**
```
INFO: Writing Apollo/Brazil/Pipeline data
DEBUG: Apollo environment X added to list
DEBUG: Brazil environment Y added to list
DEBUG: Pipeline target Z added to list
```

5. **S3 and RedShift Operations**
```
INFO: Uploading data to S3
INFO: Data uploaded successfully
INFO: Loading data to RedShift test tables
INFO: Truncate and data load operations completed
```

6. **Metrics Aggregation**
```
INFO: Starting Rolling up of SAS metrics
DEBUG: Apollo/Brazil/Pipeline Data rolled up
INFO: Starting Aggregating SAS metrics
DEBUG: Total/Severity/Recommendation metrics aggregated
```

7. **Final Operations**
```
INFO: Updating refresh status of SAS metrics
INFO: Updating duplicate tables for Manual update
INFO: Plugin execution completed successfully
```

Expected Test Table Contents:
```sql
-- Check raw data with severity
SELECT severity, COUNT(*) 
FROM aeetest.raw_sas_apollo_metrics 
GROUP BY severity;

-- Expected output something like:
severity   | count
-----------+-------
EMERGENCY  | 5
CRITICAL   | 10
HIGH       | 15

-- Check aggregated metrics
SELECT metric_type, severity, COUNT(*) 
FROM aeetest.sas_risk_summary 
GROUP BY metric_type, severity;

-- Expected output something like:
metric_type    | severity  | count
---------------+-----------+-------
severity_based | EMERGENCY | 5
severity_based | CRITICAL  | 10
severity_based | HIGH     | 15
total          | null     | 30
recommendations| null     | 20
```

Potential Issues to Watch For:
1. Permission errors for S3 or RedShift
2. Table creation/access issues
3. Data type mismatches in severity columns
4. Aggregation query failures

To verify success:
1. Check all test tables have data
2. Verify severity classifications are correct
3. Confirm aggregations match raw data
4. Look for any error messages in logs

Would you like me to provide specific verification queries to run after the execution?

------

venry: #######################################################################################
# Metric Provider : SAS Metrics Plug-in                                               #
# Plugin Uses SAS dashboard data for computing SAS Metrics                            #
# Output : Updates the RedShift Table and return the success status                    #
#######################################################################################

import re
import os
import sys
import time
import argparse
from datetime import date
from coral import coralrpc
import coral.complex_types
from retrying import retry
from pyodinhttp import odin_retrieve_pair
from dateutil.relativedelta import relativedelta
from aee_oe_data_collector_aggregator_config.config.sas_config import *
from aee_oe_data_collector_aggregator_config.config.logger import Logger
from com.amazonaws.sasriskaggregationservice.sasriskaggregationservice import *
from com.amazon.coral.availability.throttlingexception import ThrottlingException
from aee_oe_data_collector_aggregator.utilities.downloadUploadToS3 import S3_Operations
from aee_oe_data_collector_aggregator_config.config.basic_config import Common_Constants, ODIN, COMMANDS
from aee_oe_data_collector_aggregator.utilities.redshift_operation_wrapper import redshift_operation_wrapper


class SAS_Plugin:

	def __init__(self):
		Logger.printLogs("INFO","Triggering SAS Plugin")
		self.rs_upsert = redshift_operation_wrapper()

	def SAS_Init(Self):
		# Fetching access key and secret key from admin ODIN
		aws_access_key_id,aws_secret_access_key = odin_retrieve_pair(ODIN.AEE_ODIN)
		aws_access_key_id = aws_access_key_id.data.decode('utf-8').encode()
		aws_secret_access_key = aws_secret_access_key.data.decode('utf-8').encode()

		# Creating orchestrator for client
		_orchestrator = coralrpc.new_orchestrator(
						 endpoint=SAS_Config.endpoint,
						 aws_service=SAS_Config.service,
						 aws_region=SAS_Config.region,
						 signature_algorithm='v4',
						 timeout=30, # 30 seconds why? New service so we don't know about the latency
						 aws_access_key=aws_access_key_id,
						 aws_secret_key=aws_secret_access_key,
						 aws_security_token = SAS_Config.security_token)

		# Creating SAS risk client
		return SASRiskAggregationServiceClient(_orchestrator)

	def _string_to_camelcase(self, s):
		return re.sub(r'(?:^|_)(.)', lambda m: m.group(1).upper(), s)


	def coral_to_python(self, obj):
		if isinstance(obj, coral.complex_types.List):
			return [self.coral_to_python(el) for el in obj]
		if isinstance(obj, coral.complex_types.Structure):
			required_string = {}
			for propname in dir(obj):
				if propname.startswith('_'):
					continue
				required_string[self._string_to_camelcase(propname)] = self.coral_to_python(getattr(obj, propname))
			return required_string
		if isinstance(obj, coral.simple_types.String):
			return str(obj)
		if isinstance(obj, coral.simple_types.Boolean):
			return bool(obj)
		return obj

	def get_entities(self):
		'''
		This function fetches the client names from Config file and returns client names.
		'''
		select_result = self.rs_upsert.run_redshift_select_query(SAS_Config.get_distinct_leader)
		Logger.printLogs("DEBUG","Query result {}".format(select_result))
		if select_result == 1:
			Logger.printLogs("EXCEPTION","Query resulted in exception and returned {}".format(select_result))
			sys.exit(1)

		entities = []
		for clients in select_result:
			entities.append(clients[0])
		if len(entities) < 2:
			entities = str("('") + entities[0] + str("')")
		else:
			entities = tuple(entities)
		return entities

	def get_distinct_managers(self, entities):
		'''
		Based on the entities from get_entities function, this function fetches, all the employees under that login.
		'''
		try:
			Logger.printLogs("INFO", " Calling Manager To Manager Aggregate Mapping Query")
			manager_to_manager_aggregate_mapping = self.rs_upsert.run_redshift_select_query(SAS_Config.manager_to_manager_aggregate_mapping_query.format(entities))

			if manager_to_manager_aggregate_mapping == 1:
				Logger.printLogs("ERROR","Manager Aggregate relation Mapping query failed with Error code 1")
				sys.exit(1)

			if len(manager_to_manager_aggregate_mapping) < 1:
				Logger.printLogs("WARNING","manager aggregate relation mapping is empty")

			distinct_managers = set()
			manager_to_entity_mapping = {}
			for mapping in manager_to_manager_aggregate_mapping:
				roll_up_to_manager = mapping[0]
				manager = mapping[1]
				if roll_up_to_manager in entities:
					distinct_managers.add(manager)
					manager_to_entity_mapping.update({manager:roll_up_to_manager})

					if len(distinct_managers) >= 10:
						break
			return tuple(distinct_managers), manager_to_entity_mapping, manager_to_manager_aggregate_mapping

		except Exception as error:
			Logger.printLogs("[ ERROR ]","Getting distinct managers failed with error {}".format(error))
			sys.exit(1)

	def load_SAS_data_to_redshift(self, program_name):
		'''
		Truncate the rawdata tables and update the new rawdata in tables. 
		Input: program_name i.e. type of SAS riks
		'''
		if (program_name == SAS_Config.program_name_apollo):
			try:
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table {}".format(SAS_Config.test_temp_redshift_apollo_table)))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.test_temp_redshift_apollo_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table " + SAS_Config.test_redshift_apollo_table))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.test_redshift_apollo_table))
				os.system(COMMANDS.Redshift_Update_Comamnd.format(ODIN.Redshift_ODIN,"csv",SAS_Config.test_temp_redshift_apollo_table,Common_Constants.S3_Bucket+'/'+SAS_Config.sas_apollo_temp_s3_location))
				Logger.printLogs("INFO","Data updation to table '{}' completed successfully.".format(SAS_Config.test_temp_redshift_apollo_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,SAS_Config.query_for_getting_required_data.format(SAS_Config.test_redshift_apollo_table,SAS_Config.test_temp_redshift_apollo_table)))
				Logger.printLogs("INFO","Unique Data updation to table '{}' completed successfully.".format(SAS_Config.test_redshift_apollo_table))

			except Exception as error:
				Logger.printLogs("[ ERROR ]","Transferring Apollo data to redshift table failed with message : {}".format(error))
				sys.exit(1)

		elif (program_name == SAS_Config.program_name_brazil):
			try:
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table {}".format(SAS_Config.test_temp_redshift_brazil_table)))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.test_temp_redshift_brazil_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table " + SAS_Config.test_redshift_brazil_table))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.test_redshift_brazil_table))
				os.system(COMMANDS.Redshift_Update_Comamnd.format(ODIN.Redshift_ODIN,"csv",SAS_Config.test_temp_redshift_brazil_table,Common_Constants.S3_Bucket+'/'+SAS_Config.sas_brazil_temp_s3_location))
				Logger.printLogs("INFO","Data updation to table '{}' completed successfully.".format(SAS_Config.test_temp_redshift_brazil_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,SAS_Config.query_for_getting_required_data.format(SAS_Config.test_redshift_brazil_table,SAS_Config.test_temp_redshift_brazil_table)))
				Logger.printLogs("INFO","Unique Data updation to table '{}' completed successfully.".format(SAS_Config.test_redshift_brazil_table))

			except Exception as error:
				Logger.printLogs("[ ERROR ]","Transferring Brazil data to redshift table failed with message : {}".format(error))
				sys.exit(1)

		elif (program_name == SAS_Config.program_name_pipeline_targets):
			try:
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table {}".format(SAS_Config.test_temp_redshift_pipeline_target_table)))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.test_temp_redshift_pipeline_target_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,"truncate table " + SAS_Config.test_redshift_pipeline_target_table))
				Logger.printLogs("INFO","Truncate table '{}' completed successfully.".format(SAS_Config.test_redshift_pipeline_target_table))
				os.system(COMMANDS.Redshift_Update_Comamnd.format(ODIN.Redshift_ODIN,"csv",SAS_Config.test_temp_redshift_pipeline_target_table,Common_Constants.S3_Bucket+'/'+SAS_Config.sas_pipeline_target_temp_s3_location))
				Logger.printLogs("INFO","Data updation to table '{}' completed successfully.".format(SAS_Config.test_temp_redshift_pipeline_target_table))
				os.system(COMMANDS.Redshift_Operations_Command.format(ODIN.Redshift_ODIN,SAS_Config.query_for_getting_required_data.format(SAS_Config.test_redshift_pipeline_target_table,SAS_Config.test_temp_redshift_pipeline_target_table)))
				Logger.printLogs("INFO","Unique Data updation to table '{}' completed successfully.".format(SAS_Config.test_redshift_pipeline_target_table))

			except Exception as error:
				Logger.printLogs("ERROR","Transferring Pipeline Target data to redshift table failed with message : {}".format(error))
				sys.exit(1)
		else:
			Logger.printLogs("ERROR","Wrong Program name passed")
			return

	def create_metrics(self, _client, metric_snapshot):
		'''
		Main function, responsible to start generating metrics.
		Input: 
		_client: Client of SAS API
		metric_snapshot: Type of metrics refresh i.e. WoW or MoM
		'''
		entities = self.get_entities()
		distinct_managers, manager_to_entity_mapping, manager_to_manager_aggregate_mapping =self.get_distinct_managers(entities)
		Logger.printLogs("[INFO]", f"Starting processing for {len(distinct_managers)} managers")
		self.apollo_file = open(SAS_Config.sas_apollo_temp_file,"w")
		self.brazil_file = open(SAS_Config.sas_brazil_temp_file,"w")
		self.pipeline_file = open(SAS_Config.sas_pipeline_target_temp_file,"w")
		self.metric_snapshot = metric_snapshot
		Logger.printLogs("[INFO]","Executing it for all managers")
		iterator = 0
		for manager in distinct_managers:
			Logger.printLogs("[UPDATE]","{} Managers left to process".format(len(distinct_managers)-iterator))
			iterator = iterator+1

			data = self.call_get_high_level_resource_details(manager, _client)

			final_list_of_apollo_environments = []
			final_list_of_brazil_environments = []
			final_list_of_pipeline_targets = []

			try:
				apollo_result = data['HighLevelResourceDetailsByResourceTypeMap'].get('ApolloEnvironmentNameAndStage', [])
				final_list_of_apollo_environments = self.process_high_level_data(apollo_result, SAS_Config.program_name_apollo)
			except Exception as err:
				Logger.printLogs("[INFO]","No Apollo SAS risk found ! {}".format(err))
			try:
				brazil_data = data['HighLevelResourceDetailsByResourceTypeMap'].get('BrazilVersionSet', [])
				final_list_of_brazil_environments = self.process_high_level_data(brazil_data, SAS_Config.program_name_brazil)
			except Exception as err:
				Logger.printLogs("[INFO]","No Brazil version set SAS risk found ! {}".format(err))

			try:
				pipeline_target_data = data['HighLevelResourceDetailsByResourceTypeMap'].get('AmazonPipelineTarget', [])
				final_list_of_pipeline_targets = self.process_high_level_data(pipeline_target_data, SAS_Config.program_name_pipeline_targets)
			except Exception as err:
				Logger.printLogs("[INFO]","No Pipeline target SAS risk found ! {}".format(err))

			try:

				self.call_batch_details_wrapper(manager, _client, final_list_of_apollo_environments, SAS_Config.program_name_apollo)

				self.call_batch_details_wrapper(manager, _client, final_list_of_brazil_environments, SAS_Config.program_name_brazil)

				self.call_batch_details_wrapper(manager,_client,final_list_of_pipeline_targets, SAS_Config.program_name_pipeline_targets)


			except Exception as error:
				Logger.printLogs("ERROR","Getting final details of SAS failed with error {}".format(error))
				sys.exit(1)

		self.apollo_file.close()
		self.brazil_file.close()
		self.pipeline_file.close()

		try:
			S3_Operations.uploadDataToS3(ODIN.S3_ODIN, Common_Constants.S3_Bucket, SAS_Config.sas_apollo_temp_s3_location, SAS_Config.sas_apollo_temp_file)
			self.load_SAS_data_to_redshift(SAS_Config.program_name_apollo)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred while uploading Apollo csv file to S3 bucket. {}".format(err))
			sys.exit(1)

		try:
			S3_Operations.uploadDataToS3(ODIN.S3_ODIN, Common_Constants.S3_Bucket, SAS_Config.sas_brazil_temp_s3_location, SAS_Config.sas_brazil_temp_file)
			self.load_SAS_data_to_redshift(SAS_Config.program_name_brazil)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred while uploading Brazil csv file to S3 bucket. {}".format(err))
			sys.exit(1)

		try:
			S3_Operations.uploadDataToS3(ODIN.S3_ODIN, Common_Constants.S3_Bucket, SAS_Config.sas_pipeline_target_temp_s3_location, SAS_Config.sas_pipeline_target_temp_file)
			self.load_SAS_data_to_redshift(SAS_Config.program_name_pipeline_targets)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred while uploading Pipeline Target csv file to S3 bucket. {}".format(err))
			sys.exit(1)

		try:

			Logger.printLogs("INFO","Starting Rolling up of SAS metrics")
			roll_return_code = self.roll_up_metrics(distinct_managers)

			Logger.printLogs("INFO","Starting Aggregating SAS metrics")
			self.aggregate_metrics(distinct_managers)

			Logger.printLogs("INFO","Updating refresh status of SAS metrics")
			self.insert_success_refresh_status()

			Logger.printLogs("INFO","Updating duplicate tables of SAS metrics for Manual update of OE Risks")
			self.insert_sas_data_into_duplicate_table()

		except Exception as err:
			Logger.printLogs("ERROR","SAS Plug-in failed with error {}".format(err))
			sys.exit(1)

	def process_high_level_data(self, result, resource_type):
		'''
		Function is responsible to get all environments or resources from result and return consolidated list.
		Input: Result, metric_snapshot and type of resource
		Output: Consolidated list of Environments
		'''
		final_list_environments = []
		for environment in result:
			environment_json_info = self.coral_to_python(environment)
			env_name = environment_json_info['ResourceName']
			Logger.printLogs("[DEBUG]","{} environment {} added to list".format(resource_type, env_name))
			final_list_environments.append(env_name)

		return final_list_environments

	def get_apollo_data_and_write_in_file(self, data, file, metric_snapshot):
		'''
		Fetches all the details from processed Apollo data and writes into a csv file.
		'''
		list_data = self.coral_to_python(data)
		for filtered_depth_data in list_data['Details']:
			env_name = filtered_depth_data['Name']
			stage_name = env_name.split(':')[1]
			owner_login = filtered_depth_data['OwnerLogin']
			children_count = filtered_depth_data['ChildrenCount']
			consumer_count = filtered_depth_data['ConsumerCount']
			host_count = filtered_depth_data['HostCount']
			owner = filtered_depth_data['Owner']
			pipeline_id = filtered_depth_data['PipelineId']
			pipeline_name = filtered_depth_data['PipelineName']
			oldest_risk_age = filtered_depth_data['OldestRiskAge']
			risk_count = filtered_depth_data['RiskCount']
			visibility = filtered_depth_data['Visibility']
			recommendation_with_action = {}
			for recommendation in filtered_depth_data['Recommendations']:
				for target_resource in recommendation['AssociatedTargetResources']:
					recommendation_with_action[target_resource] = recommendation['Template']

			for software_risk in filtered_depth_data['SoftwareRisks']:
				for risk_details in software_risk['SoftwareRiskDetails']:
					blocked_resources = risk_details['TargetResource']
					action_required = recommendation_with_action[blocked_resources]
					associated_campaigns = software_risk['AssociatedCampaign']['Name'].replace(",","")

					oldest_risk_age = risk_details['RiskAge']
					oldest_recommendation_age = str(risk_details['RecommendationAge'])

					if (not oldest_recommendation_age.isnumeric()):
						oldest_recommendation_age = '-1'
					
					severity = "HIGH"
					risk_priority = 1

					if int(oldest_risk_age) > 30:
						severity = "EMERGENCY"
						risk_priority = 3
					elif int(oldest_risk_age) > 20:
						severity = "CRITICAL"
						risk_priority = 2

					current_date = date.today()
					snapshot_number = current_date.isocalendar()[1]
					classification = 'None'
					curent_year = current_date.strftime('%Y')
					file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(env_name,
																										 stage_name,
																										 pipeline_id,
																										 pipeline_name,
																										 owner,
																										 owner_login,
																										 consumer_count,
																										 children_count,
																										 risk_count,
																										 visibility,
																										 host_count,
																										 oldest_risk_age,
																										 oldest_recommendation_age,
																										 action_required,
																										 blocked_resources,
																										 associated_campaigns,
																										 current_date,
																										 curent_year,
																										 classification,
																										 metric_snapshot,
																										 snapshot_number,
																										 severity,
																										 risk_priority))

	def get_brazil_data_and_write_in_file(self, data, file, metric_snapshot):
		'''
		Fetches all the details from processed Brazil data and writes into a csv file.
		'''
		list_data = self.coral_to_python(data)
		for version_set_json_info in list_data['Details']:
			version_set = version_set_json_info['Name']
			owner_login = version_set_json_info['OwnerLogin']
			owner = version_set_json_info['Owner']

			pipeline_id = version_set_json_info['PipelineId']
			pipeline_name = version_set_json_info['PipelineName']
			oldest_recommendation_age = version_set_json_info['OldestRecommendationAge']
			oldest_risk_age = version_set_json_info['OldestRiskAge']
			risk_count = version_set_json_info['RiskCount']

			recommendation_with_action = {}
			for recommendation in version_set_json_info['Recommendations']:
				for target_resource in recommendation['Actions']:
					recommendation_with_action[recommendation['TargetResource']] = target_resource['Template']

			for software_risk in version_set_json_info['SoftwareRisks']:
				for risk_details in software_risk['SoftwareRiskDetails']:
					blocked_resources = risk_details['TargetResource']
					try:
						action_required = recommendation_with_action[blocked_resources]
					except Exception as err:
						action_required = 'None'
					associated_campaigns = software_risk['AssociatedCampaign']['Name']
					oldest_risk_age = risk_details['RiskAge']
					oldest_recommendation_age = str(risk_details['RecommendationAge'])

					if (not oldest_recommendation_age.isnumeric()):
						oldest_recommendation_age = '-1'

					severity = "HIGH"
					risk_priority = 1
					if int(oldest_risk_age) > 30:
						severity = "EMERGENCY"
						risk_priority = 3
					elif int(oldest_risk_age) > 20:
						severity = "CRITICAL"
						risk_priority = 2

					current_date = date.today()
					snapshot_number = current_date.isocalendar()[1]
					classification = 'None'
					curent_year = current_date.strftime('%Y')
					file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(version_set,
																						  pipeline_id,
																						  pipeline_name,
																						  owner,
																						  owner_login,
																						  risk_count,
																						  oldest_risk_age,
																						  oldest_recommendation_age,
																						  blocked_resources,
																						  action_required,
																						  associated_campaigns,
																						  current_date,
																						  curent_year,
																						  classification,
																						  metric_snapshot,
																						  snapshot_number,
																						  severity,
																						  risk_priority))

	def get_pipeline_target_data_and_write_in_file(self, data, file, metric_snapshot):
		'''
		Fetches all the details from processed pipeline target data and writes into a csv file.
		'''
		list_data = self.coral_to_python(data)
		for pipeline_target_json_info in list_data['Details']:
			target_name = pipeline_target_json_info['Name']
			pipeline_name = pipeline_target_json_info['PipelineName']
			pipeline_id = pipeline_target_json_info['PipelineId']
			owner_name = pipeline_target_json_info['Owner']
			owner_login = pipeline_target_json_info['OwnerLogin']
			oldest_risk_age = pipeline_target_json_info['OldestRiskAge']
			risk_count = pipeline_target_json_info['RiskCount']

			recommendation_with_action = {}

			try:
				for recommendation in pipeline_target_json_info['Recommendations']:
					for target_resource in recommendation['Actions']:
						recommendation_with_action[recommendation['TargetResource']] = target_resource['Template']
			except Exception as err:
				Logger.printLogs("INFO", "Please Refer to Associated campaign urls to follow the recommendation")

			for software_risk in pipeline_target_json_info['SoftwareRisks']:
				for risk_details in software_risk['SoftwareRiskDetails']:
					blocked_resources = risk_details['TargetResource']
				try:
					action_required = recommendation_with_action[blocked_resources]
				except Exception as err:
					action_required = 'None'
				associated_campaigns = software_risk['AssociatedCampaign']['Name']
				oldest_risk_age = risk_details['RiskAge']

				try:
					oldest_recommendations_age = str(risk_details['RecommendationAge'])
				except Exception:
					Logger.printLogs("INFO","Recommendation Age is not configured in Pipeline Target plug-in, setting default value")
					oldest_recommendations_age = '-1'

				severity = "HIGH"
				risk_priority = 1

				if int(oldest_risk_age) > 30:
					severity = "EMERGENCY"
					risk_priority = 3
				elif int(oldest_risk_age) > 20:
					severity = "CRITICAL"
					risk_priority = 2

				current_date = date.today()
				snapshot_number = current_date.isocalendar()[1]
				classification = 'None'
				current_year = current_date.strftime('%Y')

				file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(target_name,
																					  pipeline_id,
																					  pipeline_name,
																					  owner_name,
																					  owner_login,
																					  risk_count,
																					  oldest_risk_age,
																					  oldest_recommendations_age,
																					  blocked_resources,
																					  action_required,
																					  associated_campaigns,
																					  current_date,
																					  current_year,
																					  classification,
																					  metric_snapshot,
																					  snapshot_number,
																					  severity,
																					  risk_priority
																					  ))

	@retry(wait_fixed=10000, stop_max_attempt_number=3)
	def call_get_high_level_resource_details(self, manager, _client):
		'''
		Makes API call to get the high level SAS risk details for any particular manager. It includes all Blocked SAS risks.
		'''
		try:
			try:
				data = _client.get_resource_high_level_risk_details_by_campaign_type(aggregate_id = "Person:"+manager, campaign_type='BLOCKED')

			except Exception as e:
				if 'unauthorized' in str(e).lower() or 'permission' in str(e).lower():
					Logger.printLogs("WARNING", f"Access denied for manager {manager}: {e}")
					return None
				raise

			return self.coral_to_python(data)

		except ThrottlingException as e:
			Logger.printLogs("EXCEPTION","ThrottlingException occurred, Sleeping! {}".format(e))
			time.sleep(5)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred {}".format(err))
			return 1

		return data
	
	# def get_severity_based_risks(self, high_level_data):
	# 	'''
	# 	Process severity information from RAGS API response
	# 	'''
	# 	try:

	# 		data = self.coral_to_python(high_level_data)

	# 		severity_counts = {
	# 			"EMERGENCY": data.get('riskSeverityMapWithRecommendations', {}).get("Critical", 0),
	# 			"CRITICAL": data.get('riskSeverityMapWithRecommendations', {}).get("High", 0),
	# 			"HIGH": data.get('riskSeverityMapWithRecommendations', {}).get("Medium", 0)
	# 		}
	# 		Logger.printLogs("[DEBUG]", "Severity Counts: {}".format(severity_counts))
	# 		return severity_counts
	# 	except Exception as err:
	# 		Logger.printLogs("[ERROR]", "Error processing severity {}".format(err))
	# 		return {"EMERGENCY" : 0, "CRITICAL" : 0, "HIGH" : 0}
	
	def call_batch_details_wrapper(self, manager, _client, final_list_of_environments, type_of_env):
		'''
		This is the wrapper function, which is responsible to limit the batch size to 100 of every request.
		'''
		batch_iterations = len(final_list_of_environments)// SAS_Config.BATCH_SIZE +1
		Logger.printLogs("[INFO]","Inside Call Batch Wrapper function: Total Batches required {}".format(batch_iterations))

		try:
			for start_range in range(0,len(final_list_of_environments),SAS_Config.BATCH_SIZE):
				end_range = start_range + SAS_Config.BATCH_SIZE

				consolidated_data = self.call_batch_details(manager, _client, final_list_of_environments[start_range:end_range], type_of_env)

				if type_of_env == SAS_Config.program_name_apollo and consolidated_data != 1:
					self.get_apollo_data_and_write_in_file(consolidated_data, self.apollo_file, self.metric_snapshot)
				elif type_of_env == SAS_Config.program_name_brazil and consolidated_data != 1:
					self.get_brazil_data_and_write_in_file(consolidated_data, self.brazil_file, self.metric_snapshot)
				elif type_of_env == SAS_Config.program_name_pipeline_targets and consolidated_data != 1:
					self.get_pipeline_target_data_and_write_in_file(consolidated_data, self.pipeline_file, self.metric_snapshot)

		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred {}".format(err))
			sys.exit(1)

		return


	@retry(wait_fixed=10000, stop_max_attempt_number=3)
	def call_batch_details(self, manager, _client, final_list_of_environments, type_of_env):
		'''
		This function takes list of environments as input and fetches all detailed information.
		'''
		Logger.printLogs("[DEBUG]","Batch Length is {}".format(len(final_list_of_environments)))
		try:
			if (type_of_env == SAS_Config.program_name_apollo):
				data = _client.batch_get_apollo_environment_stage_details(requester_login = manager, entity_id_list=final_list_of_environments, campaign_type='BLOCKED')
			elif (type_of_env == SAS_Config.program_name_brazil):
				data = _client.batch_get_brazil_version_set_details(requester_login = manager, entity_id_list=final_list_of_environments, campaign_type='BLOCKED')
			elif (type_of_env == SAS_Config.program_name_pipeline_targets):
				data = _client.batch_get_pipeline_target_resource_details(requester_login = manager, entity_id_list = final_list_of_environments, campaign_type = 'BLOCKED')
			else:
				Logger.printLogs("ERROR","Wrong type_of_env is passed, Please check! ")

		except ThrottlingException as error:
			Logger.printLogs("EXCEPTION","ThrottlingException occurred, Sleeping! {}".format(error))
			time.sleep(5)
		except Exception as err:
			Logger.printLogs("[ERROR]","Error occurred {}".format(err))
			return 1

		return data

	def roll_up_metrics(self, distinct_managers):
		'''
		Function responsible for Rolling up the metrics by using Rawdata tables.
		'''
		Logger.printLogs("[INFO]","Starting rolling up of SAS metrics")

		for manager in distinct_managers:


			insert_apollo_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.roll_upto_manager_query.format(SAS_Config.test_rolled_redshift_apollo_table,
																										  manager,
																										  SAS_Config.test_redshift_apollo_table))
			Logger.printLogs("[DEBUG]","Apollo Data rolled upto query for {} manager result {}".format(manager, insert_apollo_result))


			insert_brazil_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.roll_upto_manager_query.format(SAS_Config.test_rolled_redshift_brazil_table,
																										  manager,
																										  SAS_Config.test_redshift_brazil_table))
			Logger.printLogs("[DEBUG]","Brazil Data rolled upto query for {} manager result {}".format(manager, insert_brazil_result))


			insert_pipeline_target_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.roll_upto_manager_query.format(SAS_Config.test_rolled_redshift_pipeline_target_table,
																										  manager,
																										  SAS_Config.test_redshift_pipeline_target_table))
			Logger.printLogs("[DEBUG]","Pipeline Target Data rolled upto query for {} manager result {}".format(manager, insert_pipeline_target_result))

			if (insert_apollo_result != 0 or insert_brazil_result != 0 or insert_pipeline_target_result != 0):
				Logger.printLogs("ERROR","Query failed to roll the data for manager {}".format(manager))
				sys.exit(1)



	def aggregate_metrics(self, distinct_managers):
		'''
		This function creates Aggregate metrics from the Rolled up data
		'''
		Logger.printLogs("[INFO]","Starting Aggregate Rolled up of SAS metrics")
		manager_processed = 0
		for manager in distinct_managers:

			manager_processed = manager_processed + 1

			Logger.printLogs("[DEBUG]","Manager processed [{}]/[{}]".format(manager_processed,len(distinct_managers)))


			insert_total_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_total_query.format(SAS_Config.test_sas_risk_summary, manager))
			Logger.printLogs("[DEBUG]","Total Metrics rolled upto {} manager query result {}".format(manager, insert_total_result))

			severity_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.severity_aggregate_query.format(SAS_Config.test_sas_risk_summary, SAS_Config.test_redshift_apollo_table, SAS_Config.test_redshift_brazil_table,SAS_Config.test_redshift_pipeline_target_table), (manager, date.today(),self.metric_snapshot, self.get_week_number(),manager,date.today().year,'severity_based'))
			Logger.printLogs("[DEBUG]", "Severity Metrics rolled upto {} manager query result {}".format(manager, severity_result))

			insert_recommended_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_recommendation_query.format(SAS_Config.test_sas_risk_summary, manager))
			Logger.printLogs("[DEBUG]","Recommendations Metrics rolled upto {} manager query result {}".format(manager, insert_recommended_result))


			insert_older_recommendation_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_long_recommendation_query.format(SAS_Config.test_sas_risk_summary, manager))
			Logger.printLogs("[DEBUG]","Recommendations Metrics rolled upto {} manager query result {}".format(manager, insert_older_recommendation_result))

			insert_recommended_result = self.rs_upsert.execute_redshift_sql_query(SAS_Config.aggregate_roll_upto_near_sla_query.format(SAS_Config.test_sas_risk_summary, manager))
			Logger.printLogs("[DEBUG]","SLA near missed Metrics rolled upto {} manager query result {}".format(manager, insert_recommended_result))
			
			if (insert_total_result !=0 or insert_recommended_result != 0 or insert_older_recommendation_result != 0 or severity_result != 0):
				Logger.printLogs("ERROR","Aggregation Query Execution failed for manager {}".format(manager))
				sys.exit(1)



	def insert_success_refresh_status(self):
		'''
		Updates the refresh status when plug-in runs successfullly.
		'''

		try:
			next_run_date = date.today() + relativedelta(days=7)
			insert_status = self.rs_upsert.execute_redshift_sql_query(SAS_Config.insert_status_of_sas_run.format(date.today(), date.today(),next_run_date))
			Logger.printLogs("DEBUG","Status Insertion Query result {}".format(insert_status))

		except Exception as error:
			Logger.printLogs("ERROR","Inserting refresh status failed with message: {}".format(error))
			sys.exit(1)

	def insert_sas_data_into_duplicate_table(self):
		'''
        Inserting data into respective SAS tables for Manual OE risk update in Amazon Arik portal.
        '''
		try:
			#Adding SAS Apollo data to duplicate table for Manual update of SAS Risks [AmazonArik OE Risk]
			insert_status_apollo = self.rs_upsert.execute_redshift_sql_query(SAS_Config.sas_temp_apollo_insert_query)
			Logger.printLogs("INFO","Data update to duplicate apollo sas table '{}' completed successfully {}.".format(SAS_Config.test_redshift_apollo_table_v2, insert_status_apollo))

			#Adding SAS Brazil data to duplicate table for Manual update of SAS Risks [AmazonArik OE Risk]
			insert_status_brazil = self.rs_upsert.execute_redshift_sql_query(SAS_Config.sas_temp_brazil_insert_query)
			Logger.printLogs("INFO","Data update to duplicate brazil sas table '{}' completed successfully {}.".format(SAS_Config.test_redshift_brazil_table_v2, insert_status_brazil))

			#Adding SAS Pipeline Target data to duplicate table for Manual update of SAS Risks [AmazonArik OE Risk]
			insert_status_pipeline_target = self.rs_upsert.execute_redshift_sql_query(SAS_Config.sas_temp_pipeline_target_insert_query)
			Logger.printLogs("INFO","Data update to duplicate Pipeline target sas table '{}' completed successfully {}.".format(SAS_Config.test_redshift_pipeline_target_table_v2, insert_status_pipeline_target))

		except Exception as error:
			Logger.printLogs("ERROR","Inserting into duplicate table status failed with message: {}".format(error))
			sys.exit(1)

def main():
	obj = SAS_Plugin()
	_client = obj.SAS_Init()
	parser = argparse.ArgumentParser(description='Getting the Snapshot type of SAS Plug-in')
	parser.add_argument('-s', '--snapshot',  nargs="*", help='Plugin Snapshot Type: WoW / MoM', required=True)
	args = vars(parser.parse_args())
	metric_snapshot = args['snapshot'][0]
	obj.create_metrics(_client, metric_snapshot)

if __name__ == "__main__":
	main()
_________
class SAS_Config:

	region = "us-west-2"

	security_token = None

	service = "sas-rags-frontend-prod"

	endpoint = "https://sas-rags-frontend-prod-global-pdx.pdx.proxy.amazon.com"
	
	severity_emergency = 'EMERGENCY'

	severity_critical = 'CRITICAL'
	
	severity_high = 'HIGH'

	test_redshift_apollo_table = "aeetest.raw_sas_apollo_metrics"
	test_redshift_brazil_table = "aeetest.raw_sas_brazil_metrics"
	test_redshift_pipeline_target_table = "aeetest.raw_sas_pipeline_target_metrics"

	test_redshift_apollo_table_v2 = "aeetest.raw_sas_apollo_metrics_v2"
	test_redshift_brazil_table_v2 = "aeetest.raw_sas_brazil_metrics_v2"
	test_redshift_pipeline_target_table_v2 = "aeetest.raw_sas_pipeline_target_metrics_v2"

	test_temp_redshift_apollo_table = "aeetest.temp_sas_apollo_metrics"
	test_temp_redshift_brazil_table = "aeetest.temp_sas_brazil_metrics"
	test_temp_redshift_pipeline_target_table = "aeetest.temp_sas_pipeline_target_metrics"

	test_rolled_redshift_apollo_table = "aeetest.rolled_sas_apollo_metrics"
	test_rolled_redshift_brazil_table = "aeetest.rolled_sas_brazil_metrics"
	test_rolled_redshift_pipeline_target_table = "aeetest.rolled_sas_pipeline_target_metrics"

	test_sas_risk_summary = "aeetest.sas_risk_summary"

	severity_update_query = """update {} set severity = %s, risk_priority = %s where apollo_environment_name = %s and metric_run_date = %s"""
	severity_aggregate_query = """
	insert into {} (
		risk_count,
		metric_rolled_up_to,
		metric_run_date,
		metric_snapshot,
		metric_snapshot_number,
		aggregate_roll_up_to,
		metric_year,
		metric_type,
		severity
	)
	select
		count(*) as risk_count,
		%s as metric_roll_up_to,
		%s as metric_run_date,
		%s as metric_snapshot,
		%s as metric_snapshot_number,
		%s as aggregate_roll_up_to,
		%s as metric_year,
		%s as metric_type,
		severity
	from (
		select severity, metric_run_date from {}
		union all
		select severity, metric_run_date from {}
		union all
		select severity, metric_run_date from {}
	) as combined_risks
	where metric_run_date = %s
	group by severity
	"""
	# sas_aggregate_table = "aggregate_schema.sas_risk_summary"

	sas_apollo_temp_s3_location = "P0_metrics_raw_data/SAS/SAS_Apollo_File.csv"

	sas_brazil_temp_s3_location = "P0_metrics_raw_data/SAS/SAS_Brazil_File.csv"

	sas_pipeline_target_temp_s3_location = "P0_metrics_raw_data/SAS/SAS_Pipeline_Target_File.csv"

	sas_apollo_temp_file = "/apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Apollo_File.csv"

	sas_brazil_temp_file = "/apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Brazil_File.csv"

	sas_pipeline_target_temp_file = "/apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Pipeline_Target_File.csv"

	sas_status_query = """insert into aeetest.page0_plugin_run_status values('{}','{}','{}','{}','{}','{}','{}','{}','{}','{}')"""

	# redshift_apollo_table = "rawdata_schema.raw_sas_apollo_metrics"

	# redshift_brazil_table = "rawdata_schema.raw_sas_brazil_metrics"

	# redshift_pipeline_target_table = "rawdata_schema.raw_sas_pipeline_target_metrics"

	# redshift_apollo_table_v2 = "rawdata_schema.sas_apollo_metrics_v2"

	# redshift_brazil_table_v2 = "rawdata_schema.sas_brazil_metrics_v2"

	# redshift_pipeline_target_table_v2 = "rawdata_schema.sas_pipeline_target_metrics_v2"

	# temp_redshift_apollo_table = "rawdata_schema.temp_sas_apollo_metrics"

	# temp_redshift_brazil_table = "rawdata_schema.temp_sas_brazil_metrics"

	# temp_redshift_pipeline_target_table = "rawdata_schema.temp_sas_pipeline_target_metrics"

	# rolled_redshift_apollo_table = "rawdata_schema.sas_apollo_metrics"

	# rolled_redshift_brazil_table = "rawdata_schema.sas_brazil_metrics"

	# rolled_redshift_pipeline_target_table = "rawdata_schema.sas_pipeline_target_metrics"

	program_name_apollo = 'APOLLO'

	program_name_brazil = 'BRAZIL'

	program_name_pipeline_targets = 'PIPELINE_TARGET'

	pipeline_target_function_flag = 'PIPELINE_TARGET_FUNCTION'

	BATCH_SIZE = 100

	query_for_getting_required_data = """insert into {} select distinct * from {}"""

	get_distinct_leader = "select distinct leader from relationship_schema.p0_clients_primary"

	insert_status_of_sas_run = """ 
	insert into aeetest.page0_plugin_run_status 
	values('ALLORG','ALLManager','','SAS','SAS Risks',0,'{}','{}','Weekly','{}');
	"""

	manager_to_manager_aggregate_mapping_query = """
	select relation_entity,relation_manager FROM 
	relationship_schema.entity_manager_relationship WHERE relation_entity IN (SELECT DISTINCT relation_manager FROM 
	relationship_schema.entity_manager_relationship WHERE relation_entity IN {0}) and 
	relation_rundate in (select max(relation_rundate) from relationship_schema.entity_manager_relationship)"""

	# roll_upto_manager_query = """
	# insert into {0}
	# select *,
	# 	   '{1}' AS rolled_upto
	# FROM {2}
	# WHERE owner_login IN (SELECT DISTINCT login_name
	# 				  FROM BOOKER.D_EMPLOYEE_DETAILS
	# 				  WHERE supervisor_login_name IN (SELECT DISTINCT relation_manager
	# 												  FROM relationship_schema.entity_manager_relationship
	# 												  WHERE relation_entity = '{1}'))
	# UNION all
	# SELECT *,
	# 	   '{1}' AS rolled_upto
	# FROM {2}
	# WHERE owner_login = '{1}'
	# """
	roll_upto_manager_query = """
		insert into {0}
		select 
			apollo_environment_name,
			apollo_environment_stage,
			pipeline_id,
			pipeline_name,
			owner,
			owner_login,
			consumers,
			children,
			software_count,
			visibility,
			host_count,
			days_non_compliant,
			days_non_compliant_recommendation,
			apollo_action,
			blocked_resources,
			associated_campaigns,
			metric_run_date,
			metric_year,
			classification,
			metric_snapshot,
			metric_snapshot_number,
			'{1}' as aggregate_roll_up_to,
			severity,
			risk_priority
		FROM {2}
		WHERE owner_login IN (SELECT DISTINCT login_name
						FROM BOOKER.D_EMPLOYEE_DETAILS
						WHERE supervisor_login_name IN (SELECT DISTINCT relation_manager
														FROM relationship_schema.entity_manager_relationship
														WHERE relation_entity = '{1}'))
		UNION all
		SELECT 
			apollo_environment_name,
			apollo_environment_stage,
			pipeline_id,
			pipeline_name,
			owner,
			owner_login,
			consumers,
			children,
			software_count,
			visibility,
			host_count,
			days_non_compliant,
			days_non_compliant_recommendation,
			apollo_action,
			blocked_resources,
			associated_campaigns,
			metric_run_date,
			metric_year,
			classification,
			metric_snapshot,
			metric_snapshot_number,
			'{1}' as aggregate_roll_up_to,
			severity,
			risk_priority
		FROM {2}
		WHERE owner_login = '{1}'
	"""

	aggregate_roll_upto_total_query ="""
	insert into {0} (
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'total' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from aeetest.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from aeetest.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_brazil_metrics) and aggregate_roll_up_to in
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))
	union all
	select pipeline_target_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,pipeline_target_action_type as action_type
	from aeetest.sas_pipeline_target_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_pipeline_target_metrics) and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}')))
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_rolled_upto,metric_year,metric_snapshot_number
	union all
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'total' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from aeetest.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from aeetest.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_brazil_metrics) and aggregate_roll_up_to in ('{1}')
	union all
	select pipeline_target_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,pipeline_target_action_type as action_type
	from aeetest.sas_pipeline_target_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_pipeline_target_metrics) and aggregate_roll_up_to in ('{1}'))
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_roll_up_to,metric_year,metric_snapshot_number)
	"""

	aggregate_roll_upto_near_sla_query = """
	insert into {0} (
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'75DaySASRisk' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from aeetest.sas_apollo_metrics where metric_run_date = (select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))  and 
		cast(days_non_compliant_recommendation as int) >= 20 and 
		cast(days_non_compliant_recommendation as int) < 30
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from aeetest.sas_brazil_metrics where metric_run_date = (select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}')) and 
		cast(days_non_compliant_recommendation as int) >= 20 and 
		cast(days_non_compliant_recommendation as int) < 30)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_rolled_upto,metric_year,metric_snapshot_number
	union all
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'75DaySASRisk' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from aeetest.sas_apollo_metrics where metric_run_date = (select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) >= 20 and 
		cast(days_non_compliant_recommendation as int) < 30
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from aeetest.sas_brazil_metrics where metric_run_date = (select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) >= 20 and 
		cast(days_non_compliant_recommendation as int) < 30)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_roll_up_to,metric_year,metric_snapshot_number)
	"""

	aggregate_roll_upto_recommendation_query = """
	insert into {0} (
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'recommendations' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from aeetest.sas_apollo_metrics where metric_run_date = (select max(metric_run_date) from aeetest.sas_apollo_metrics)
	and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))  and 
		cast(days_non_compliant_recommendation as int) > 0
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from aeetest.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}')) and 
		cast(days_non_compliant_recommendation as int) > 0)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_rolled_upto,metric_year,metric_snapshot_number
	union all
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'recommendations' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from aeetest.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) > 0
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from aeetest.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) > 0)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_roll_up_to,metric_year,metric_snapshot_number)
	"""

	aggregate_roll_upto_long_recommendation_query = """
	insert into {0} (
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'90DaySASRisk' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from aeetest.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in 
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}'))  and 
		cast(days_non_compliant_recommendation as int) > 30
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from aeetest.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_brazil_metrics) and aggregate_roll_up_to in
	(SELECT DISTINCT login_name
						  FROM BOOKER.D_EMPLOYEE_DETAILS
						  WHERE supervisor_login_name IN ('{1}')) and 
		cast(days_non_compliant_recommendation as int) > 30)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_rolled_upto,metric_year,metric_snapshot_number
	union all
	select count(risk_name) as risk_count,aggregate_roll_up_to,metric_run_date,
	metric_snapshot,metric_snapshot_number,'{1}' as aggregate_rolled_upto,metric_year,'90DaySASRisk' as metric_type 
	from (
	select apollo_environment_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,apollo_action as action_type
	from aeetest.sas_apollo_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_apollo_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) > 30
	union all
	select brazil_versionset_name as risk_name,aggregate_roll_up_to,metric_run_date,metric_snapshot,
	metric_snapshot_number,metric_year,days_non_compliant,days_non_compliant_recommendation,brazil_action_type as action_type
	 from aeetest.sas_brazil_metrics where metric_run_date = 
	(select max(metric_run_date) from aeetest.sas_brazil_metrics) and aggregate_roll_up_to in ('{1}')  and 
		cast(days_non_compliant_recommendation as int) > 30)
	group by aggregate_roll_up_to,metric_run_date,metric_snapshot,aggregate_roll_up_to,metric_year,metric_snapshot_number)
	"""

	'''
	Insert query to copy retrieve data from parent table to new table with Max(rundate)
	for Manual update of OE Risks by SE/Dev in Arik portal project.

	Master ticket - https://issues.amazon.com/issues/AEESE-3612
	'''

	sas_temp_apollo_insert_query = """
    INSERT INTO aeetest.sas_apollo_metrics_v2 (apollo_environment_name, apollo_environment_stage, pipeline_id,
    pipeline_name, owner, owner_login, consumers, children, software_count, visibility, host_count, days_non_compliant,
    days_non_compliant_recommendation, apollo_action, blocked_resources, associated_campaigns, metric_run_date, metric_year,
    classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to, severity, risk_priority) 
    SELECT apollo_environment_name, apollo_environment_stage, pipeline_id, pipeline_name, owner, owner_login, consumers,
    children, software_count, visibility, host_count, days_non_compliant, days_non_compliant_recommendation, apollo_action,
    blocked_resources, associated_campaigns, metric_run_date, metric_year, classification, metric_snapshot, metric_snapshot_number,
    aggregate_roll_up_to,severity, risk_priority FROM aeetest.sas_apollo_metrics
    WHERE metric_run_date = (SELECT MAX(metric_run_date) FROM aeetest.sas_apollo_metrics)
    """

	sas_temp_brazil_insert_query = """
	INSERT INTO aeetest.sas_brazil_metrics_v2 (brazil_versionset_name, pipeline_id, pipeline_name, owner, owner_login,
	software_count, days_non_compliant, days_non_compliant_recommendation, blocked_resource, brazil_action_type, associated_campaigns,
	metric_run_date, metric_year, classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to,severity, risk_priority) 
    SELECT brazil_versionset_name, pipeline_id, pipeline_name, owner, owner_login, software_count, days_non_compliant,
    days_non_compliant_recommendation, blocked_resource, brazil_action_type, associated_campaigns, metric_run_date, metric_year,
    classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to,severity, risk_priority FROM aeetest.sas_brazil_metrics
    WHERE metric_run_date = (SELECT MAX(metric_run_date) FROM aeetest.sas_brazil_metrics)
    """

	sas_temp_pipeline_target_insert_query = """
	INSERT INTO aeetest.sas_pipeline_target_metrics_v2 (pipeline_target_name, pipeline_id, pipeline_name, owner, owner_login,
	software_count, days_non_compliant, days_non_compliant_recommendation, blocked_resource, pipeline_target_action_type, associated_campaigns,
	metric_run_date, metric_year, classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to,severity, risk_priority) 
    SELECT pipeline_target_name, pipeline_id, pipeline_name, owner, owner_login, software_count, days_non_compliant,
    days_non_compliant_recommendation, blocked_resource, pipeline_target_action_type, associated_campaigns, metric_run_date, metric_year,
    classification, metric_snapshot, metric_snapshot_number, aggregate_roll_up_to,severity, risk_priority FROM aeetest.sas_pipeline_target_metrics
    WHERE metric_run_date = (SELECT MAX(metric_run_date) FROM aeetest.sas_pipeline_target_metrics)
    """
________
[ Fri Feb 21 09:48:02 2025 ] [ INFO ] Triggering SAS Plugin
[ Fri Feb 21 09:48:04 2025 ] [ INFO ] Established Connection to Redshift Database ... 
[ Fri Feb 21 09:48:04 2025 ] [ INFO ] Running Query ...
[ Fri Feb 21 09:48:05 2025 ] [ SUCCESS ] Execution successfull 
[ Fri Feb 21 09:48:05 2025 ] [ DEBUG ] Query result [['tapas'], ['russ'], ['anbhuvan'], ['posadan'], ['jglastra'], ['jags'], ['premald'], ['skaushik'], ['yanweng'], ['zheyinli'], ['agrim'], ['ronnie'], ['gupsweta'], ['sarmato'], ['hannahm'], ['danlloyd']]
[ Fri Feb 21 09:48:05 2025 ] [ INFO ]  Calling Manager To Manager Aggregate Mapping Query
[ Fri Feb 21 09:48:05 2025 ] [ INFO ] Running Query ...
[ Fri Feb 21 09:48:06 2025 ] [ SUCCESS ] Execution successfull 
[ Fri Feb 21 09:48:06 2025 ] [ [INFO] ] Starting processing for 10 managers
[ Fri Feb 21 09:48:06 2025 ] [ [INFO] ] Executing it for all managers
[ Fri Feb 21 09:48:06 2025 ] [ [UPDATE] ] 10 Managers left to process
[ Fri Feb 21 09:48:06 2025 ] [ WARNING ] Access denied for manager paditya: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:paditya]')>
[ Fri Feb 21 09:48:06 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:06 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:06 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:06 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:06 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:06 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:06 2025 ] [ [UPDATE] ] 9 Managers left to process
[ Fri Feb 21 09:48:07 2025 ] [ WARNING ] Access denied for manager mattwilt: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:mattwilt]')>
[ Fri Feb 21 09:48:07 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:07 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:07 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:07 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:07 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:07 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:07 2025 ] [ [UPDATE] ] 8 Managers left to process
[ Fri Feb 21 09:48:07 2025 ] [ WARNING ] Access denied for manager pssahdev: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:pssahdev]')>
[ Fri Feb 21 09:48:07 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:07 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:07 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:07 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:07 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:07 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:07 2025 ] [ [UPDATE] ] 7 Managers left to process
[ Fri Feb 21 09:48:08 2025 ] [ WARNING ] Access denied for manager singhalm: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:singhalm]')>
[ Fri Feb 21 09:48:08 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:08 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:08 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:08 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:08 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:08 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:08 2025 ] [ [UPDATE] ] 6 Managers left to process
[ Fri Feb 21 09:48:08 2025 ] [ WARNING ] Access denied for manager ibranko: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:ibranko]')>
[ Fri Feb 21 09:48:08 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:08 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:08 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:08 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:08 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:08 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:08 2025 ] [ [UPDATE] ] 5 Managers left to process
[ Fri Feb 21 09:48:09 2025 ] [ WARNING ] Access denied for manager krithi: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:krithi]')>
[ Fri Feb 21 09:48:09 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:09 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:09 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:09 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:09 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:09 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:09 2025 ] [ [UPDATE] ] 4 Managers left to process
[ Fri Feb 21 09:48:10 2025 ] [ WARNING ] Access denied for manager nitendv: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:nitendv]')>
[ Fri Feb 21 09:48:10 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:10 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:10 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:10 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:10 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:10 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:10 2025 ] [ [UPDATE] ] 3 Managers left to process
[ Fri Feb 21 09:48:10 2025 ] [ WARNING ] Access denied for manager ravponno: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:ravponno]')>
[ Fri Feb 21 09:48:10 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:10 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:10 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:10 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:10 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:10 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:10 2025 ] [ [UPDATE] ] 2 Managers left to process
[ Fri Feb 21 09:48:11 2025 ] [ WARNING ] Access denied for manager nishgarg: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:nishgarg]')>
[ Fri Feb 21 09:48:11 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:11 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:11 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:11 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:11 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:11 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:11 2025 ] [ [UPDATE] ] 1 Managers left to process
[ Fri Feb 21 09:48:11 2025 ] [ WARNING ] Access denied for manager kvkrant: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:kvkrant]')>
[ Fri Feb 21 09:48:11 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:11 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:11 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 09:48:11 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:11 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:11 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 09:48:11 2025 ] [ INFO ] File need to be uploaded in bucket aee-p0-metrics-raw-data with name P0_metrics_raw_data/SAS/SAS_Apollo_File.csv and file path is /apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Apollo_File.csv
[ Fri Feb 21 09:48:12 2025 ] [ INFO ] Established Connection to S3 File system
[ Fri Feb 21 09:48:12 2025 ] [ SUCCESS ] The raw data from the file /apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Apollo_File.csv is succesfully uploaded into S3 bucket aee-p0-metrics-raw-data/P0_metrics_raw_data/SAS/SAS_Apollo_File.csv 
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 09:48:12 2025 ] [ INFO ] Truncate table 'aeetest.temp_sas_apollo_metrics' completed successfully.
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 09:48:12 2025 ] [ INFO ] Truncate table 'aeetest.raw_sas_apollo_metrics' completed successfully.
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 09:48:12 2025 ] [ INFO ] Data updation to table 'aeetest.temp_sas_apollo_metrics' completed successfully.
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 09:48:12 2025 ] [ INFO ] Unique Data updation to table 'aeetest.raw_sas_apollo_metrics' completed successfully.
[ Fri Feb 21 09:48:12 2025 ] [ INFO ] File need to be uploaded in bucket aee-p0-metrics-raw-data with name P0_metrics_raw_data/SAS/SAS_Brazil_File.csv and file path is /apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Brazil_File.csv
[ Fri Feb 21 09:48:12 2025 ] [ INFO ] Established Connection to S3 File system
[ Fri Feb 21 09:48:12 2025 ] [ SUCCESS ] The raw data from the file /apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Brazil_File.csv is succesfully uploaded into S3 bucket aee-p0-metrics-raw-data/P0_metrics_raw_data/SAS/SAS_Brazil_File.csv 
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 09:48:12 2025 ] [ INFO ] Truncate table 'aeetest.temp_sas_brazil_metrics' completed successfully.
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 09:48:12 2025 ] [ INFO ] Truncate table 'aeetest.raw_sas_brazil_metrics' completed successfully.
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 09:48:12 2025 ] [ INFO ] Data updation to table 'aeetest.temp_sas_brazil_metrics' completed successfully.
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 09:48:12 2025 ] [ INFO ] Unique Data updation to table 'aeetest.raw_sas_brazil_metrics' completed successfully.
[ Fri Feb 21 09:48:12 2025 ] [ INFO ] File need to be uploaded in bucket aee-p0-metrics-raw-data with name P0_metrics_raw_data/SAS/SAS_Pipeline_Target_File.csv and file path is /apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Pipeline_Target_File.csv
[ Fri Feb 21 09:48:12 2025 ] [ INFO ] Established Connection to S3 File system
[ Fri Feb 21 09:48:13 2025 ] [ SUCCESS ] The raw data from the file /apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Pipeline_Target_File.csv is succesfully uploaded into S3 bucket aee-p0-metrics-raw-data/P0_metrics_raw_data/SAS/SAS_Pipeline_Target_File.csv 
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 09:48:13 2025 ] [ INFO ] Truncate table 'aeetest.temp_sas_pipeline_target_metrics' completed successfully.
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 09:48:13 2025 ] [ INFO ] Truncate table 'aeetest.raw_sas_pipeline_target_metrics' completed successfully.
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 09:48:13 2025 ] [ INFO ] Data updation to table 'aeetest.temp_sas_pipeline_target_metrics' completed successfully.
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 09:48:13 2025 ] [ INFO ] Unique Data updation to table 'aeetest.raw_sas_pipeline_target_metrics' completed successfully.
[ Fri Feb 21 09:48:13 2025 ] [ INFO ] Starting Rolling up of SAS metrics
[ Fri Feb 21 09:48:13 2025 ] [ [INFO] ] Starting rolling up of SAS metrics
[ Fri Feb 21 09:48:13 2025 ] [ ERROR ] Failed to execute the query 
                insert into aeetest.rolled_sas_apollo_metrics
                select 
                        apollo_environment_name,
                        apollo_environment_stage,
                        pipeline_id,
                        pipeline_name,
                        owner,
                        owner_login,
                        consumers,
                        children,
                        software_count,
                        visibility,
                        host_count,
                        days_non_compliant,
                        days_non_compliant_recommendation,
                        apollo_action,
                        blocked_resources,
                        associated_campaigns,
                        metric_run_date,
                        metric_year,
                        classification,
                        metric_snapshot,
                        metric_snapshot_number,
                        'paditya' as aggregate_roll_up_to,
                        severity,
                        risk_priority
                FROM aeetest.raw_sas_apollo_metrics
                WHERE owner_login IN (SELECT DISTINCT login_name
                                                FROM BOOKER.D_EMPLOYEE_DETAILS
                                                WHERE supervisor_login_name IN (SELECT DISTINCT relation_manager
                                                                                                                FROM relationship_schema.entity_manager_relationship
                                                                                                                WHERE relation_entity = 'paditya'))
                UNION all
                SELECT 
                        apollo_environment_name,
                        apollo_environment_stage,
                        pipeline_id,
                        pipeline_name,
                        owner,
                        owner_login,
                        consumers,
                        children,
                        software_count,
                        visibility,
                        host_count,
                        days_non_compliant,
                        days_non_compliant_recommendation,
                        apollo_action,
                        blocked_resources,
                        associated_campaigns,
                        metric_run_date,
                        metric_year,
                        classification,
                        metric_snapshot,
                        metric_snapshot_number,
                        'paditya' as aggregate_roll_up_to,
                        severity,
                        risk_priority
                FROM aeetest.raw_sas_apollo_metrics
                WHERE owner_login = 'paditya'
         with error ERROR:  column "metric_snapshot_number" does not exist in raw_sas_apollo_metrics

[ Fri Feb 21 09:48:13 2025 ] [ [DEBUG] ] Apollo Data rolled upto query for paditya manager result 1
[ Fri Feb 21 09:48:13 2025 ] [ ERROR ] Failed to execute the query 
                insert into aeetest.rolled_sas_brazil_metrics
                select 
                        apollo_environment_name,
                        apollo_environment_stage,
                        pipeline_id,
                        pipeline_name,
                        owner,
                        owner_login,
                        consumers,
                        children,
                        software_count,
                        visibility,
                        host_count,
                        days_non_compliant,
                        days_non_compliant_recommendation,
                        apollo_action,
                        blocked_resources,
                        associated_campaigns,
                        metric_run_date,
                        metric_year,
                        classification,
                        metric_snapshot,
                        metric_snapshot_number,
                        'paditya' as aggregate_roll_up_to,
                        severity,
                        risk_priority
                FROM aeetest.raw_sas_brazil_metrics
                WHERE owner_login IN (SELECT DISTINCT login_name
                                                FROM BOOKER.D_EMPLOYEE_DETAILS
                                                WHERE supervisor_login_name IN (SELECT DISTINCT relation_manager
                                                                                                                FROM relationship_schema.entity_manager_relationship
                                                                                                                WHERE relation_entity = 'paditya'))
                UNION all
                SELECT 
                        apollo_environment_name,
                        apollo_environment_stage,
                        pipeline_id,
                        pipeline_name,
                        owner,
                        owner_login,
                        consumers,
                        children,
                        software_count,
                        visibility,
                        host_count,
                        days_non_compliant,
                        days_non_compliant_recommendation,
                        apollo_action,
                        blocked_resources,
                        associated_campaigns,
                        metric_run_date,
                        metric_year,
                        classification,
                        metric_snapshot,
                        metric_snapshot_number,
                        'paditya' as aggregate_roll_up_to,
                        severity,
                        risk_priority
                FROM aeetest.raw_sas_brazil_metrics
                WHERE owner_login = 'paditya'
         with error ERROR:  current transaction is aborted, commands ignored until end of transaction block

[ Fri Feb 21 09:48:13 2025 ] [ [DEBUG] ] Brazil Data rolled upto query for paditya manager result 1
[ Fri Feb 21 09:48:13 2025 ] [ ERROR ] Failed to execute the query 
                insert into aeetest.rolled_sas_pipeline_target_metrics
                select 
                        apollo_environment_name,
                        apollo_environment_stage,
                        pipeline_id,
                        pipeline_name,
                        owner,
                        owner_login,
                        consumers,
                        children,
                        software_count,
                        visibility,
                        host_count,
                        days_non_compliant,
                        days_non_compliant_recommendation,
                        apollo_action,
                        blocked_resources,
                        associated_campaigns,
                        metric_run_date,
                        metric_year,
                        classification,
                        metric_snapshot,
                        metric_snapshot_number,
                        'paditya' as aggregate_roll_up_to,
                        severity,
                        risk_priority
                FROM aeetest.raw_sas_pipeline_target_metrics
                WHERE owner_login IN (SELECT DISTINCT login_name
                                                FROM BOOKER.D_EMPLOYEE_DETAILS
                                                WHERE supervisor_login_name IN (SELECT DISTINCT relation_manager
                                                                                                                FROM relationship_schema.entity_manager_relationship
                                                                                                                WHERE relation_entity = 'paditya'))
                UNION all
                SELECT 
                        apollo_environment_name,
                        apollo_environment_stage,
                        pipeline_id,
                        pipeline_name,
                        owner,
                        owner_login,
                        consumers,
                        children,
                        software_count,
                        visibility,
                        host_count,
                        days_non_compliant,
                        days_non_compliant_recommendation,
                        apollo_action,
                        blocked_resources,
                        associated_campaigns,
                        metric_run_date,
                        metric_year,
                        classification,
                        metric_snapshot,
                        metric_snapshot_number,
                        'paditya' as aggregate_roll_up_to,
                        severity,
                        risk_priority
                FROM aeetest.raw_sas_pipeline_target_metrics
                WHERE owner_login = 'paditya'
         with error ERROR:  current transaction is aborted, commands ignored until end of transaction block

[ Fri Feb 21 09:48:13 2025 ] [ [DEBUG] ] Pipeline Target Data rolled upto query for paditya manager result 1
[ Fri Feb 21 09:48:13 2025 ] [ ERROR ] Query failed to roll the data for manager paditya
[ Fri Feb 21 09:48:13 2025 ] [ INFO ] Connection Destroyed ...

------

Cedric: I see the error in the execution. The main issue is with the table structure and queries. Here are the fixes needed:

1. **Table Structure Error**
```sql
ERROR: column "metric_snapshot_number" does not exist in raw_sas_apollo_metrics
```

First, we need to create the test tables with correct structure. Create these tables:

```sql
-- Create Apollo test table
CREATE TABLE aeetest.raw_sas_apollo_metrics (
    apollo_environment_name VARCHAR(255),
    apollo_environment_stage VARCHAR(50),
    pipeline_id VARCHAR(100),
    pipeline_name VARCHAR(255),
    owner VARCHAR(255),
    owner_login VARCHAR(100),
    consumers INT,
    children INT,
    software_count INT,
    visibility VARCHAR(50),
    host_count INT,
    days_non_compliant INT,
    days_non_compliant_recommendation INT,
    apollo_action VARCHAR(100),
    blocked_resources VARCHAR(255),
    associated_campaigns VARCHAR(255),
    metric_run_date DATE,
    metric_year VARCHAR(4),
    classification VARCHAR(50),
    metric_snapshot VARCHAR(10),
    snapshot_number INT,
    severity VARCHAR(10),
    risk_priority INT
);

-- Create similar tables for Brazil and Pipeline
CREATE TABLE aeetest.raw_sas_brazil_metrics (
    -- Similar structure with Brazil-specific columns
);

CREATE TABLE aeetest.raw_sas_pipeline_target_metrics (
    -- Similar structure with Pipeline-specific columns
);

-- Create rolled tables with same structure
CREATE TABLE aeetest.rolled_sas_apollo_metrics (
    -- Same structure as raw_sas_apollo_metrics
    -- Plus aggregate_roll_up_to column
);
```

2. **Fix the roll_upto_manager_query**
```python
roll_upto_manager_query = """
    insert into {0}
    select 
        apollo_environment_name,
        apollo_environment_stage,
        pipeline_id,
        pipeline_name,
        owner,
        owner_login,
        consumers,
        children,
        software_count,
        visibility,
        host_count,
        days_non_compliant,
        days_non_compliant_recommendation,
        apollo_action,
        blocked_resources,
        associated_campaigns,
        metric_run_date,
        metric_year,
        classification,
        metric_snapshot,
        snapshot_number,  -- Changed from metric_snapshot_number
        '{1}' as aggregate_roll_up_to,
        severity,
        risk_priority
    FROM {2}
    WHERE owner_login IN (
        SELECT DISTINCT login_name
        FROM BOOKER.D_EMPLOYEE_DETAILS
        WHERE supervisor_login_name IN (
            SELECT DISTINCT relation_manager
            FROM relationship_schema.entity_manager_relationship
            WHERE relation_entity = '{1}'
        )
    )
    UNION all
    SELECT 
        apollo_environment_name,
        apollo_environment_stage,
        pipeline_id,
        pipeline_name,
        owner,
        owner_login,
        consumers,
        children,
        software_count,
        visibility,
        host_count,
        days_non_compliant,
        days_non_compliant_recommendation,
        apollo_action,
        blocked_resources,
        associated_campaigns,
        metric_run_date,
        metric_year,
        classification,
        metric_snapshot,
        snapshot_number,  -- Changed from metric_snapshot_number
        '{1}' as aggregate_roll_up_to,
        severity,
        risk_priority
    FROM {2}
    WHERE owner_login = '{1}'
"""
```

3. **Add Transaction Handling**
```python
def roll_up_metrics(self, distinct_managers):
    '''
    Function responsible for Rolling up the metrics by using Rawdata tables.
    '''
    Logger.printLogs("[INFO]","Starting rolling up of SAS metrics")

    for manager in distinct_managers:
        try:
            # Start transaction
            self.rs_upsert.execute_redshift_sql_query("BEGIN;")

            insert_apollo_result = self.rs_upsert.execute_redshift_sql_query(
                SAS_Config.roll_upto_manager_query.format(
                    SAS_Config.test_rolled_redshift_apollo_table,
                    manager,
                    SAS_Config.test_redshift_apollo_table
                )
            )

            insert_brazil_result = self.rs_upsert.execute_redshift_sql_query(
                SAS_Config.roll_upto_manager_query.format(
                    SAS_Config.test_rolled_redshift_brazil_table,
                    manager,
                    SAS_Config.test_redshift_brazil_table
                )
            )

            insert_pipeline_target_result = self.rs_upsert.execute_redshift_sql_query(
                SAS_Config.roll_upto_manager_query.format(
                    SAS_Config.test_rolled_redshift_pipeline_target_table,
                    manager,
                    SAS_Config.test_redshift_pipeline_target_table
                )
            )

            if (insert_apollo_result != 0 or insert_brazil_result != 0 or insert_pipeline_target_result != 0):
                self.rs_upsert.execute_redshift_sql_query("ROLLBACK;")
                Logger.printLogs("ERROR", f"Query failed to roll the data for manager {manager}")
                continue

            # Commit transaction
            self.rs_upsert.execute_redshift_sql_query("COMMIT;")

        except Exception as error:
            self.rs_upsert.execute_redshift_sql_query("ROLLBACK;")
            Logger.printLogs("ERROR", f"Rolling up metrics failed for manager {manager}: {error}")
            continue
```

Would you like me to provide the complete set of SQL statements to create all the required test tables?

------

venry: write show table command for every table I wanna see the structure including rawdata schema tables

CREATE TABLE aeetest.raw_sas_apollo_metrics AS SELECT * FROM rawdata_schema.raw_sas_apollo_metrics;
CREATE TABLE aeetest.raw_sas_brazil_metrics AS SELECT * FROM rawdata_schema.raw_sas_brazil_metrics;
CREATE TABLE aeetest.raw_sas_pipeline_target_metrics AS SELECT * FROM rawdata_schema.raw_sas_pipeline_target_metrics;
CREATE TABLE aeetest.sas_risk_summary AS SELECT * FROM aggregate_schema.sas_risk_summary;

CREATE TABLE aeetest.sas_apollo_metrics_v2 AS SELECT * FROM rawdata_schema.sas_apollo_metrics_v2;
CREATE TABLE aeetest.sas_brazil_metrics_v2 AS SELECT * FROM rawdata_schema.sas_brazil_metrics_v2;
CREATE TABLE aeetest.sas_pipeline_target_metrics_v2 AS SELECT * FROM rawdata_schema.sas_pipeline_target_metrics_v2;

CREATE TABLE aeetest.sas_apollo_metrics AS SELECT * FROM rawdata_schema.sas_apollo_metrics;
CREATE TABLE aeetest.sas_brazil_metrics AS SELECT * FROM rawdata_schema.sas_brazil_metrics;
CREATE TABLE aeetest.sas_pipeline_target_metrics AS SELECT * FROM rawdata_schema.sas_pipeline_target_metrics;
------------------------------------
CREATE TABLE aeetest.rolled_sas_apollo_metrics AS SELECT * FROM rawdata_schema.sas_apollo_metrics;
CREATE TABLE aeetest.rolled_sas_brazil_metrics AS SELECT * FROM rawdata_schema.sas_brazil_metrics;
CREATE TABLE aeetest.rolled_sas_pipeline_target_metrics AS SELECT * FROM rawdata_schema.sas_pipeline_target_metrics;

CREATE TABLE aeetest.temp_sas_apollo_metrics AS SELECT * FROM rawdata_schema.temp_sas_apollo_metrics;
CREATE TABLE aeetest.temp_sas_brazil_metrics AS SELECT * FROM rawdata_schema.temp_sas_brazil_metrics;
CREATE TABLE aeetest.temp_sas_pipeline_target_metrics AS SELECT * FROM rawdata_schema.temp_sas_pipeline_target_metrics;



------

Cedric: Here are the commands to show table structures for all tables:

```sql
-- Raw Data Schema Tables
SELECT column_name, data_type, character_maximum_length 
FROM information_schema.columns 
WHERE table_schema = 'rawdata_schema' 
AND table_name IN (
    'raw_sas_apollo_metrics',
    'raw_sas_brazil_metrics',
    'raw_sas_pipeline_target_metrics',
    'sas_apollo_metrics_v2',
    'sas_brazil_metrics_v2',
    'sas_pipeline_target_metrics_v2',
    'sas_apollo_metrics',
    'sas_brazil_metrics',
    'sas_pipeline_target_metrics',
    'temp_sas_apollo_metrics',
    'temp_sas_brazil_metrics',
    'temp_sas_pipeline_target_metrics'
)
ORDER BY table_name, ordinal_position;

-- Aggregate Schema Table
SELECT column_name, data_type, character_maximum_length 
FROM information_schema.columns 
WHERE table_schema = 'aggregate_schema' 
AND table_name = 'sas_risk_summary'
ORDER BY ordinal_position;

-- Test Schema Tables (After Creation)
SELECT column_name, data_type, character_maximum_length 
FROM information_schema.columns 
WHERE table_schema = 'aeetest' 
AND table_name IN (
    'raw_sas_apollo_metrics',
    'raw_sas_brazil_metrics',
    'raw_sas_pipeline_target_metrics',
    'sas_risk_summary',
    'sas_apollo_metrics_v2',
    'sas_brazil_metrics_v2',
    'sas_pipeline_target_metrics_v2',
    'sas_apollo_metrics',
    'sas_brazil_metrics',
    'sas_pipeline_target_metrics',
    'rolled_sas_apollo_metrics',
    'rolled_sas_brazil_metrics',
    'rolled_sas_pipeline_target_metrics',
    'temp_sas_apollo_metrics',
    'temp_sas_brazil_metrics',
    'temp_sas_pipeline_target_metrics'
)
ORDER BY table_name, ordinal_position;

-- Quick check for table existence and row counts
SELECT 
    schemaname,
    tablename,
    size,
    tbl_rows
FROM svv_table_info
WHERE schemaname IN ('rawdata_schema', 'aggregate_schema', 'aeetest')
AND tablename LIKE '%sas%'
ORDER BY schemaname, tablename;

-- Check columns specifically for metric_snapshot_number
SELECT 
    table_schema,
    table_name,
    column_name,
    data_type
FROM information_schema.columns
WHERE table_schema IN ('rawdata_schema', 'aggregate_schema', 'aeetest')
AND table_name LIKE '%sas%'
AND column_name IN ('metric_snapshot_number', 'snapshot_number')
ORDER BY table_schema, table_name;

-- Check for severity and risk_priority columns
SELECT 
    table_schema,
    table_name,
    column_name,
    data_type
FROM information_schema.columns
WHERE table_schema IN ('rawdata_schema', 'aggregate_schema', 'aeetest')
AND table_name LIKE '%sas%'
AND column_name IN ('severity', 'risk_priority')
ORDER BY table_schema, table_name;
```

After running these queries, we can:
1. Verify all tables were created correctly
2. Confirm column names and types match
3. Identify any missing columns
4. Ensure severity columns exist
5. Check for any discrepancies between schemas

Would you like me to provide example fixes for any specific table structures that don't match?

------

venry: for test schema table query:
column_name	data_type	character_maximum_length
apollo_environment_name	character varying	255
apollo_environment_stage	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
consumers	character varying	50
children	character varying	50
software_count	integer	
visibility	character varying	50
host_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
apollo_action	character varying	50
blocked_resources	character varying	255
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
snapshot_number	integer	
severity	character varying	10
risk_priority	integer	
brazil_versionset_name	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
software_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
blocked_resource	character varying	255
brazil_action_type	character varying	50
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
snapshot_number	integer	
severity	character varying	10
risk_priority	integer	
pipeline_target_name	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
software_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
blocked_resource	character varying	255
pipeline_target_action_type	character varying	50
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	
severity	character varying	10
risk_priority	integer	
apollo_environment_name	character varying	255
apollo_environment_stage	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
consumers	character varying	50
children	character varying	50
software_count	integer	
visibility	character varying	50
host_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
apollo_action	character varying	50
blocked_resources	character varying	255
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	
aggregate_roll_up_to	character varying	50
severity	character varying	10
risk_priority	integer	
brazil_versionset_name	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
software_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
blocked_resource	character varying	255
brazil_action_type	character varying	255
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	
aggregate_roll_up_to	character varying	50
severity	character varying	10
risk_priority	integer	
pipeline_target_name	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
software_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
blocked_resource	character varying	255
pipeline_target_action_type	character varying	50
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	
aggregate_roll_up_to	character varying	50
severity	character varying	10
risk_priority	integer	
apollo_environment_name	character varying	255
apollo_environment_stage	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
consumers	character varying	50
children	character varying	50
software_count	integer	
visibility	character varying	50
host_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
apollo_action	character varying	50
blocked_resources	character varying	255
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	
aggregate_roll_up_to	character varying	50
severity	character varying	10
risk_priority	integer	
apollo_environment_name	character varying	255
apollo_environment_stage	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
consumers	character varying	50
children	character varying	50
software_count	integer	
visibility	character varying	50
host_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
apollo_action	character varying	50
blocked_resources	character varying	255
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	
aggregate_roll_up_to	character varying	50
resolved_user_updated_date	character varying	1000
resolved_user	character varying	1000
severity	character varying	10
risk_priority	integer	
brazil_versionset_name	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
software_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
blocked_resource	character varying	255
brazil_action_type	character varying	255
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	
aggregate_roll_up_to	character varying	50
severity	character varying	10
risk_priority	integer	
brazil_versionset_name	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
software_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
blocked_resource	character varying	255
brazil_action_type	character varying	255
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	
aggregate_roll_up_to	character varying	50
resolved_user	character varying	1000
resolved_user_updated_date	character varying	1000
pipeline_target_name	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
software_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
blocked_resource	character varying	255
pipeline_target_action_type	character varying	50
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	
aggregate_roll_up_to	character varying	50
severity	character varying	10
risk_priority	integer	
pipeline_target_name	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
software_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
blocked_resource	character varying	255
pipeline_target_action_type	character varying	50
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	
aggregate_roll_up_to	character varying	50
resolved_user	character varying	1000
resolved_user_updated_date	character varying	1000
severity	character varying	10
risk_priority	integer	
risk_count	integer	
metric_rolled_up_to	character varying	50
metric_run_date	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	
aggregate_roll_up_to	character varying	50
metric_year	integer	
metric_type	character varying	50
severity	character varying	10
apollo_environment_name	character varying	255
apollo_environment_stage	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
consumers	character varying	50
children	character varying	50
software_count	integer	
visibility	character varying	50
host_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
apollo_action	character varying	50
blocked_resources	character varying	255
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
snapshot_number	integer	
severity	character varying	10
risk_priority	integer	
brazil_versionset_name	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
software_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
blocked_resource	character varying	255
brazil_action_type	character varying	50
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
snapshot_number	integer	
severity	character varying	10
risk_priority	integer	
pipeline_target_name	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
software_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
blocked_resource	character varying	255
pipeline_target_action_type	character varying	50
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	
severity	character varying	10
risk_priority	integer	

for rawdata schema 
column_name	data_type	character_maximum_length
apollo_environment_name	character varying	255
apollo_environment_stage	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
consumers	character varying	50
children	character varying	50
software_count	integer	
visibility	character varying	50
host_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
apollo_action	character varying	50
blocked_resources	character varying	255
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
snapshot_number	integer	
brazil_versionset_name	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
software_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
blocked_resource	character varying	255
brazil_action_type	character varying	50
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
snapshot_number	integer	
pipeline_target_name	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
software_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
blocked_resource	character varying	255
pipeline_target_action_type	character varying	50
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	
apollo_environment_name	character varying	255
apollo_environment_stage	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
consumers	character varying	50
children	character varying	50
software_count	integer	
visibility	character varying	50
host_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
apollo_action	character varying	50
blocked_resources	character varying	255
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	
aggregate_roll_up_to	character varying	50
apollo_environment_name	character varying	255
apollo_environment_stage	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
consumers	character varying	50
children	character varying	50
software_count	integer	
visibility	character varying	50
host_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
apollo_action	character varying	50
blocked_resources	character varying	255
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	
aggregate_roll_up_to	character varying	50
resolved_user_updated_date	character varying	1000
resolved_user	character varying	1000
brazil_versionset_name	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
software_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
blocked_resource	character varying	255
brazil_action_type	character varying	255
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	
aggregate_roll_up_to	character varying	50
brazil_versionset_name	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
software_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
blocked_resource	character varying	255
brazil_action_type	character varying	255
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	
aggregate_roll_up_to	character varying	50
resolved_user	character varying	1000
resolved_user_updated_date	character varying	1000
pipeline_target_name	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
software_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
blocked_resource	character varying	255
pipeline_target_action_type	character varying	50
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	
aggregate_roll_up_to	character varying	50
pipeline_target_name	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
software_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
blocked_resource	character varying	255
pipeline_target_action_type	character varying	50
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	
aggregate_roll_up_to	character varying	50
resolved_user	character varying	1000
resolved_user_updated_date	character varying	1000
apollo_environment_name	character varying	255
apollo_environment_stage	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
consumers	character varying	50
children	character varying	50
software_count	integer	
visibility	character varying	50
host_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
apollo_action	character varying	50
blocked_resources	character varying	255
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
snapshot_number	integer	
brazil_versionset_name	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
software_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
blocked_resource	character varying	255
brazil_action_type	character varying	50
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
snapshot_number	integer	
pipeline_target_name	character varying	255
pipeline_id	character varying	255
pipeline_name	character varying	255
owner	character varying	50
owner_login	character varying	50
software_count	integer	
days_non_compliant	integer	
days_non_compliant_recommendation	integer	
blocked_resource	character varying	255
pipeline_target_action_type	character varying	50
associated_campaigns	character varying	255
metric_run_date	character varying	50
metric_year	integer	
classification	character varying	50
metric_snapshot	character varying	50
metric_snapshot_number	integer	

------

Cedric: I notice a few key differences between the test schema and rawdata schema tables:

1. **Column Name Inconsistency**:
```sql
-- Some tables use 'snapshot_number' while others use 'metric_snapshot_number'
-- Need to make this consistent across all tables
```

2. **Missing Severity Columns in rawdata**:
```sql
-- Test tables have these additional columns that rawdata doesn't:
severity VARCHAR(10)
risk_priority INTEGER
```

3. **Fix Required**:
```sql
-- Add missing columns to test tables
ALTER TABLE aeetest.raw_sas_apollo_metrics 
RENAME COLUMN metric_snapshot_number TO snapshot_number;

ALTER TABLE aeetest.raw_sas_brazil_metrics 
RENAME COLUMN metric_snapshot_number TO snapshot_number;

-- Add severity columns to raw tables
ALTER TABLE rawdata_schema.raw_sas_apollo_metrics 
ADD COLUMN severity VARCHAR(10),
ADD COLUMN risk_priority INTEGER;

ALTER TABLE rawdata_schema.raw_sas_brazil_metrics 
ADD COLUMN severity VARCHAR(10),
ADD COLUMN risk_priority INTEGER;

ALTER TABLE rawdata_schema.raw_sas_pipeline_target_metrics 
ADD COLUMN severity VARCHAR(10),
ADD COLUMN risk_priority INTEGER;
```

4. **Update Roll-up Query**:
```python
roll_upto_manager_query = """
    insert into {0}
    select 
        apollo_environment_name,
        apollo_environment_stage,
        pipeline_id,
        pipeline_name,
        owner,
        owner_login,
        consumers,
        children,
        software_count,
        visibility,
        host_count,
        days_non_compliant,
        days_non_compliant_recommendation,
        apollo_action,
        blocked_resources,
        associated_campaigns,
        metric_run_date,
        metric_year,
        classification,
        metric_snapshot,
        snapshot_number,  # Use consistent column name
        '{1}' as aggregate_roll_up_to,
        severity,
        risk_priority
    FROM {2}
    WHERE owner_login IN (...)
"""
```

5. **File Writing Format Update**:
```python
def get_apollo_data_and_write_in_file(self, data, file, metric_snapshot):
    # ...
    file.write("{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\n".format(
        env_name,
        stage_name,
        pipeline_id,
        pipeline_name,
        owner,
        owner_login,
        consumer_count,
        children_count,
        risk_count,
        visibility,
        host_count,
        oldest_risk_age,
        oldest_recommendation_age,
        action_required,
        blocked_resources,
        associated_campaigns,
        current_date,
        current_year,
        classification,
        metric_snapshot,
        snapshot_number,  # Use consistent name
        severity,
        risk_priority
    ))
```

Would you like me to provide the complete set of ALTER TABLE statements to make all tables consistent?

------

venry: what is the difference 

------

Cedric: Let me explain the difference between `snapshot_number` and `metric_snapshot_number`:

```sql
-- Example data:
metric_snapshot  | snapshot_number  | metric_snapshot_number
-----------------|-----------------|----------------------
WoW             | 8               | 8
MoM             | 8               | 8
```

1. **Current Inconsistency in Tables**:
```sql
-- Some tables use:
snapshot_number INTEGER

-- Other tables use:
metric_snapshot_number INTEGER
```

2. **Where They're Used**:
```python
# In File Writing:
current_date = date.today()
snapshot_number = current_date.isocalendar()[1]  # Week number of the year

# In Queries:
"SELECT ... metric_snapshot_number ..."  # Some queries
"SELECT ... snapshot_number ..."         # Other queries
```

3. **How It's Generated**:
```python
# Both are generated the same way:
def get_week_number(self):
    return date.today().isocalendar()[1]

# Used in file writing as:
snapshot_number = current_date.isocalendar()[1]
# OR
metric_snapshot_number = current_date.isocalendar()[1]
```

4. **They Represent the Same Thing**: 
- Both represent the week number of the year
- Both are used for tracking metrics over time
- They're just named differently in different places

5. **Your Tables Currently**:
```sql
-- Raw Tables use:
snapshot_number INTEGER

-- Aggregation Tables use:
metric_snapshot_number INTEGER
```

To maintain consistency, we should:
1. Use `metric_snapshot_number` everywhere
2. Rename `snapshot_number` columns to `metric_snapshot_number`
3. Update all queries and code to use `metric_snapshot_number`

This will make the code more consistent and easier to maintain. Would you like me to provide the specific rename statements for your tables?

------

venry: how did it work with previously with rawdata schema tables I've copied aeetest tables from rawdata tables

------

Cedric: Let's check how it worked previously:

1. **Check Data Flow**:
```sql
-- Check rawdata schema tables
SELECT metric_snapshot, snapshot_number, metric_snapshot_number
FROM rawdata_schema.raw_sas_apollo_metrics
LIMIT 5;

-- Check test tables we created
SELECT metric_snapshot, snapshot_number, metric_snapshot_number
FROM aeetest.raw_sas_apollo_metrics
LIMIT 5;
```

2. **Check Original Code Structure**:
```python
# Original file writing had:
file.write("{},{},{},... {},{}\n".format(
    # ... other fields ...,
    metric_snapshot,      # WoW/MoM
    snapshot_number      # Week number
))

# But when we copied tables:
CREATE TABLE aeetest.raw_sas_apollo_metrics 
AS SELECT * FROM rawdata_schema.raw_sas_apollo_metrics;
```

The key point here is that we copied the structure from rawdata schema, so we should:

1. **Check Original Structure**:
```sql
-- Look at the original table structure
SELECT column_name, data_type 
FROM information_schema.columns 
WHERE table_schema = 'rawdata_schema' 
AND table_name = 'raw_sas_apollo_metrics'
ORDER BY ordinal_position;
```

2. **Use Same Column Names**:
```python
# If rawdata schema uses snapshot_number, we should use:
file.write("{},{},{},... {},{}\n".format(
    # ... other fields ...,
    metric_snapshot,
    snapshot_number,     # Keep this as snapshot_number if that's what rawdata uses
    severity,
    risk_priority
))
```

3. **Match Original Queries**:
```sql
# If original queries use snapshot_number, keep using that
roll_upto_manager_query = """
    ... snapshot_number ...  # Use whatever column name rawdata schema uses
"""
```

Would you like me to:
1. Check the actual column names in rawdata schema?
2. Show how to modify the code to match rawdata schema exactly?
3. Help verify if we have the right column names in our test tables?

This way we can ensure we're maintaining compatibility with the existing system.

------

venry: select * from rawdata_schema.raw_sas_apollo_metrics limit 5

apollo_environment_name	apollo_environment_stage	pipeline_id	pipeline_name	owner	owner_login	consumers	children	software_count	visibility	host_count	days_non_compliant	days_non_compliant_recommendation	apollo_action	blocked_resources	associated_campaigns	metric_run_date	metric_year	classification	metric_snapshot	snapshot_number
GlidePlatformHorizonte/EU/InAppBilling:Gamma	Gamma	None	None	Ravindran Rethinam	raviret	0	0	1	Internal	1	360	-1	APOLLO_NO_ACTION	Jetty:8.1.12	CDO Blocked Software Campaign - Jan 2022	2025-02-17	2025	None	WoW	8
TPSGenerator/SSA/FE:Gamma	Gamma	None	None	Mashkoor Alam	alammash	0	0	5	Internal	1	88	-1	APOLLO_NO_ACTION	Netty:3.10	CDO Blocked Software Campaign - Sep 2021	2025-02-17	2025	None	WoW	8
TPSGenerator/ATSTPSGeneratorControllerEU:Prod	Prod	None	None	Vidhya Prakash Sivashanmugam	vsivasha	0	0	5	Internal	1	264	208	APOLLO_CAN_SYNC	SnakeYAML:1.16	CDO Blocked Software Campaign - Jan 2022	2025-02-17	2025	None	WoW	8
GlidePlatformHorizonte/NA/OneBox:Prod	Prod	127929	GlidePlatform-master	Ravindran Rethinam	raviret	0	0	1	Internal	2	1130	-1	APOLLO_NO_ACTION	Jetty:8.1.12	CDO Blocked Software Campaign - Jan 2022	2025-02-17	2025	None	WoW	8
KindleConversionDevUtils:Alpha	Alpha	None	None	Arunkumar Subramanian	sznaru	0	0	2	Internal	20	260	-1	APOLLO_NO_ACTION	JakartaCommons-beanutils:1.0	CDO Blocked Software Campaign - Jan 2022	2025-02-17	2025	None	WoW	8

select * from aeetest.raw_sas_apollo_metrics limit 5

apollo_environment_name	apollo_environment_stage	pipeline_id	pipeline_name	owner	owner_login	consumers	children	software_count	visibility	host_count	days_non_compliant	days_non_compliant_recommendation	apollo_action	blocked_resources	associated_campaigns	metric_run_date	metric_year	classification	metric_snapshot	snapshot_number	severity	risk_priority
GlidePlatformHorizonte/EU/InAppBilling:Gamma	Gamma	None	None	Ravindran Rethinam	raviret	0	0	1	Internal	1	360	-1	APOLLO_NO_ACTION	Jetty:8.1.12	CDO Blocked Software Campaign - Jan 2022	2025-02-17	2025	None	WoW	8
TPSGenerator/SSA/FE:Gamma	Gamma	None	None	Mashkoor Alam	alammash	0	0	5	Internal	1	88	-1	APOLLO_NO_ACTION	Netty:3.10	CDO Blocked Software Campaign - Sep 2021	2025-02-17	2025	None	WoW	8
TPSGenerator/ATSTPSGeneratorControllerEU:Prod	Prod	None	None	Vidhya Prakash Sivashanmugam	vsivasha	0	0	5	Internal	1	264	208	APOLLO_CAN_SYNC	SnakeYAML:1.16	CDO Blocked Software Campaign - Jan 2022	2025-02-17	2025	None	WoW	8
GlidePlatformHorizonte/NA/OneBox:Prod	Prod	127929	GlidePlatform-master	Ravindran Rethinam	raviret	0	0	1	Internal	2	1130	-1	APOLLO_NO_ACTION	Jetty:8.1.12	CDO Blocked Software Campaign - Jan 2022	2025-02-17	2025	None	WoW	8
KindleConversionDevUtils:Alpha	Alpha	None	None	Arunkumar Subramanian	sznaru	0	0	2	Internal	20	260	-1	APOLLO_NO_ACTION	JakartaCommons-beanutils:1.0	CDO Blocked Software Campaign - Jan 2022	2025-02-17	2025	None	WoW	8

------

Cedric: I see the difference now:

1. **rawdata_schema.raw_sas_apollo_metrics** has:
```sql
metric_snapshot  | snapshot_number
----------------|----------------
WoW            | 8
```

2. **aeetest.raw_sas_apollo_metrics** has:
```sql
metric_snapshot  | snapshot_number | severity | risk_priority
----------------|----------------|-----------|---------------
WoW            | 8              | null      | null
```

The only difference is that our test table has the additional columns:
- severity
- risk_priority

So we should:

1. Keep using `snapshot_number` (not metric_snapshot_number) in our code:
```python
def get_apollo_data_and_write_in_file(self, data, file, metric_snapshot):
    # ...
    current_date = date.today()
    snapshot_number = current_date.isocalendar()[1]  # Use snapshot_number
    # ...
    file.write("{},{},{},... {},{},{},{}\n".format(
        # ... other fields ...,
        metric_snapshot,
        snapshot_number,    # Keep as snapshot_number
        severity,          # New field
        risk_priority     # New field
    ))
```

2. Update the roll_up_manager_query:
```python
roll_upto_manager_query = """
    insert into {0}
    select 
        # ... other fields ...,
        metric_snapshot,
        snapshot_number,    # Keep as snapshot_number
        severity,          # New field
        risk_priority     # New field
    FROM {2}
    WHERE owner_login IN (...)
"""
```

3. No need to rename any columns - just use them as they are in the original schema plus our new severity columns.

This way we maintain compatibility with the existing system while adding our new severity functionality. Would you like me to show how to update any specific queries or functions?

------

venry: [ Fri Feb 21 10:16:45 2025 ] [ INFO ] Triggering SAS Plugin
[ Fri Feb 21 10:16:47 2025 ] [ INFO ] Established Connection to Redshift Database ... 
[ Fri Feb 21 10:16:47 2025 ] [ INFO ] Running Query ...
[ Fri Feb 21 10:16:48 2025 ] [ SUCCESS ] Execution successfull 
[ Fri Feb 21 10:16:48 2025 ] [ DEBUG ] Query result [['tapas'], ['russ'], ['anbhuvan'], ['posadan'], ['jglastra'], ['jags'], ['premald'], ['skaushik'], ['yanweng'], ['zheyinli'], ['agrim'], ['ronnie'], ['gupsweta'], ['sarmato'], ['hannahm'], ['danlloyd']]
[ Fri Feb 21 10:16:48 2025 ] [ INFO ]  Calling Manager To Manager Aggregate Mapping Query
[ Fri Feb 21 10:16:48 2025 ] [ INFO ] Running Query ...
[ Fri Feb 21 10:16:48 2025 ] [ SUCCESS ] Execution successfull 
[ Fri Feb 21 10:16:48 2025 ] [ [INFO] ] Starting processing for 10 managers
[ Fri Feb 21 10:16:48 2025 ] [ [INFO] ] Executing it for all managers
[ Fri Feb 21 10:16:48 2025 ] [ [UPDATE] ] 10 Managers left to process
[ Fri Feb 21 10:16:49 2025 ] [ WARNING ] Access denied for manager singhalm: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:singhalm]')>
[ Fri Feb 21 10:16:49 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:16:49 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:16:49 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:16:49 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:16:49 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:16:49 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:16:49 2025 ] [ [UPDATE] ] 9 Managers left to process
[ Fri Feb 21 10:16:49 2025 ] [ WARNING ] Access denied for manager mattwilt: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:mattwilt]')>
[ Fri Feb 21 10:16:49 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:16:49 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:16:49 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:16:49 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:16:49 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:16:49 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:16:49 2025 ] [ [UPDATE] ] 8 Managers left to process
[ Fri Feb 21 10:16:50 2025 ] [ WARNING ] Access denied for manager paditya: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:paditya]')>
[ Fri Feb 21 10:16:50 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:16:50 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:16:50 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:16:50 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:16:50 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:16:50 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:16:50 2025 ] [ [UPDATE] ] 7 Managers left to process
[ Fri Feb 21 10:16:51 2025 ] [ WARNING ] Access denied for manager pssahdev: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:pssahdev]')>
[ Fri Feb 21 10:16:51 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:16:51 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:16:51 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:16:51 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:16:51 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:16:51 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:16:51 2025 ] [ [UPDATE] ] 6 Managers left to process
[ Fri Feb 21 10:16:51 2025 ] [ WARNING ] Access denied for manager krithi: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:krithi]')>
[ Fri Feb 21 10:16:51 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:16:51 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:16:51 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:16:51 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:16:51 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:16:51 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:16:51 2025 ] [ [UPDATE] ] 5 Managers left to process
[ Fri Feb 21 10:16:52 2025 ] [ EXCEPTION ] ThrottlingException occurred, Sleeping! <ThrottlingException(message='Rate exceeded')>
[ Fri Feb 21 10:17:07 2025 ] [ WARNING ] Access denied for manager ibranko: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:ibranko]')>
[ Fri Feb 21 10:17:07 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:17:07 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:17:07 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:17:07 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:17:07 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:17:07 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:17:07 2025 ] [ [UPDATE] ] 4 Managers left to process
[ Fri Feb 21 10:17:08 2025 ] [ WARNING ] Access denied for manager nishgarg: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:nishgarg]')>
[ Fri Feb 21 10:17:08 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:17:08 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:17:08 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:17:08 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:17:08 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:17:08 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:17:08 2025 ] [ [UPDATE] ] 3 Managers left to process
[ Fri Feb 21 10:17:08 2025 ] [ WARNING ] Access denied for manager ravponno: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:ravponno]')>
[ Fri Feb 21 10:17:08 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:17:08 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:17:08 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:17:08 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:17:08 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:17:08 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:17:08 2025 ] [ [UPDATE] ] 2 Managers left to process
[ Fri Feb 21 10:17:09 2025 ] [ WARNING ] Access denied for manager nitendv: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:nitendv]')>
[ Fri Feb 21 10:17:09 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:17:09 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:17:09 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:17:09 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:17:09 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:17:09 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:17:09 2025 ] [ [UPDATE] ] 1 Managers left to process
[ Fri Feb 21 10:17:09 2025 ] [ WARNING ] Access denied for manager kvkrant: <UnauthorizedUserException(message='checkAuthorization: [aggregateId=AWSAccount:899085032812] does not have permission to access data for [aggregateId=Person:kvkrant]')>
[ Fri Feb 21 10:17:09 2025 ] [ [INFO] ] No Apollo SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:17:09 2025 ] [ [INFO] ] No Brazil version set SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:17:09 2025 ] [ [INFO] ] No Pipeline target SAS risk found ! 'NoneType' object is not subscriptable
[ Fri Feb 21 10:17:09 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:17:09 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:17:09 2025 ] [ [INFO] ] Inside Call Batch Wrapper function: Total Batches required 1
[ Fri Feb 21 10:17:09 2025 ] [ INFO ] File need to be uploaded in bucket aee-p0-metrics-raw-data with name P0_metrics_raw_data/SAS/SAS_Apollo_File.csv and file path is /apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Apollo_File.csv
[ Fri Feb 21 10:17:10 2025 ] [ INFO ] Established Connection to S3 File system
[ Fri Feb 21 10:17:10 2025 ] [ SUCCESS ] The raw data from the file /apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Apollo_File.csv is succesfully uploaded into S3 bucket aee-p0-metrics-raw-data/P0_metrics_raw_data/SAS/SAS_Apollo_File.csv 
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 10:17:10 2025 ] [ INFO ] Truncate table 'aeetest.temp_sas_apollo_metrics' completed successfully.
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 10:17:10 2025 ] [ INFO ] Truncate table 'aeetest.raw_sas_apollo_metrics' completed successfully.
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 10:17:10 2025 ] [ INFO ] Data updation to table 'aeetest.temp_sas_apollo_metrics' completed successfully.
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 10:17:10 2025 ] [ INFO ] Unique Data updation to table 'aeetest.raw_sas_apollo_metrics' completed successfully.
[ Fri Feb 21 10:17:10 2025 ] [ INFO ] File need to be uploaded in bucket aee-p0-metrics-raw-data with name P0_metrics_raw_data/SAS/SAS_Brazil_File.csv and file path is /apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Brazil_File.csv
[ Fri Feb 21 10:17:10 2025 ] [ INFO ] Established Connection to S3 File system
[ Fri Feb 21 10:17:11 2025 ] [ SUCCESS ] The raw data from the file /apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Brazil_File.csv is succesfully uploaded into S3 bucket aee-p0-metrics-raw-data/P0_metrics_raw_data/SAS/SAS_Brazil_File.csv 
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 10:17:11 2025 ] [ INFO ] Truncate table 'aeetest.temp_sas_brazil_metrics' completed successfully.
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 10:17:11 2025 ] [ INFO ] Truncate table 'aeetest.raw_sas_brazil_metrics' completed successfully.
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 10:17:11 2025 ] [ INFO ] Data updation to table 'aeetest.temp_sas_brazil_metrics' completed successfully.
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 10:17:11 2025 ] [ INFO ] Unique Data updation to table 'aeetest.raw_sas_brazil_metrics' completed successfully.
[ Fri Feb 21 10:17:11 2025 ] [ INFO ] File need to be uploaded in bucket aee-p0-metrics-raw-data with name P0_metrics_raw_data/SAS/SAS_Pipeline_Target_File.csv and file path is /apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Pipeline_Target_File.csv
[ Fri Feb 21 10:17:11 2025 ] [ INFO ] Established Connection to S3 File system
[ Fri Feb 21 10:17:11 2025 ] [ SUCCESS ] The raw data from the file /apollo/env/AEE-OE-Data-Collector-Aggregator/var/SAS_Pipeline_Target_File.csv is succesfully uploaded into S3 bucket aee-p0-metrics-raw-data/P0_metrics_raw_data/SAS/SAS_Pipeline_Target_File.csv 
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 10:17:11 2025 ] [ INFO ] Truncate table 'aeetest.temp_sas_pipeline_target_metrics' completed successfully.
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 10:17:11 2025 ] [ INFO ] Truncate table 'aeetest.raw_sas_pipeline_target_metrics' completed successfully.
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 10:17:11 2025 ] [ INFO ] Data updation to table 'aeetest.temp_sas_pipeline_target_metrics' completed successfully.
sh: /apollo/env/AEE-OE-Data-Collector-Aggregator/bin/python: No such file or directory
[ Fri Feb 21 10:17:11 2025 ] [ INFO ] Unique Data updation to table 'aeetest.raw_sas_pipeline_target_metrics' completed successfully.
[ Fri Feb 21 10:17:11 2025 ] [ INFO ] Starting Rolling up of SAS metrics
[ Fri Feb 21 10:17:11 2025 ] [ [INFO] ] Starting rolling up of SAS metrics
[ Fri Feb 21 10:17:26 2025 ] [ SUCCESS ] Execution successfull 
[ Fri Feb 21 10:17:26 2025 ] [ [DEBUG] ] Apollo Data rolled upto query for singhalm manager result 0
[ Fri Feb 21 10:17:26 2025 ] [ ERROR ] Failed to execute the query 
                insert into aeetest.rolled_sas_brazil_metrics
                select 
                        apollo_environment_name,
                        apollo_environment_stage,
                        pipeline_id,
                        pipeline_name,
                        owner,
                        owner_login,
                        consumers,
                        children,
                        software_count,
                        visibility,
                        host_count,
                        days_non_compliant,
                        days_non_compliant_recommendation,
                        apollo_action,
                        blocked_resources,
                        associated_campaigns,
                        metric_run_date,
                        metric_year,
                        classification,
                        metric_snapshot,
                        snapshot_number,
                        'singhalm' as aggregate_roll_up_to,
                        severity,
                        risk_priority
                FROM aeetest.raw_sas_brazil_metrics
                WHERE owner_login IN (SELECT DISTINCT login_name
                                                FROM BOOKER.D_EMPLOYEE_DETAILS
                                                WHERE supervisor_login_name IN (SELECT DISTINCT relation_manager
                                                                                                                FROM relationship_schema.entity_manager_relationship
                                                                                                                WHERE relation_entity = 'singhalm'))
                UNION all
                SELECT 
                        apollo_environment_name,
                        apollo_environment_stage,
                        pipeline_id,
                        pipeline_name,
                        owner,
                        owner_login,
                        consumers,
                        children,
                        software_count,
                        visibility,
                        host_count,
                        days_non_compliant,
                        days_non_compliant_recommendation,
                        apollo_action,
                        blocked_resources,
                        associated_campaigns,
                        metric_run_date,
                        metric_year,
                        classification,
                        metric_snapshot,
                        snapshot_number,
                        'singhalm' as aggregate_roll_up_to,
                        severity,
                        risk_priority
                FROM aeetest.raw_sas_brazil_metrics
                WHERE owner_login = 'singhalm'
         with error ERROR:  column "apollo_environment_name" does not exist in raw_sas_brazil_metrics

[ Fri Feb 21 10:17:26 2025 ] [ [DEBUG] ] Brazil Data rolled upto query for singhalm manager result 1
[ Fri Feb 21 10:17:26 2025 ] [ ERROR ] Failed to execute the query 
                insert into aeetest.rolled_sas_pipeline_target_metrics
                select 
                        apollo_environment_name,
                        apollo_environment_stage,
                        pipeline_id,
                        pipeline_name,
                        owner,
                        owner_login,
                        consumers,
                        children,
                        software_count,
                        visibility,
                        host_count,
                        days_non_compliant,
                        days_non_compliant_recommendation,
                        apollo_action,
                        blocked_resources,
                        associated_campaigns,
                        metric_run_date,
                        metric_year,
                        classification,
                        metric_snapshot,
                        snapshot_number,
                        'singhalm' as aggregate_roll_up_to,
                        severity,
                        risk_priority
                FROM aeetest.raw_sas_pipeline_target_metrics
                WHERE owner_login IN (SELECT DISTINCT login_name
                                                FROM BOOKER.D_EMPLOYEE_DETAILS
                                                WHERE supervisor_login_name IN (SELECT DISTINCT relation_manager
                                                                                                                FROM relationship_schema.entity_manager_relationship
                                                                                                                WHERE relation_entity = 'singhalm'))
                UNION all
                SELECT 
                        apollo_environment_name,
                        apollo_environment_stage,
                        pipeline_id,
                        pipeline_name,
                        owner,
                        owner_login,
                        consumers,
                        children,
                        software_count,
                        visibility,
                        host_count,
                        days_non_compliant,
                        days_non_compliant_recommendation,
                        apollo_action,
                        blocked_resources,
                        associated_campaigns,
                        metric_run_date,
                        metric_year,
                        classification,
                        metric_snapshot,
                        snapshot_number,
                        'singhalm' as aggregate_roll_up_to,
                        severity,
                        risk_priority
                FROM aeetest.raw_sas_pipeline_target_metrics
                WHERE owner_login = 'singhalm'
         with error ERROR:  current transaction is aborted, commands ignored until end of transaction block

[ Fri Feb 21 10:17:26 2025 ] [ [DEBUG] ] Pipeline Target Data rolled upto query for singhalm manager result 1
[ Fri Feb 21 10:17:26 2025 ] [ ERROR ] Query failed to roll the data for manager singhalm
[ Fri Feb 21 10:17:26 2025 ] [ INFO ] Connection Destroyed ...

------

Cedric: I see the issue. The roll-up query is using Apollo's column structure for Brazil and Pipeline tables, but they have different column names. Let's fix the roll_upto_manager_query to handle each table type correctly:

```python
def roll_up_metrics(self, distinct_managers):
    '''
    Function responsible for Rolling up the metrics by using Rawdata tables.
    '''
    Logger.printLogs("[INFO]","Starting rolling up of SAS metrics")

    for manager in distinct_managers:
        try:
            # Apollo Roll-up Query
            apollo_query = """
            insert into aeetest.rolled_sas_apollo_metrics
            select 
                apollo_environment_name,
                apollo_environment_stage,
                pipeline_id,
                pipeline_name,
                owner,
                owner_login,
                consumers,
                children,
                software_count,
                visibility,
                host_count,
                days_non_compliant,
                days_non_compliant_recommendation,
                apollo_action,
                blocked_resources,
                associated_campaigns,
                metric_run_date,
                metric_year,
                classification,
                metric_snapshot,
                snapshot_number,
                '{0}' as aggregate_roll_up_to,
                severity,
                risk_priority
            FROM aeetest.raw_sas_apollo_metrics
            WHERE owner_login IN (
                SELECT DISTINCT login_name
                FROM BOOKER.D_EMPLOYEE_DETAILS
                WHERE supervisor_login_name IN (
                    SELECT DISTINCT relation_manager
                    FROM relationship_schema.entity_manager_relationship
                    WHERE relation_entity = '{0}'
                )
            )
            UNION all
            SELECT *,
                '{0}' as aggregate_roll_up_to
            FROM aeetest.raw_sas_apollo_metrics
            WHERE owner_login = '{0}'
            """

            # Brazil Roll-up Query
            brazil_query = """
            insert into aeetest.rolled_sas_brazil_metrics
            select 
                brazil_versionset_name,
                pipeline_id,
                pipeline_name,
                owner,
                owner_login,
                software_count,
                days_non_compliant,
                days_non_compliant_recommendation,
                blocked_resource,
                brazil_action_type,
                associated_campaigns,
                metric_run_date,
                metric_year,
                classification,
                metric_snapshot,
                snapshot_number,
                '{0}' as aggregate_roll_up_to,
                severity,
                risk_priority
            FROM aeetest.raw_sas_brazil_metrics
            WHERE owner_login IN (
                SELECT DISTINCT login_name
                FROM BOOKER.D_EMPLOYEE_DETAILS
                WHERE supervisor_login_name IN (
                    SELECT DISTINCT relation_manager
                    FROM relationship_schema.entity_manager_relationship
                    WHERE relation_entity = '{0}'
                )
            )
            UNION all
            SELECT *,
                '{0}' as aggregate_roll_up_to
            FROM aeetest.raw_sas_brazil_metrics
            WHERE owner_login = '{0}'
            """

            # Pipeline Roll-up Query
            pipeline_query = """
            insert into aeetest.rolled_sas_pipeline_target_metrics
            select 
                pipeline_target_name,
                pipeline_id,
                pipeline_name,
                owner,
                owner_login,
                software_count,
                days_non_compliant,
                days_non_compliant_recommendation,
                blocked_resource,
                pipeline_target_action_type,
                associated_campaigns,
                metric_run_date,
                metric_year,
                classification,
                metric_snapshot,
                snapshot_number,
                '{0}' as aggregate_roll_up_to,
                severity,
                risk_priority
            FROM aeetest.raw_sas_pipeline_target_metrics
            WHERE owner_login IN (
                SELECT DISTINCT login_name
                FROM BOOKER.D_EMPLOYEE_DETAILS
                WHERE supervisor_login_name IN (
                    SELECT DISTINCT relation_manager
                    FROM relationship_schema.entity_manager_relationship
                    WHERE relation_entity = '{0}'
                )
            )
            UNION all
            SELECT *,
                '{0}' as aggregate_roll_up_to
            FROM aeetest.raw_sas_pipeline_target_metrics
            WHERE owner_login = '{0}'
            """

            # Execute queries
            insert_apollo_result = self.rs_upsert.execute_redshift_sql_query(apollo_query.format(manager))
            Logger.printLogs("[DEBUG]", f"Apollo Data rolled up for manager {manager} result {insert_apollo_result}")

            insert_brazil_result = self.rs_upsert.execute_redshift_sql_query(brazil_query.format(manager))
            Logger.printLogs("[DEBUG]", f"Brazil Data rolled up for manager {manager} result {insert_brazil_result}")

            insert_pipeline_target_result = self.rs_upsert.execute_redshift_sql_query(pipeline_query.format(manager))
            Logger.printLogs("[DEBUG]", f"Pipeline Target Data rolled up for manager {manager} result {insert_pipeline_target_result}")

            if (insert_apollo_result != 0 or insert_brazil_result != 0 or insert_pipeline_target_result != 0):
                Logger.printLogs("ERROR", f"Query failed to roll the data for manager {manager}")
                continue

        except Exception as error:
            Logger.printLogs("ERROR", f"Rolling up metrics failed for manager {manager}: {error}")
            continue
```

Key changes:
1. Separate queries for each table type with correct columns
2. Proper error handling per manager
3. Continue on error instead of sys.exit
4. Each query matches its table's schema

Would you like me to provide the exact table schemas as well?

------

venry: will this query work ?
# roll_upto_manager_query = """
	# insert into {0}
	# select *,
	# 	   '{1}' AS rolled_upto
	# FROM {2}
	# WHERE owner_login IN (SELECT DISTINCT login_name
	# 				  FROM BOOKER.D_EMPLOYEE_DETAILS
	# 				  WHERE supervisor_login_name IN (SELECT DISTINCT relation_manager
	# 												  FROM relationship_schema.entity_manager_relationship
	# 												  WHERE relation_entity = '{1}'))
	# UNION all
	# SELECT *,
	# 	   '{1}' AS rolled_upto
	# FROM {2}
	# WHERE owner_login = '{1}'
	# """

------

Cedric: That query won't work directly because:

1. Using `SELECT *` with adding a new column can cause issues
2. Different tables (Apollo/Brazil/Pipeline) have different column structures

Here's how we should modify it:

```python
roll_upto_manager_query = {
    'apollo': """
        insert into {0}
        select *,
               '{1}' AS aggregate_roll_up_to
        FROM {2}
        WHERE owner_login IN (
            SELECT DISTINCT login_name
            FROM BOOKER.D_EMPLOYEE_DETAILS
            WHERE supervisor_login_name IN (
                SELECT DISTINCT relation_manager
                FROM relationship_schema.entity_manager_relationship
                WHERE relation_entity = '{1}'
            )
        )
        UNION all
        SELECT *,
               '{1}' AS aggregate_roll_up_to
        FROM {2}
        WHERE owner_login = '{1}'
    """,
    
    'brazil': """
        insert into {0}
        select *,
               '{1}' AS aggregate_roll_up_to
        FROM {2}
        WHERE owner_login IN (
            SELECT DISTINCT login_name
            FROM BOOKER.D_EMPLOYEE_DETAILS
            WHERE supervisor_login_name IN (
                SELECT DISTINCT relation_manager
                FROM relationship_schema.entity_manager_relationship
                WHERE relation_entity = '{1}'
            )
        )
        UNION all
        SELECT *,
               '{1}' AS aggregate_roll_up_to
        FROM {2}
        WHERE owner_login = '{1}'
    """,
    
    'pipeline': """
        insert into {0}
        select *,
               '{1}' AS aggregate_roll_up_to
        FROM {2}
        WHERE owner_login IN (
            SELECT DISTINCT login_name
            FROM BOOKER.D_EMPLOYEE_DETAILS
            WHERE supervisor_login_name IN (
                SELECT DISTINCT relation_manager
                FROM relationship_schema.entity_manager_relationship
                WHERE relation_entity = '{1}'
            )
        )
        UNION all
        SELECT *,
               '{1}' AS aggregate_roll_up_to
        FROM {2}
        WHERE owner_login = '{1}'
    """
}
```

Then in the roll_up_metrics function:

```python
def roll_up_metrics(self, distinct_managers):
    '''
    Function responsible for Rolling up the metrics by using Rawdata tables.
    '''
    Logger.printLogs("[INFO]","Starting rolling up of SAS metrics")

    for manager in distinct_managers:
        try:
            # Apollo roll-up
            insert_apollo_result = self.rs_upsert.execute_redshift_sql_query(
                self.roll_upto_manager_query['apollo'].format(
                    SAS_Config.test_rolled_redshift_apollo_table,
                    manager,
                    SAS_Config.test_redshift_apollo_table
                )
            )
            Logger.printLogs("[DEBUG]", "Apollo Data rolled upto query for {} manager result {}".format(
                manager, 
                insert_apollo_result
            ))

            # Brazil roll-up
            insert_brazil_result = self.rs_upsert.execute_redshift_sql_query(
                self.roll_upto_manager_query['brazil'].format(
                    SAS_Config.test_rolled_redshift_brazil_table,
                    manager,
                    SAS_Config.test_redshift_brazil_table
                )
            )
            Logger.printLogs("[DEBUG]", "Brazil Data rolled upto query for {} manager result {}".format(
                manager, 
                insert_brazil_result
            ))

            # Pipeline roll-up
            insert_pipeline_target_result = self.rs_upsert.execute_redshift_sql_query(
                self.roll_upto_manager_query['pipeline'].format(
                    SAS_Config.test_rolled_redshift_pipeline_target_table,
                    manager,
                    SAS_Config.test_redshift_pipeline_target_table
                )
            )
            Logger.printLogs("[DEBUG]", "Pipeline Target Data rolled upto query for {} manager result {}".format(
                manager, 
                insert_pipeline_target_result
            ))

            if (insert_apollo_result != 0 or insert_brazil_result != 0 or insert_pipeline_target_result != 0):
                Logger.printLogs("ERROR", "Query failed to roll the data for manager {}".format(manager))
                continue

        except Exception as error:
            Logger.printLogs("ERROR", f"Rolling up metrics failed for manager {manager}: {error}")
            continue
```

This approach:
1. Uses separate queries for each table type
2. Maintains table structure integrity
3. Properly handles the roll-up column
4. Includes error handling per manager
5. Uses test tables

Would you like me to provide example table structures to verify the queries?

------

venry: why the rollup to manager query needs to be changed from the older version?

------

Cedric: The roll-up manager query doesn't need to be changed from the older version. I was overcomplicating it. Looking at your error:

```
ERROR:  column "apollo_environment_name" does not exist in raw_sas_brazil_metrics
```

This is happening because we're using the Apollo columns template for Brazil and Pipeline tables. We should have different templates for each table type in the `roll_upto_manager_query`:

```python
# In SAS_Config
roll_upto_manager_query_apollo = """
    insert into {0}
    select *,
           '{1}' AS rolled_upto
    FROM {2}
    WHERE owner_login IN (SELECT DISTINCT login_name
                      FROM BOOKER.D_EMPLOYEE_DETAILS
                      WHERE supervisor_login_name IN (SELECT DISTINCT relation_manager
                                                      FROM relationship_schema.entity_manager_relationship
                                                      WHERE relation_entity = '{1}'))
    UNION all
    SELECT *,
           '{1}' AS rolled_upto
    FROM {2}
    WHERE owner_login = '{1}'
"""

roll_upto_manager_query_brazil = """
    insert into {0}
    select *,
           '{1}' AS rolled_upto
    FROM {2}
    WHERE owner_login IN (SELECT DISTINCT login_name
                      FROM BOOKER.D_EMPLOYEE_DETAILS
                      WHERE supervisor_login_name IN (SELECT DISTINCT relation_manager
                                                      FROM relationship_schema.entity_manager_relationship
                                                      WHERE relation_entity = '{1}'))
    UNION all
    SELECT *,
           '{1}' AS rolled_upto
    FROM {2}
    WHERE owner_login = '{1}'
"""

roll_upto_manager_query_pipeline = """
    insert into {0}
    select *,
           '{1}' AS rolled_upto
    FROM {2}
    WHERE owner_login IN (SELECT DISTINCT login_name
                      FROM BOOKER.D_EMPLOYEE_DETAILS
                      WHERE supervisor_login_name IN (SELECT DISTINCT relation_manager
                                                      FROM relationship_schema.entity_manager_relationship
                                                      WHERE relation_entity = '{1}'))
    UNION all
    SELECT *,
           '{1}' AS rolled_upto
    FROM {2}
    WHERE owner_login = '{1}'
"""
```

Then in the roll_up_metrics function:

```python
def roll_up_metrics(self, distinct_managers):
    '''
    Function responsible for Rolling up the metrics by using Rawdata tables.
    '''
    Logger.printLogs("[INFO]","Starting rolling up of SAS metrics")

    for manager in distinct_managers:
        try:
            # Using appropriate query for each table type
            insert_apollo_result = self.rs_upsert.execute_redshift_sql_query(
                SAS_Config.roll_upto_manager_query_apollo.format(
                    SAS_Config.test_rolled_redshift_apollo_table,
                    manager,
                    SAS_Config.test_redshift_apollo_table
                )
            )
            Logger.printLogs("[DEBUG]","Apollo Data rolled upto query for {} manager result {}".format(manager, insert_apollo_result))

            insert_brazil_result = self.rs_upsert.execute_redshift_sql_query(
                SAS_Config.roll_upto_manager_query_brazil.format(
                    SAS_Config.test_rolled_redshift_brazil_table,
                    manager,
                    SAS_Config.test_redshift_brazil_table
                )
            )
            Logger.printLogs("[DEBUG]","Brazil Data rolled upto query for {} manager result {}".format(manager, insert_brazil_result))

            insert_pipeline_target_result = self.rs_upsert.execute_redshift_sql_query(
                SAS_Config.roll_upto_manager_query_pipeline.format(
                    SAS_Config.test_rolled_redshift_pipeline_target_table,
                    manager,
                    SAS_Config.test_redshift_pipeline_target_table
                )
            )
            Logger.printLogs("[DEBUG]","Pipeline Target Data rolled upto query for {} manager result {}".format(manager, insert_pipeline_target_result))

            if (insert_apollo_result != 0 or insert_brazil_result != 0 or insert_pipeline_target_result != 0):
                Logger.printLogs("ERROR","Query failed to roll the data for manager {}".format(manager))
                sys.exit(1)

        except Exception as error:
            Logger.printLogs("ERROR", f"Rolling up metrics failed for manager {manager}: {error}")
            sys.exit(1)
```

Or even simpler, we could keep the original query and just use SELECT * since the tables were created with the same structure as the source:

```python
roll_upto_manager_query = """
    insert into {0}
    select *,
           '{1}' AS rolled_upto
    FROM {2}
    WHERE owner_login IN (SELECT DISTINCT login_name
                      FROM BOOKER.D_EMPLOYEE_DETAILS
                      WHERE supervisor_login_name IN (SELECT DISTINCT relation_manager
                                                      FROM relationship_schema.entity_manager_relationship
                                                      WHERE relation_entity = '{1}'))
    UNION all
    SELECT *,
           '{1}' AS rolled_upto
    FROM {2}
    WHERE owner_login = '{1}'
"""
```

Since you created the test tables as copies of the production tables, using SELECT * should work. Would you like to try this simpler approach first?